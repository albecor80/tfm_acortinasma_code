{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a9db1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 08:54:20,695 - INFO - [2025-05-15 08:54:20] Iniciando script de modelos de stacking (con modelos base preentrenados) por clusters - v3\n",
      "2025-05-15 08:54:20,697 - INFO - [2025-05-15 08:54:20] Directorio de salida principal: ../models/stacking_preloaded_v3\n",
      "2025-05-15 08:54:20,697 - INFO - [2025-05-15 08:54:20] Modelos base preentrenados se esperan en: ../models/stacking/trained_models\n",
      "2025-05-15 08:54:20,698 - INFO - [2025-05-15 08:54:20] Meta-modelos se guardarán en: ../models/stacking_preloaded_v3/trained_meta_models\n",
      "2025-05-15 08:54:20,698 - INFO - [2025-05-15 08:54:20] Las predicciones se guardarán en: ../models/stacking_preloaded_v3/predictions\n",
      "2025-05-15 08:54:20,699 - INFO - [2025-05-15 08:54:20] Los gráficos se guardarán en: ../models/stacking_preloaded_v3/prediction_plots\n",
      "2025-05-15 08:54:20,699 - INFO - [2025-05-15 08:54:20] El archivo de métricas será: ../models/stacking_preloaded_v3/all_clusters_metrics.csv\n",
      "2025-05-15 08:54:20,700 - INFO - [2025-05-15 08:54:20] El archivo de checkpoint será: ../models/stacking_preloaded_v3/stacking_checkpoint.txt\n",
      "2025-05-15 08:54:20,700 - INFO - [2025-05-15 08:54:20] Meta-modelo a utilizar: RIDGE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit # Aunque no se use para OOF base, puede ser útil para otras cosas\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import joblib\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "import pyarrow.parquet as pq\n",
    "import psutil # Para registrar uso de memoria\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.preprocessing import MinMaxScaler # Necesario para cargar/usar scalers\n",
    "\n",
    "\n",
    "# --- Configuración del Logging ---\n",
    "LOG_FILE = 'stacking_model_progress_v3.log' # Nuevo nombre de log\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(LOG_FILE, mode='w'), # mode='w' para sobrescribir en cada ejecución\n",
    "        logging.StreamHandler(sys.stdout)\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "log_progress_initial = lambda message: logger.info(f\"[{time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())}] {message}\")\n",
    "\n",
    "log_progress_initial(\"Iniciando script de modelos de stacking (con modelos base preentrenados) por clusters - v3\")\n",
    "\n",
    "# --- Supresión de Warnings ---\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # Ignorar UserWarning de LightGBM sobre tipos de datos\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning) # Ignorar PerformanceWarning de Pandas\n",
    "\n",
    "# --- Configuración Global ---\n",
    "# Rutas de archivos y directorios\n",
    "PARQUET_FILE_PATH = '../data/gold_ventas_semanales_clustered_lgbm.parquet' # Asegúrate que esta ruta sea correcta\n",
    "OUTPUT_DIR = '../models/stacking_preloaded_v3' # Directorio de salida\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'trained_meta_models') # Directorio para meta-modelos entrenados\n",
    "PRETRAINED_MODELS_DIR = '../models/stacking/trained_models' # !! IMPORTANTE: Ruta a TUS modelos base preentrenados !!\n",
    "PREDICTIONS_DIR = os.path.join(OUTPUT_DIR, 'predictions')\n",
    "PLOTS_DIR = os.path.join(OUTPUT_DIR, 'prediction_plots')\n",
    "METRICS_FILE = os.path.join(OUTPUT_DIR, 'all_clusters_metrics.csv')\n",
    "CHECKPOINT_FILE = os.path.join(OUTPUT_DIR, 'stacking_checkpoint.txt')\n",
    "\n",
    "# Nombres de columnas\n",
    "TARGET_COL = 'weekly_volume'\n",
    "CLUSTER_COL = 'cluster_label'\n",
    "DATE_COL = 'week'\n",
    "SERIES_ID_COLS = ['establecimiento', 'material'] # Identificadores únicos de series temporales\n",
    "\n",
    "# Parámetros del Modelo y Procesamiento\n",
    "N_TEST_WEEKS = 12  # Semanas para el conjunto de prueba final de CADA cluster\n",
    "META_MODEL_TYPE = 'ridge'  # Opciones: 'ridge', 'lightgbm', 'rf' (Random Forest)\n",
    "\n",
    "# Parámetros para control de memoria y procesamiento por lotes\n",
    "MAX_ROWS_PER_CLUSTER = 1000000  # Límite de filas por clúster (None para sin límite)\n",
    "CHUNK_SIZE = 5000  # Tamaño de lote para predicciones si los datasets son grandes\n",
    "\n",
    "# Plantillas para nombres de archivos de modelos base preentrenados\n",
    "# !! AJUSTA LA EXTENSIÓN (.joblib o .pkl) SEGÚN TUS ARCHIVOS !!\n",
    "BASE_LGBM_MODEL_NAME_TEMPLATE = 'lgbm_base_cluster_{cluster_id}.joblib'\n",
    "BASE_RF_MODEL_NAME_TEMPLATE = 'rf_base_cluster_{cluster_id}.joblib'\n",
    "\n",
    "# --- Creación de Directorios de Salida ---\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "log_progress_initial(f\"Directorio de salida principal: {OUTPUT_DIR}\")\n",
    "log_progress_initial(f\"Modelos base preentrenados se esperan en: {PRETRAINED_MODELS_DIR}\")\n",
    "log_progress_initial(f\"Meta-modelos se guardarán en: {MODELS_DIR}\")\n",
    "log_progress_initial(f\"Las predicciones se guardarán en: {PREDICTIONS_DIR}\")\n",
    "log_progress_initial(f\"Los gráficos se guardarán en: {PLOTS_DIR}\")\n",
    "log_progress_initial(f\"El archivo de métricas será: {METRICS_FILE}\")\n",
    "log_progress_initial(f\"El archivo de checkpoint será: {CHECKPOINT_FILE}\")\n",
    "log_progress_initial(f\"Meta-modelo a utilizar: {META_MODEL_TYPE.upper()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28bdd23",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5650df5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Definiciones de Funciones ---\n",
    "\n",
    "def log_memory_usage():\n",
    "    \"\"\"Registra el uso actual de memoria del proceso.\"\"\"\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_info = process.memory_info()\n",
    "    mem_gb = mem_info.rss / (1024 ** 3)  # Convertir bytes a GB\n",
    "    logger.info(f\"Uso de memoria: {mem_gb:.2f} GB\")\n",
    "\n",
    "def log_progress(message):\n",
    "    \"\"\"Registra un mensaje de progreso con timestamp y fuerza la escritura inmediata.\"\"\"\n",
    "    timestamp = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "    logger.info(f\"[{timestamp}] {message}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "def load_and_prepare_cluster_data(parquet_path, cluster_id, target_col, date_col, cluster_col_name, series_id_cols, n_test_weeks, max_rows=None):\n",
    "    \"\"\"Carga y prepara los datos para un clúster específico.\"\"\"\n",
    "    log_progress(f\"\\n--- Cargando y Preparando Datos para Cluster {cluster_id} ---\")\n",
    "    data_tuple = (None,) * 7 # X_train, y_train, X_test, y_test, cat_features, dates_test, ids_test\n",
    "\n",
    "    try:\n",
    "        # Leer solo las columnas necesarias inicialmente para determinar el rango de fechas si max_rows está activo\n",
    "        initial_cols_to_read = [date_col, cluster_col_name] + series_id_cols + [target_col]\n",
    "        # Asegurarse que no haya duplicados en initial_cols_to_read\n",
    "        initial_cols_to_read = sorted(list(set(initial_cols_to_read)))\n",
    "\n",
    "        filters = [(cluster_col_name, '=', cluster_id)]\n",
    "        df_initial_check = pd.read_parquet(parquet_path, filters=filters, columns=initial_cols_to_read)\n",
    "        \n",
    "        if df_initial_check.empty:\n",
    "            logger.warning(f\"No hay datos para cluster {cluster_id} tras el filtro inicial.\")\n",
    "            return data_tuple\n",
    "        \n",
    "        total_rows = len(df_initial_check)\n",
    "        log_progress(f\"Cluster {cluster_id}: Total de {total_rows} filas encontradas inicialmente.\")\n",
    "\n",
    "        cutoff_date_limit = None\n",
    "        if max_rows is not None and total_rows > max_rows:\n",
    "            log_progress(f\"Limitando a ~{max_rows} filas (de {total_rows}) para control de memoria, seleccionando las más recientes.\")\n",
    "            df_initial_check[date_col] = pd.to_datetime(df_initial_check[date_col])\n",
    "            df_initial_check = df_initial_check.sort_values(by=date_col, ascending=False)\n",
    "            cutoff_date_limit = df_initial_check[date_col].iloc[max_rows -1] # -1 porque es 0-indexed\n",
    "            log_progress(f\"Se conservarán datos desde la fecha: {cutoff_date_limit} para el cluster {cluster_id}\")\n",
    "        \n",
    "        del df_initial_check\n",
    "        gc.collect()\n",
    "\n",
    "        # Cargar todas las columnas necesarias para el clúster, aplicando el filtro de fecha si es necesario\n",
    "        if cutoff_date_limit:\n",
    "            # Este filtro puede ser lento si se aplica después de leer todo el Parquet.\n",
    "            # Idealmente, Parquet apoya predicados, pero pd.read_parquet no siempre los pasa bien para > en fechas.\n",
    "            # Una estrategia es leer por chunks o usar Dask/PyArrow directo si esto es un cuello de botella.\n",
    "            # Por ahora, se lee todo el cluster y luego se filtra en Pandas.\n",
    "            df_cluster = pd.read_parquet(parquet_path, filters=filters)\n",
    "            df_cluster[date_col] = pd.to_datetime(df_cluster[date_col])\n",
    "            df_cluster = df_cluster[df_cluster[date_col] >= cutoff_date_limit]\n",
    "            log_progress(f\"Filtrado por fecha en pandas: {len(df_cluster)} filas mantenidas desde {cutoff_date_limit}\")\n",
    "        else:\n",
    "            df_cluster = pd.read_parquet(parquet_path, filters=filters)\n",
    "        \n",
    "        log_progress(f\"Cluster {cluster_id}: {len(df_cluster)} filas cargadas efectivamente para procesamiento.\")\n",
    "        if df_cluster.empty:\n",
    "            logger.warning(f\"No hay datos para cluster {cluster_id} después de la carga y filtrado de fecha.\")\n",
    "            return data_tuple\n",
    "        log_memory_usage()\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error severo cargando datos para cluster {cluster_id}: {e}\", exc_info=True)\n",
    "        return data_tuple\n",
    "\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df_cluster[date_col]):\n",
    "        try:\n",
    "            df_cluster[date_col] = pd.to_datetime(df_cluster[date_col])\n",
    "        except Exception as parse_error:\n",
    "            logger.error(f\"Error convirtiendo columna '{date_col}' a datetime para cluster {cluster_id}: {parse_error}.\")\n",
    "            return data_tuple\n",
    "\n",
    "    df_cluster = df_cluster.sort_values(by=series_id_cols + [date_col]).reset_index(drop=True)\n",
    "\n",
    "    # Manejo de NaNs en features (asumiendo que las features ya existen en el Parquet)\n",
    "    initial_rows = len(df_cluster)\n",
    "    cols_with_potential_nans = [col for col in df_cluster.columns if\n",
    "                                'lag_' in col or 'roll_' in col or 'ewm' in col or\n",
    "                                'diff' in col or 'days_since_last_sale' in col or\n",
    "                                'expanding_' in col or 'shift_' in col]\n",
    "    cols_to_drop_nans_present = [col for col in cols_with_potential_nans if col in df_cluster.columns]\n",
    "    \n",
    "    if cols_to_drop_nans_present:\n",
    "        df_cluster.dropna(subset=cols_to_drop_nans_present, inplace=True)\n",
    "    \n",
    "    final_rows = len(df_cluster)\n",
    "    if initial_rows > final_rows:\n",
    "        log_progress(f\"Manejo de NaNs en features derivadas: Se eliminaron {initial_rows - final_rows} filas.\")\n",
    "    \n",
    "    if df_cluster.empty:\n",
    "        logger.error(f\"DataFrame del cluster {cluster_id} vacío después de eliminar NaNs en features.\")\n",
    "        return data_tuple\n",
    "\n",
    "    # Identificación de features categóricas\n",
    "    # Esta lista debe ser lo más completa posible, o inferirse de dtypes si es más robusto\n",
    "    categorical_features_list = [\n",
    "        'establecimiento', 'material', # Suelen ser categóricas\n",
    "        'year', 'month', 'week_of_year', 'day_of_week', 'day_of_month', 'day_of_year', 'quarter', # Derivadas de fecha\n",
    "        'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week' # Indicadores booleanos/categóricos\n",
    "    ]\n",
    "    # Mantener solo las que existen en el DataFrame\n",
    "    categorical_features_present = [col for col in categorical_features_list if col in df_cluster.columns]\n",
    "    \n",
    "    for col in categorical_features_present:\n",
    "        # Convertir a 'category' dtype si no lo son ya. LightGBM lo maneja bien.\n",
    "        # Para RF, esto significa que usará los códigos de categoría subyacentes si no se hace OHE.\n",
    "        if df_cluster[col].isnull().any():\n",
    "             df_cluster[col] = df_cluster[col].fillna('Missing').astype('category') # Rellenar y convertir\n",
    "        elif not isinstance(df_cluster[col].dtype, pd.CategoricalDtype):\n",
    "            try:\n",
    "                df_cluster[col] = df_cluster[col].astype('category')\n",
    "            except TypeError as e:\n",
    "                 logger.warning(f\"No se pudo convertir '{col}' a category en cluster {cluster_id}: {e}. Se usará como está.\")\n",
    "\n",
    "    # Definición de features (X) y target (y)\n",
    "    columns_to_exclude = [target_col, date_col, cluster_col_name, 'last_sale_week', 'unique_id'] # unique_id si existe\n",
    "    features = [col for col in df_cluster.columns if col not in columns_to_exclude]\n",
    "    X = df_cluster[features]\n",
    "    y = df_cluster[target_col]\n",
    "\n",
    "    # Split Train/Test basado en tiempo\n",
    "    if len(df_cluster) < (n_test_weeks + 1): # Necesitas al menos 1 para entrenar y n_test_weeks para testear\n",
    "         logger.warning(f\"Datos insuficientes en cluster {cluster_id} ({len(df_cluster)} filas) para split con {n_test_weeks} semanas de test.\")\n",
    "         return data_tuple\n",
    "\n",
    "    last_date_in_cluster = df_cluster[date_col].max()\n",
    "    # El cutoff es la primera fecha del conjunto de test\n",
    "    cutoff_date_for_test_start = last_date_in_cluster - pd.Timedelta(weeks=n_test_weeks - 1) \n",
    "    \n",
    "    train_mask = (df_cluster[date_col] < cutoff_date_for_test_start)\n",
    "    test_mask = (df_cluster[date_col] >= cutoff_date_for_test_start)\n",
    "\n",
    "    if train_mask.sum() == 0:\n",
    "        logger.warning(f\"Conjunto de entrenamiento vacío después del split en cluster {cluster_id} (cutoff: {cutoff_date_for_test_start}).\")\n",
    "        return data_tuple\n",
    "    if test_mask.sum() == 0:\n",
    "        logger.warning(f\"Conjunto de prueba vacío después del split en cluster {cluster_id} (cutoff: {cutoff_date_for_test_start}).\")\n",
    "        return data_tuple\n",
    "\n",
    "    X_train, X_test = X[train_mask].copy(), X[test_mask].copy()\n",
    "    y_train, y_test = y[train_mask].copy(), y[test_mask].copy()\n",
    "    dates_test = df_cluster.loc[test_mask, date_col].copy()\n",
    "    ids_test = df_cluster.loc[test_mask, series_id_cols].copy()\n",
    "\n",
    "    del df_cluster # Liberar memoria\n",
    "    gc.collect()\n",
    "    log_memory_usage()\n",
    "\n",
    "    log_progress(f\"Cluster {cluster_id} Split: Train={X_train.shape[0]}, Test={X_test.shape[0]}. Features categóricas identificadas: {categorical_features_present}\")\n",
    "    return X_train, y_train, X_test, y_test, categorical_features_present, dates_test, ids_test\n",
    "\n",
    "\n",
    "def generate_base_model_predictions_cluster(X_train, X_test, cluster_id, pretrained_models_path, lgbm_template, rf_template, cat_features_for_lgbm):\n",
    "    \"\"\"Carga modelos LGBM y RF preentrenados y genera predicciones.\"\"\"\n",
    "    log_progress(f\"--- Cargando Modelos Base y Generando Predicciones para Cluster {cluster_id} ---\")\n",
    "    predictions_tuple = (None, None, None, None) # oof_lgbm, oof_rf, test_lgbm, test_rf\n",
    "\n",
    "    lgbm_model_file = os.path.join(pretrained_models_path, lgbm_template.format(cluster_id=cluster_id))\n",
    "    rf_model_file = os.path.join(pretrained_models_path, rf_template.format(cluster_id=cluster_id))\n",
    "\n",
    "    base_model_lgbm, base_model_rf = None, None\n",
    "    try:\n",
    "        log_progress(f\"Cargando modelo LightGBM desde: {lgbm_model_file}\")\n",
    "        base_model_lgbm = joblib.load(lgbm_model_file)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Modelo LightGBM NO ENCONTRADO para cluster {cluster_id} en {lgbm_model_file}\")\n",
    "        return predictions_tuple\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cargando modelo LightGBM para cluster {cluster_id}: {e}\", exc_info=True)\n",
    "        return predictions_tuple\n",
    "\n",
    "    try:\n",
    "        log_progress(f\"Cargando modelo Random Forest desde: {rf_model_file}\")\n",
    "        base_model_rf = joblib.load(rf_model_file)\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Modelo Random Forest NO ENCONTRADO para cluster {cluster_id} en {rf_model_file}\")\n",
    "        return predictions_tuple\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cargando modelo Random Forest para cluster {cluster_id}: {e}\", exc_info=True)\n",
    "        return predictions_tuple\n",
    "\n",
    "    # Asegurar que X_train y X_test tengan las features categóricas como 'category' para LGBM si es necesario\n",
    "    # Esta conversión ya se hace en load_and_prepare_cluster_data, pero un check doble no hace daño.\n",
    "    # X_train_lgbm_ready = X_train.copy()\n",
    "    # X_test_lgbm_ready = X_test.copy()\n",
    "    # for col in cat_features_for_lgbm:\n",
    "    #     if col in X_train_lgbm_ready.columns and not isinstance(X_train_lgbm_ready[col].dtype, pd.CategoricalDtype):\n",
    "    #         X_train_lgbm_ready[col] = X_train_lgbm_ready[col].astype('category')\n",
    "    #     if col in X_test_lgbm_ready.columns and not isinstance(X_test_lgbm_ready[col].dtype, pd.CategoricalDtype):\n",
    "    #         X_test_lgbm_ready[col] = X_test_lgbm_ready[col].astype('category')\n",
    "    \n",
    "    # Predicciones \"OOF\" (directas sobre el conjunto de entrenamiento)\n",
    "    log_progress(f\"Generando predicciones (tipo OOF) en datos de entrenamiento para cluster {cluster_id}\")\n",
    "    # Para RF, se asume que las features categóricas ya están codificadas como esperaba el modelo (ej. label encoded por .cat.codes)\n",
    "    # o que el modelo RF puede manejarlas si son 'category'. Sklearn RF no las usa directamente.\n",
    "    # Si RF fue entrenado con OHE, X_train/X_test necesitarían esa OHE aquí.\n",
    "    try:\n",
    "        oof_preds_lgbm_vals = base_model_lgbm.predict(X_train) # Usar X_train directamente\n",
    "        oof_preds_rf_vals = base_model_rf.predict(X_train)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generando predicciones OOF para cluster {cluster_id}: {e}\", exc_info=True)\n",
    "        logger.error(f\"Columnas X_train: {X_train.columns.tolist()}\")\n",
    "        logger.error(f\"Tipos X_train: {X_train.dtypes}\")\n",
    "        return predictions_tuple\n",
    "\n",
    "\n",
    "    oof_series_lgbm = pd.Series(oof_preds_lgbm_vals, index=X_train.index, name='oof_lgbm_pred')\n",
    "    oof_series_rf = pd.Series(oof_preds_rf_vals, index=X_train.index, name='oof_rf_pred')\n",
    "\n",
    "    # Predicciones en el conjunto de prueba\n",
    "    log_progress(f\"Generando predicciones en datos de prueba para cluster {cluster_id}\")\n",
    "    try:\n",
    "        if len(X_test) > CHUNK_SIZE:\n",
    "            log_progress(f\"Prediciendo en lotes (chunk_size={CHUNK_SIZE}) para Test (Cluster {cluster_id})\")\n",
    "            test_preds_lgbm = np.concatenate([\n",
    "                base_model_lgbm.predict(X_test.iloc[i:i + CHUNK_SIZE]) \n",
    "                for i in range(0, len(X_test), CHUNK_SIZE)\n",
    "            ])\n",
    "            test_preds_rf = np.concatenate([\n",
    "                base_model_rf.predict(X_test.iloc[i:i + CHUNK_SIZE])\n",
    "                for i in range(0, len(X_test), CHUNK_SIZE)\n",
    "            ])\n",
    "        else:\n",
    "            test_preds_lgbm = base_model_lgbm.predict(X_test) # Usar X_test directamente\n",
    "            test_preds_rf = base_model_rf.predict(X_test)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generando predicciones en Test para cluster {cluster_id}: {e}\", exc_info=True)\n",
    "        logger.error(f\"Columnas X_test: {X_test.columns.tolist()}\")\n",
    "        logger.error(f\"Tipos X_test: {X_test.dtypes}\")\n",
    "\n",
    "        return predictions_tuple\n",
    "        \n",
    "    del base_model_lgbm, base_model_rf\n",
    "    gc.collect()\n",
    "    log_memory_usage()\n",
    "    return oof_series_lgbm, oof_series_rf, test_preds_lgbm, test_preds_rf\n",
    "\n",
    "def train_meta_model_cluster(X_train_original_features, oof_preds_lgbm, oof_preds_rf, y_train, cluster_id, meta_model_type, models_save_dir):\n",
    "    \"\"\"Entrena el meta-modelo para un clúster.\"\"\"\n",
    "    log_progress(f\"--- Entrenando Meta-Modelo ({meta_model_type.upper()}) para Cluster {cluster_id} ---\")\n",
    "    \n",
    "    # Construir el DataFrame para entrenar el meta-modelo\n",
    "    # Opción 1: Usar solo las predicciones OOF como features\n",
    "    # X_meta_train = pd.DataFrame({\n",
    "    #     'oof_lgbm': oof_preds_lgbm.values,\n",
    "    #     'oof_rf': oof_preds_rf.values\n",
    "    # }, index=oof_preds_lgbm.index)\n",
    "\n",
    "    # Opción 2: Usar las features originales + las predicciones OOF\n",
    "    X_meta_train = pd.DataFrame({\n",
    "        'oof_lgbm_pred_meta': oof_preds_lgbm.values,\n",
    "        'oof_rf_pred_meta': oof_preds_rf.values\n",
    "    }, index=X_train_original_features.index) # Usar el índice original para consistencia con y_train\n",
    "\n",
    "    meta_model = None\n",
    "    try:\n",
    "        if meta_model_type == 'ridge':\n",
    "            categorical_cols_in_meta = X_meta_train.select_dtypes(include='category').columns\n",
    "            if not categorical_cols_in_meta.empty:\n",
    "                log_progress(f\"Aplicando get_dummies a X_meta_train para Ridge: {categorical_cols_in_meta.tolist()}\")\n",
    "                X_meta_train = pd.get_dummies(X_meta_train, columns=categorical_cols_in_meta, dummy_na=False, sparse=True)\n",
    "            meta_model = Ridge(alpha=1.0, random_state=42)\n",
    "\n",
    "        elif meta_model_type == 'lightgbm':\n",
    "            meta_params_lgbm = {\n",
    "                'objective': 'regression_l1', 'metric': 'mae', 'verbosity': -1, 'boosting_type': 'gbdt',\n",
    "                'seed': 42, 'n_jobs': -1, 'learning_rate': 0.05, 'num_leaves': 25, \n",
    "                'max_depth': 5, 'n_estimators': 200, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 1, 'min_child_samples': 30\n",
    "            }\n",
    "            meta_model = lgb.LGBMRegressor(**meta_params_lgbm)\n",
    "            # Identificar features categóricas para LGBM meta (excluyendo las OOF que son numéricas)\n",
    "            # Las features originales que eran categóricas deben pasarse a LGBM\n",
    "            cat_features_for_meta_lgbm = [\n",
    "                col for col in X_meta_train.select_dtypes(include='category').columns\n",
    "                if col not in ['oof_lgbm_pred_meta', 'oof_rf_pred_meta']\n",
    "            ]\n",
    "            if not cat_features_for_meta_lgbm: cat_features_for_meta_lgbm = 'auto'\n",
    "            \n",
    "            log_progress(f\"Entrenando meta-modelo LightGBM con features categóricas: {cat_features_for_meta_lgbm}\")\n",
    "            meta_model.fit(X_meta_train, y_train, categorical_feature=cat_features_for_meta_lgbm)\n",
    "            # Early stopping podría añadirse aquí si se define un eval_set para el meta-modelo\n",
    "\n",
    "        elif meta_model_type == 'rf':\n",
    "            categorical_cols_in_meta_rf = X_meta_train.select_dtypes(include='category').columns\n",
    "            if not categorical_cols_in_meta_rf.empty:\n",
    "                log_progress(f\"Aplicando get_dummies a X_meta_train para RF: {categorical_cols_in_meta_rf.tolist()}\")\n",
    "                X_meta_train = pd.get_dummies(X_meta_train, columns=categorical_cols_in_meta_rf, dummy_na=False)\n",
    "            \n",
    "            meta_params_rf = {\n",
    "                'n_estimators': 150, 'random_state': 42, 'n_jobs': -1, 'max_depth': 10,\n",
    "                'min_samples_split': 40, 'min_samples_leaf': 25, 'max_features': 0.7\n",
    "            }\n",
    "            meta_model = RandomForestRegressor(**meta_params_rf)\n",
    "        else:\n",
    "            logger.error(f\"Tipo de meta-modelo '{meta_model_type}' no soportado.\")\n",
    "            return None\n",
    "\n",
    "        # Entrenar (si no es LGBM que ya se entrenó con sus particularidades)\n",
    "        if meta_model_type != 'lightgbm': # LGBM ya se entrenó\n",
    "             # Si X_meta_train es sparse (por Ridge) y el modelo no lo maneja, convertir a denso\n",
    "             # if hasattr(X_meta_train, \"toarray\") and meta_model_type not in ['lightgbm']: # lightgbm maneja sparse\n",
    "             #     X_meta_train = X_meta_train.toarray()\n",
    "            meta_model.fit(X_meta_train, y_train)\n",
    "\n",
    "        log_progress(f\"Meta-modelo ({meta_model_type.upper()}) para cluster {cluster_id} entrenado.\")\n",
    "        \n",
    "        model_meta_path = os.path.join(models_save_dir, f'meta_model_{meta_model_type}_cluster_{cluster_id}.joblib')\n",
    "        joblib.dump(meta_model, model_meta_path)\n",
    "        log_progress(f\"Meta-modelo guardado en: {model_meta_path}\")\n",
    "        \n",
    "        return meta_model\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"¡ERROR entrenando el meta-modelo ({meta_model_type.upper()}) para Cluster {cluster_id}!: {e}\", exc_info=True)\n",
    "        if 'X_meta_train' in locals():\n",
    "            logger.error(f\"X_meta_train dtypes: \\n{X_meta_train.dtypes.value_counts() if isinstance(X_meta_train, pd.DataFrame) else type(X_meta_train)}\")\n",
    "        return None\n",
    "    finally:\n",
    "        if 'X_meta_train' in locals(): del X_meta_train # Asegurar liberación\n",
    "        gc.collect()\n",
    "        log_memory_usage()\n",
    "\n",
    "def evaluate_model_cluster(y_true, y_pred, cluster_id, model_label=\"Stacking\"):\n",
    "    \"\"\"Evalúa el modelo y devuelve un diccionario de métricas.\"\"\"\n",
    "    log_progress(f\"--- Evaluando Modelo {model_label} para Cluster {cluster_id} ---\")\n",
    "    \n",
    "    y_true_safe = np.asarray(y_true)\n",
    "    y_pred_safe = np.asarray(y_pred)\n",
    "    \n",
    "    # Reemplazar NaNs/Infs en predicciones si los hubiera\n",
    "    if np.isnan(y_pred_safe).any() or np.isinf(y_pred_safe).any():\n",
    "        logger.warning(f\"Cluster {cluster_id}, Modelo {model_label}: Predicciones contienen NaNs/Infs. Se reemplazarán con 0 para evaluación.\")\n",
    "        y_pred_safe = np.nan_to_num(y_pred_safe, nan=0.0, posinf=np.nanmax(y_true_safe) if y_true_safe.size > 0 else 0.0 , neginf=0.0)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_true_safe, y_pred_safe))\n",
    "    mae = mean_absolute_error(y_true_safe, y_pred_safe)\n",
    "    r2 = r2_score(y_true_safe, y_pred_safe)  # <--- CALCULAR R-CUADRADO\n",
    "\n",
    "    log_progress(f\"  {model_label} - MAE: {mae:.4f}, RMSE: {rmse:.4f}, R2: {r2:.4f}\") \n",
    "    return {'cluster_id': cluster_id, f'{model_label}_mae': mae, f'{model_label}_rmse': rmse, f'{model_label}_r2': r2}\n",
    "\n",
    "def plot_predictions_series(dates, y_true, y_pred, estab, material, cluster_id, save_path, model_label=\"Stacking\"):\n",
    "    \"\"\"Genera y guarda un gráfico de predicciones vs reales para una serie.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(dates, y_true, label='Real', marker='.', linestyle='-', color='dodgerblue', alpha=0.8)\n",
    "    plt.plot(dates, y_pred, label=f'Predicción {model_label}', marker='x', linestyle='--', color='orangered', alpha=0.8)\n",
    "    plt.title(f'Predicción vs Real - Est: {estab}, Mat: {material} (Clúster {cluster_id})', fontsize=14)\n",
    "    plt.xlabel('Semana', fontsize=12)\n",
    "    plt.ylabel('Volumen Semanal', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, linestyle=':', alpha=0.6)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    try:\n",
    "        plt.savefig(save_path, dpi=100)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error guardando gráfico {save_path} para cluster {cluster_id}: {e}\")\n",
    "    plt.close() # Cerrar la figura para liberar memoria\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e75bee52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-15 08:57:46,973 - INFO - [2025-05-15 08:57:46] ================ INICIO DEL PROCESAMIENTO DE STACKING ================\n",
      "2025-05-15 08:57:46,975 - INFO - [2025-05-15 08:57:46] Leyendo columna de clusters 'cluster_label' del archivo Parquet: ../data/gold_ventas_semanales_clustered_lgbm.parquet\n",
      "2025-05-15 08:57:47,199 - INFO - [2025-05-15 08:57:47] Encontrados 7 clusters únicos para procesar: [np.int32(0), np.int32(1), np.int32(2), np.int32(3), np.int32(4), np.int32(5), np.int32(6)]\n",
      "2025-05-15 08:57:47,201 - INFO - [2025-05-15 08:57:47] Archivo de métricas ../models/stacking_preloaded_v3/all_clusters_metrics.csv creado con encabezado.\n",
      "2025-05-15 08:57:47,202 - INFO - [2025-05-15 08:57:47] \n",
      "========== Procesando Cluster 1/7: ID = 0 ==========\n",
      "2025-05-15 08:57:47,203 - INFO - Uso de memoria: 1.51 GB\n",
      "2025-05-15 08:57:47,203 - INFO - [2025-05-15 08:57:47] \n",
      "--- Cargando y Preparando Datos para Cluster 0 ---\n",
      "2025-05-15 08:57:47,505 - INFO - [2025-05-15 08:57:47] Cluster 0: Total de 7702523 filas encontradas inicialmente.\n",
      "2025-05-15 08:57:47,506 - INFO - [2025-05-15 08:57:47] Limitando a ~1000000 filas (de 7702523) para control de memoria, seleccionando las más recientes.\n",
      "2025-05-15 08:57:48,384 - INFO - [2025-05-15 08:57:48] Se conservarán datos desde la fecha: 2024-08-12 00:00:00 para el cluster 0\n",
      "2025-05-15 08:57:52,819 - INFO - [2025-05-15 08:57:52] Filtrado por fecha en pandas: 1009872 filas mantenidas desde 2024-08-12 00:00:00\n",
      "2025-05-15 08:57:52,822 - INFO - [2025-05-15 08:57:52] Cluster 0: 1009872 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:57:52,822 - INFO - Uso de memoria: 0.25 GB\n",
      "2025-05-15 08:57:53,245 - INFO - [2025-05-15 08:57:53] Manejo de NaNs en features derivadas: Se eliminaron 9400 filas.\n",
      "2025-05-15 08:57:53,607 - INFO - Uso de memoria: 1.74 GB\n",
      "2025-05-15 08:57:53,608 - INFO - [2025-05-15 08:57:53] Cluster 0 Split: Train=465729, Test=534743. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:57:53,618 - INFO - [2025-05-15 08:57:53] --- Cargando Modelos Base y Generando Predicciones para Cluster 0 ---\n",
      "2025-05-15 08:57:53,618 - INFO - [2025-05-15 08:57:53] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_0.joblib\n",
      "2025-05-15 08:57:53,625 - INFO - [2025-05-15 08:57:53] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_0.joblib\n",
      "2025-05-15 08:57:53,632 - INFO - [2025-05-15 08:57:53] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 0\n",
      "2025-05-15 08:57:57,139 - INFO - [2025-05-15 08:57:57] Generando predicciones en datos de prueba para cluster 0\n",
      "2025-05-15 08:57:57,140 - INFO - [2025-05-15 08:57:57] Prediciendo en lotes (chunk_size=5000) para Test (Cluster 0)\n",
      "2025-05-15 08:58:11,651 - INFO - Uso de memoria: 1.39 GB\n",
      "2025-05-15 08:58:11,652 - INFO - [2025-05-15 08:58:11] --- Entrenando Meta-Modelo (RIDGE) para Cluster 0 ---\n",
      "2025-05-15 08:58:11,657 - INFO - [2025-05-15 08:58:11] Meta-modelo (RIDGE) para cluster 0 entrenado.\n",
      "2025-05-15 08:58:11,659 - INFO - [2025-05-15 08:58:11] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_0.joblib\n",
      "2025-05-15 08:58:11,698 - INFO - Uso de memoria: 1.39 GB\n",
      "2025-05-15 08:58:11,699 - INFO - [2025-05-15 08:58:11] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:58:11,700 - INFO - [2025-05-15 08:58:11] Generando predicciones finales Stacking para cluster 0\n",
      "2025-05-15 08:58:11,703 - INFO - [2025-05-15 08:58:11] --- Evaluando Modelo Stacking para Cluster 0 ---\n",
      "2025-05-15 08:58:11,707 - INFO - [2025-05-15 08:58:11]   Stacking - MAE: 16.1939, RMSE: 29.5939, R2: 0.5312\n",
      "2025-05-15 08:58:12,739 - INFO - [2025-05-15 08:58:12] Predicciones para cluster 0 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_0.csv\n",
      "2025-05-15 08:58:12,739 - INFO - [2025-05-15 08:58:12] Generando gráficos para series del cluster 0\n",
      "2025-05-15 08:58:13,365 - INFO - [2025-05-15 08:58:13] Checkpoint actualizado a índice 0 (Cluster 0 procesado con éxito).\n",
      "2025-05-15 08:58:13,416 - INFO - [2025-05-15 08:58:13] --- Cluster 0 (índice 0) finalizado. Tiempo: 26.21 seg. ---\n",
      "2025-05-15 08:58:13,417 - INFO - Uso de memoria: 1.06 GB\n",
      "2025-05-15 08:58:13,417 - INFO - [2025-05-15 08:58:13] \n",
      "========== Procesando Cluster 2/7: ID = 1 ==========\n",
      "2025-05-15 08:58:13,418 - INFO - Uso de memoria: 1.06 GB\n",
      "2025-05-15 08:58:13,418 - INFO - [2025-05-15 08:58:13] \n",
      "--- Cargando y Preparando Datos para Cluster 1 ---\n",
      "2025-05-15 08:58:13,595 - INFO - [2025-05-15 08:58:13] Cluster 1: Total de 89 filas encontradas inicialmente.\n",
      "2025-05-15 08:58:14,397 - INFO - [2025-05-15 08:58:14] Cluster 1: 89 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:58:14,400 - INFO - Uso de memoria: 3.17 GB\n",
      "2025-05-15 08:58:14,406 - INFO - [2025-05-15 08:58:14] Manejo de NaNs en features derivadas: Se eliminaron 52 filas.\n",
      "2025-05-15 08:58:14,549 - INFO - Uso de memoria: 3.30 GB\n",
      "2025-05-15 08:58:14,550 - INFO - [2025-05-15 08:58:14] Cluster 1 Split: Train=25, Test=12. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:58:14,550 - INFO - [2025-05-15 08:58:14] --- Cargando Modelos Base y Generando Predicciones para Cluster 1 ---\n",
      "2025-05-15 08:58:14,550 - INFO - [2025-05-15 08:58:14] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_1.joblib\n",
      "2025-05-15 08:58:14,552 - INFO - [2025-05-15 08:58:14] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_1.joblib\n",
      "2025-05-15 08:58:14,554 - INFO - [2025-05-15 08:58:14] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 1\n",
      "2025-05-15 08:58:14,561 - INFO - [2025-05-15 08:58:14] Generando predicciones en datos de prueba para cluster 1\n",
      "2025-05-15 08:58:14,618 - INFO - Uso de memoria: 3.30 GB\n",
      "2025-05-15 08:58:14,619 - INFO - [2025-05-15 08:58:14] --- Entrenando Meta-Modelo (RIDGE) para Cluster 1 ---\n",
      "2025-05-15 08:58:14,620 - INFO - [2025-05-15 08:58:14] Meta-modelo (RIDGE) para cluster 1 entrenado.\n",
      "2025-05-15 08:58:14,622 - INFO - [2025-05-15 08:58:14] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_1.joblib\n",
      "2025-05-15 08:58:14,668 - INFO - Uso de memoria: 3.30 GB\n",
      "2025-05-15 08:58:14,668 - INFO - [2025-05-15 08:58:14] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:58:14,669 - INFO - [2025-05-15 08:58:14] Generando predicciones finales Stacking para cluster 1\n",
      "2025-05-15 08:58:14,670 - INFO - [2025-05-15 08:58:14] --- Evaluando Modelo Stacking para Cluster 1 ---\n",
      "2025-05-15 08:58:14,671 - INFO - [2025-05-15 08:58:14]   Stacking - MAE: 39878.7962, RMSE: 45117.7199, R2: -9.3078\n",
      "2025-05-15 08:58:14,674 - INFO - [2025-05-15 08:58:14] Predicciones para cluster 1 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_1.csv\n",
      "2025-05-15 08:58:14,674 - INFO - [2025-05-15 08:58:14] Generando gráficos para series del cluster 1\n",
      "2025-05-15 08:58:14,801 - INFO - [2025-05-15 08:58:14] Checkpoint actualizado a índice 1 (Cluster 1 procesado con éxito).\n",
      "2025-05-15 08:58:14,846 - INFO - [2025-05-15 08:58:14] --- Cluster 1 (índice 1) finalizado. Tiempo: 1.43 seg. ---\n",
      "2025-05-15 08:58:14,847 - INFO - Uso de memoria: 3.31 GB\n",
      "2025-05-15 08:58:14,847 - INFO - [2025-05-15 08:58:14] \n",
      "========== Procesando Cluster 3/7: ID = 2 ==========\n",
      "2025-05-15 08:58:14,848 - INFO - Uso de memoria: 3.31 GB\n",
      "2025-05-15 08:58:14,848 - INFO - [2025-05-15 08:58:14] \n",
      "--- Cargando y Preparando Datos para Cluster 2 ---\n",
      "2025-05-15 08:58:15,296 - INFO - [2025-05-15 08:58:15] Cluster 2: Total de 16151645 filas encontradas inicialmente.\n",
      "2025-05-15 08:58:15,298 - INFO - [2025-05-15 08:58:15] Limitando a ~1000000 filas (de 16151645) para control de memoria, seleccionando las más recientes.\n",
      "2025-05-15 08:58:17,340 - INFO - [2025-05-15 08:58:17] Se conservarán datos desde la fecha: 2024-09-16 00:00:00 para el cluster 2\n",
      "2025-05-15 08:58:25,227 - INFO - [2025-05-15 08:58:25] Filtrado por fecha en pandas: 1077593 filas mantenidas desde 2024-09-16 00:00:00\n",
      "2025-05-15 08:58:25,232 - INFO - [2025-05-15 08:58:25] Cluster 2: 1077593 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:58:25,233 - INFO - Uso de memoria: 0.25 GB\n",
      "2025-05-15 08:58:25,713 - INFO - [2025-05-15 08:58:25] Manejo de NaNs en features derivadas: Se eliminaron 15015 filas.\n",
      "2025-05-15 08:58:26,129 - INFO - Uso de memoria: 1.94 GB\n",
      "2025-05-15 08:58:26,130 - INFO - [2025-05-15 08:58:26] Cluster 2 Split: Train=359361, Test=703217. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:58:26,144 - INFO - [2025-05-15 08:58:26] --- Cargando Modelos Base y Generando Predicciones para Cluster 2 ---\n",
      "2025-05-15 08:58:26,145 - INFO - [2025-05-15 08:58:26] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_2.joblib\n",
      "2025-05-15 08:58:26,157 - INFO - [2025-05-15 08:58:26] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_2.joblib\n",
      "2025-05-15 08:58:26,167 - INFO - [2025-05-15 08:58:26] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 2\n",
      "2025-05-15 08:58:28,546 - INFO - [2025-05-15 08:58:28] Generando predicciones en datos de prueba para cluster 2\n",
      "2025-05-15 08:58:28,546 - INFO - [2025-05-15 08:58:28] Prediciendo en lotes (chunk_size=5000) para Test (Cluster 2)\n",
      "2025-05-15 08:58:57,392 - INFO - Uso de memoria: 1.49 GB\n",
      "2025-05-15 08:58:57,392 - INFO - [2025-05-15 08:58:57] --- Entrenando Meta-Modelo (RIDGE) para Cluster 2 ---\n",
      "2025-05-15 08:58:57,397 - INFO - [2025-05-15 08:58:57] Meta-modelo (RIDGE) para cluster 2 entrenado.\n",
      "2025-05-15 08:58:57,398 - INFO - [2025-05-15 08:58:57] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_2.joblib\n",
      "2025-05-15 08:58:57,441 - INFO - Uso de memoria: 1.50 GB\n",
      "2025-05-15 08:58:57,442 - INFO - [2025-05-15 08:58:57] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:58:57,444 - INFO - [2025-05-15 08:58:57] Generando predicciones finales Stacking para cluster 2\n",
      "2025-05-15 08:58:57,447 - INFO - [2025-05-15 08:58:57] --- Evaluando Modelo Stacking para Cluster 2 ---\n",
      "2025-05-15 08:58:57,453 - INFO - [2025-05-15 08:58:57]   Stacking - MAE: 7.6576, RMSE: 21.1094, R2: 0.4387\n",
      "2025-05-15 08:58:58,849 - INFO - [2025-05-15 08:58:58] Predicciones para cluster 2 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_2.csv\n",
      "2025-05-15 08:58:58,849 - INFO - [2025-05-15 08:58:58] Generando gráficos para series del cluster 2\n",
      "2025-05-15 08:58:59,488 - INFO - [2025-05-15 08:58:59] Checkpoint actualizado a índice 2 (Cluster 2 procesado con éxito).\n",
      "2025-05-15 08:58:59,546 - INFO - [2025-05-15 08:58:59] --- Cluster 2 (índice 2) finalizado. Tiempo: 44.70 seg. ---\n",
      "2025-05-15 08:58:59,547 - INFO - Uso de memoria: 1.10 GB\n",
      "2025-05-15 08:58:59,547 - INFO - [2025-05-15 08:58:59] \n",
      "========== Procesando Cluster 4/7: ID = 3 ==========\n",
      "2025-05-15 08:58:59,547 - INFO - Uso de memoria: 1.10 GB\n",
      "2025-05-15 08:58:59,548 - INFO - [2025-05-15 08:58:59] \n",
      "--- Cargando y Preparando Datos para Cluster 3 ---\n",
      "2025-05-15 08:58:59,719 - INFO - [2025-05-15 08:58:59] Cluster 3: Total de 1728 filas encontradas inicialmente.\n",
      "2025-05-15 08:59:00,324 - INFO - [2025-05-15 08:59:00] Cluster 3: 1728 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:59:00,326 - INFO - Uso de memoria: 3.64 GB\n",
      "2025-05-15 08:59:00,329 - INFO - [2025-05-15 08:59:00] Manejo de NaNs en features derivadas: Se eliminaron 780 filas.\n",
      "2025-05-15 08:59:00,398 - INFO - Uso de memoria: 3.64 GB\n",
      "2025-05-15 08:59:00,399 - INFO - [2025-05-15 08:59:00] Cluster 3 Split: Train=839, Test=109. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:59:00,399 - INFO - [2025-05-15 08:59:00] --- Cargando Modelos Base y Generando Predicciones para Cluster 3 ---\n",
      "2025-05-15 08:59:00,400 - INFO - [2025-05-15 08:59:00] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_3.joblib\n",
      "2025-05-15 08:59:00,402 - INFO - [2025-05-15 08:59:00] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_3.joblib\n",
      "2025-05-15 08:59:00,404 - INFO - [2025-05-15 08:59:00] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 3\n",
      "2025-05-15 08:59:00,413 - INFO - [2025-05-15 08:59:00] Generando predicciones en datos de prueba para cluster 3\n",
      "2025-05-15 08:59:00,470 - INFO - Uso de memoria: 3.64 GB\n",
      "2025-05-15 08:59:00,470 - INFO - [2025-05-15 08:59:00] --- Entrenando Meta-Modelo (RIDGE) para Cluster 3 ---\n",
      "2025-05-15 08:59:00,472 - INFO - [2025-05-15 08:59:00] Meta-modelo (RIDGE) para cluster 3 entrenado.\n",
      "2025-05-15 08:59:00,473 - INFO - [2025-05-15 08:59:00] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_3.joblib\n",
      "2025-05-15 08:59:00,520 - INFO - Uso de memoria: 3.64 GB\n",
      "2025-05-15 08:59:00,521 - INFO - [2025-05-15 08:59:00] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:59:00,522 - INFO - [2025-05-15 08:59:00] Generando predicciones finales Stacking para cluster 3\n",
      "2025-05-15 08:59:00,522 - INFO - [2025-05-15 08:59:00] --- Evaluando Modelo Stacking para Cluster 3 ---\n",
      "2025-05-15 08:59:00,523 - INFO - [2025-05-15 08:59:00]   Stacking - MAE: 2649.7729, RMSE: 3883.7142, R2: 0.5740\n",
      "2025-05-15 08:59:00,526 - INFO - [2025-05-15 08:59:00] Predicciones para cluster 3 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_3.csv\n",
      "2025-05-15 08:59:00,527 - INFO - [2025-05-15 08:59:00] Generando gráficos para series del cluster 3\n",
      "2025-05-15 08:59:01,196 - INFO - [2025-05-15 08:59:01] Checkpoint actualizado a índice 3 (Cluster 3 procesado con éxito).\n",
      "2025-05-15 08:59:01,249 - INFO - [2025-05-15 08:59:01] --- Cluster 3 (índice 3) finalizado. Tiempo: 1.70 seg. ---\n",
      "2025-05-15 08:59:01,250 - INFO - Uso de memoria: 3.64 GB\n",
      "2025-05-15 08:59:01,250 - INFO - [2025-05-15 08:59:01] \n",
      "========== Procesando Cluster 5/7: ID = 4 ==========\n",
      "2025-05-15 08:59:01,250 - INFO - Uso de memoria: 3.64 GB\n",
      "2025-05-15 08:59:01,251 - INFO - [2025-05-15 08:59:01] \n",
      "--- Cargando y Preparando Datos para Cluster 4 ---\n",
      "2025-05-15 08:59:01,369 - INFO - [2025-05-15 08:59:01] Cluster 4: Total de 27984 filas encontradas inicialmente.\n",
      "2025-05-15 08:59:02,003 - INFO - [2025-05-15 08:59:02] Cluster 4: 27984 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:59:02,005 - INFO - Uso de memoria: 3.79 GB\n",
      "2025-05-15 08:59:02,018 - INFO - [2025-05-15 08:59:02] Manejo de NaNs en features derivadas: Se eliminaron 11388 filas.\n",
      "2025-05-15 08:59:02,078 - INFO - Uso de memoria: 3.82 GB\n",
      "2025-05-15 08:59:02,078 - INFO - [2025-05-15 08:59:02] Cluster 4 Split: Train=15008, Test=1588. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:59:02,079 - INFO - [2025-05-15 08:59:02] --- Cargando Modelos Base y Generando Predicciones para Cluster 4 ---\n",
      "2025-05-15 08:59:02,080 - INFO - [2025-05-15 08:59:02] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_4.joblib\n",
      "2025-05-15 08:59:02,082 - INFO - [2025-05-15 08:59:02] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_4.joblib\n",
      "2025-05-15 08:59:02,084 - INFO - [2025-05-15 08:59:02] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 4\n",
      "2025-05-15 08:59:02,155 - INFO - [2025-05-15 08:59:02] Generando predicciones en datos de prueba para cluster 4\n",
      "2025-05-15 08:59:02,221 - INFO - Uso de memoria: 3.90 GB\n",
      "2025-05-15 08:59:02,222 - INFO - [2025-05-15 08:59:02] --- Entrenando Meta-Modelo (RIDGE) para Cluster 4 ---\n",
      "2025-05-15 08:59:02,224 - INFO - [2025-05-15 08:59:02] Meta-modelo (RIDGE) para cluster 4 entrenado.\n",
      "2025-05-15 08:59:02,225 - INFO - [2025-05-15 08:59:02] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_4.joblib\n",
      "2025-05-15 08:59:02,276 - INFO - Uso de memoria: 3.90 GB\n",
      "2025-05-15 08:59:02,276 - INFO - [2025-05-15 08:59:02] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:59:02,277 - INFO - [2025-05-15 08:59:02] Generando predicciones finales Stacking para cluster 4\n",
      "2025-05-15 08:59:02,278 - INFO - [2025-05-15 08:59:02] --- Evaluando Modelo Stacking para Cluster 4 ---\n",
      "2025-05-15 08:59:02,279 - INFO - [2025-05-15 08:59:02]   Stacking - MAE: 466.4223, RMSE: 989.6514, R2: 0.4562\n",
      "2025-05-15 08:59:02,285 - INFO - [2025-05-15 08:59:02] Predicciones para cluster 4 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_4.csv\n",
      "2025-05-15 08:59:02,285 - INFO - [2025-05-15 08:59:02] Generando gráficos para series del cluster 4\n",
      "2025-05-15 08:59:02,964 - INFO - [2025-05-15 08:59:02] Checkpoint actualizado a índice 4 (Cluster 4 procesado con éxito).\n",
      "2025-05-15 08:59:03,028 - INFO - [2025-05-15 08:59:03] --- Cluster 4 (índice 4) finalizado. Tiempo: 1.78 seg. ---\n",
      "2025-05-15 08:59:03,028 - INFO - Uso de memoria: 3.91 GB\n",
      "2025-05-15 08:59:03,029 - INFO - [2025-05-15 08:59:03] \n",
      "========== Procesando Cluster 6/7: ID = 5 ==========\n",
      "2025-05-15 08:59:03,029 - INFO - Uso de memoria: 3.91 GB\n",
      "2025-05-15 08:59:03,029 - INFO - [2025-05-15 08:59:03] \n",
      "--- Cargando y Preparando Datos para Cluster 5 ---\n",
      "2025-05-15 08:59:03,163 - INFO - [2025-05-15 08:59:03] Cluster 5: Total de 849750 filas encontradas inicialmente.\n",
      "2025-05-15 08:59:03,733 - INFO - [2025-05-15 08:59:03] Cluster 5: 849750 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:59:03,733 - INFO - Uso de memoria: 4.58 GB\n",
      "2025-05-15 08:59:04,138 - INFO - [2025-05-15 08:59:04] Manejo de NaNs en features derivadas: Se eliminaron 462696 filas.\n",
      "2025-05-15 08:59:04,318 - INFO - Uso de memoria: 4.24 GB\n",
      "2025-05-15 08:59:04,318 - INFO - [2025-05-15 08:59:04] Cluster 5 Split: Train=364355, Test=22699. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:59:04,323 - INFO - [2025-05-15 08:59:04] --- Cargando Modelos Base y Generando Predicciones para Cluster 5 ---\n",
      "2025-05-15 08:59:04,324 - INFO - [2025-05-15 08:59:04] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_5.joblib\n",
      "2025-05-15 08:59:04,327 - INFO - [2025-05-15 08:59:04] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_5.joblib\n",
      "2025-05-15 08:59:04,329 - INFO - [2025-05-15 08:59:04] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 5\n",
      "2025-05-15 08:59:05,441 - INFO - [2025-05-15 08:59:05] Generando predicciones en datos de prueba para cluster 5\n",
      "2025-05-15 08:59:05,442 - INFO - [2025-05-15 08:59:05] Prediciendo en lotes (chunk_size=5000) para Test (Cluster 5)\n",
      "2025-05-15 08:59:05,650 - INFO - Uso de memoria: 3.97 GB\n",
      "2025-05-15 08:59:05,651 - INFO - [2025-05-15 08:59:05] --- Entrenando Meta-Modelo (RIDGE) para Cluster 5 ---\n",
      "2025-05-15 08:59:05,655 - INFO - [2025-05-15 08:59:05] Meta-modelo (RIDGE) para cluster 5 entrenado.\n",
      "2025-05-15 08:59:05,656 - INFO - [2025-05-15 08:59:05] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_5.joblib\n",
      "2025-05-15 08:59:05,705 - INFO - Uso de memoria: 3.97 GB\n",
      "2025-05-15 08:59:05,706 - INFO - [2025-05-15 08:59:05] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:59:05,707 - INFO - [2025-05-15 08:59:05] Generando predicciones finales Stacking para cluster 5\n",
      "2025-05-15 08:59:05,708 - INFO - [2025-05-15 08:59:05] --- Evaluando Modelo Stacking para Cluster 5 ---\n",
      "2025-05-15 08:59:05,710 - INFO - [2025-05-15 08:59:05]   Stacking - MAE: 2.6126, RMSE: 22.6697, R2: 0.1029\n",
      "2025-05-15 08:59:05,756 - INFO - [2025-05-15 08:59:05] Predicciones para cluster 5 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_5.csv\n",
      "2025-05-15 08:59:05,757 - INFO - [2025-05-15 08:59:05] Generando gráficos para series del cluster 5\n",
      "2025-05-15 08:59:06,413 - INFO - [2025-05-15 08:59:06] Checkpoint actualizado a índice 5 (Cluster 5 procesado con éxito).\n",
      "2025-05-15 08:59:06,470 - INFO - [2025-05-15 08:59:06] --- Cluster 5 (índice 5) finalizado. Tiempo: 3.44 seg. ---\n",
      "2025-05-15 08:59:06,470 - INFO - Uso de memoria: 3.81 GB\n",
      "2025-05-15 08:59:06,470 - INFO - [2025-05-15 08:59:06] \n",
      "========== Procesando Cluster 7/7: ID = 6 ==========\n",
      "2025-05-15 08:59:06,471 - INFO - Uso de memoria: 3.81 GB\n",
      "2025-05-15 08:59:06,471 - INFO - [2025-05-15 08:59:06] \n",
      "--- Cargando y Preparando Datos para Cluster 6 ---\n",
      "2025-05-15 08:59:06,604 - INFO - [2025-05-15 08:59:06] Cluster 6: Total de 941142 filas encontradas inicialmente.\n",
      "2025-05-15 08:59:07,150 - INFO - [2025-05-15 08:59:07] Cluster 6: 941142 filas cargadas efectivamente para procesamiento.\n",
      "2025-05-15 08:59:07,151 - INFO - Uso de memoria: 4.68 GB\n",
      "2025-05-15 08:59:07,608 - INFO - [2025-05-15 08:59:07] Manejo de NaNs en features derivadas: Se eliminaron 375388 filas.\n",
      "2025-05-15 08:59:07,844 - INFO - Uso de memoria: 4.35 GB\n",
      "2025-05-15 08:59:07,845 - INFO - [2025-05-15 08:59:07] Cluster 6 Split: Train=501226, Test=64528. Features categóricas identificadas: ['establecimiento', 'material', 'year', 'month', 'week_of_year', 'has_promo', 'is_covid_period', 'is_holiday_exact_date', 'is_holiday_in_week']\n",
      "2025-05-15 08:59:07,852 - INFO - [2025-05-15 08:59:07] --- Cargando Modelos Base y Generando Predicciones para Cluster 6 ---\n",
      "2025-05-15 08:59:07,852 - INFO - [2025-05-15 08:59:07] Cargando modelo LightGBM desde: ../models/stacking/trained_models/lgbm_base_cluster_6.joblib\n",
      "2025-05-15 08:59:07,857 - INFO - [2025-05-15 08:59:07] Cargando modelo Random Forest desde: ../models/stacking/trained_models/rf_base_cluster_6.joblib\n",
      "2025-05-15 08:59:07,861 - INFO - [2025-05-15 08:59:07] Generando predicciones (tipo OOF) en datos de entrenamiento para cluster 6\n",
      "2025-05-15 08:59:11,422 - INFO - [2025-05-15 08:59:11] Generando predicciones en datos de prueba para cluster 6\n",
      "2025-05-15 08:59:11,422 - INFO - [2025-05-15 08:59:11] Prediciendo en lotes (chunk_size=5000) para Test (Cluster 6)\n",
      "2025-05-15 08:59:12,196 - INFO - Uso de memoria: 3.88 GB\n",
      "2025-05-15 08:59:12,196 - INFO - [2025-05-15 08:59:12] --- Entrenando Meta-Modelo (RIDGE) para Cluster 6 ---\n",
      "2025-05-15 08:59:12,202 - INFO - [2025-05-15 08:59:12] Meta-modelo (RIDGE) para cluster 6 entrenado.\n",
      "2025-05-15 08:59:12,203 - INFO - [2025-05-15 08:59:12] Meta-modelo guardado en: ../models/stacking_preloaded_v3/trained_meta_models/meta_model_ridge_cluster_6.joblib\n",
      "2025-05-15 08:59:12,255 - INFO - Uso de memoria: 3.89 GB\n",
      "2025-05-15 08:59:12,256 - INFO - [2025-05-15 08:59:12] Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\n",
      "2025-05-15 08:59:12,257 - INFO - [2025-05-15 08:59:12] Generando predicciones finales Stacking para cluster 6\n",
      "2025-05-15 08:59:12,258 - INFO - [2025-05-15 08:59:12] --- Evaluando Modelo Stacking para Cluster 6 ---\n",
      "2025-05-15 08:59:12,259 - INFO - [2025-05-15 08:59:12]   Stacking - MAE: 76.1940, RMSE: 151.6435, R2: 0.4482\n",
      "2025-05-15 08:59:12,389 - INFO - [2025-05-15 08:59:12] Predicciones para cluster 6 guardadas en: ../models/stacking_preloaded_v3/predictions/stacking_preds_cluster_6.csv\n",
      "2025-05-15 08:59:12,390 - INFO - [2025-05-15 08:59:12] Generando gráficos para series del cluster 6\n",
      "2025-05-15 08:59:13,073 - INFO - [2025-05-15 08:59:13] Checkpoint actualizado a índice 6 (Cluster 6 procesado con éxito).\n",
      "2025-05-15 08:59:13,142 - INFO - [2025-05-15 08:59:13] --- Cluster 6 (índice 6) finalizado. Tiempo: 6.67 seg. ---\n",
      "2025-05-15 08:59:13,142 - INFO - Uso de memoria: 3.65 GB\n",
      "2025-05-15 08:59:13,143 - INFO - [2025-05-15 08:59:13] \n",
      "====================== PROCESAMIENTO DE TODOS LOS CLUSTERS COMPLETADO ======================\n",
      "2025-05-15 08:59:13,143 - INFO - [2025-05-15 08:59:13] Tiempo total de ejecución del script: 1.44 minutos (0.02 horas).\n",
      "2025-05-15 08:59:13,144 - INFO - [2025-05-15 08:59:13] Total de clusters configurados: 7\n",
      "2025-05-15 08:59:13,144 - INFO - [2025-05-15 08:59:13] Clusters procesados con éxito: 7\n",
      "2025-05-15 08:59:13,144 - INFO - [2025-05-15 08:59:13] Clusters saltados o fallidos: 0\n",
      "2025-05-15 08:59:13,145 - INFO - [2025-05-15 08:59:13] Resultados guardados en el directorio base: ../models/stacking_preloaded_v3\n",
      "2025-05-15 08:59:13,145 - INFO - [2025-05-15 08:59:13] Métricas consolidadas por cluster en: ../models/stacking_preloaded_v3/all_clusters_metrics.csv\n",
      "2025-05-15 08:59:13,147 - INFO - [2025-05-15 08:59:13] \n",
      "--- Resumen Métricas Finales Agregadas (sobre clusters procesados exitosamente) ---\n",
      "2025-05-15 08:59:13,148 - INFO - [2025-05-15 08:59:13] MAE Promedio Stacking: 6156.8071\n",
      "2025-05-15 08:59:13,148 - INFO - [2025-05-15 08:59:13] RMSE Promedio Stacking: 7173.7288\n",
      "2025-05-15 08:59:13,148 - INFO - [2025-05-15 08:59:13] R2 Promedio Stacking: -0.9652\n",
      "2025-05-15 08:59:13,149 - INFO - [2025-05-15 08:59:13] Todos los clusters han sido abordados. Eliminando archivo checkpoint.\n",
      "2025-05-15 08:59:13,149 - INFO - [2025-05-15 08:59:13] ========================== SCRIPT DE STACKING FINALIZADO ==========================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "master_start_time = time.time()\n",
    "log_progress(\"================ INICIO DEL PROCESAMIENTO DE STACKING ================\")\n",
    "\n",
    "try:\n",
    "    log_progress(f\"Leyendo columna de clusters '{CLUSTER_COL}' del archivo Parquet: {PARQUET_FILE_PATH}\")\n",
    "    # Leer solo la columna de clusters para eficiencia\n",
    "    cluster_labels_df = pd.read_parquet(PARQUET_FILE_PATH, columns=[CLUSTER_COL])\n",
    "    unique_clusters = sorted([c for c in cluster_labels_df[CLUSTER_COL].unique() if pd.notna(c)])\n",
    "    del cluster_labels_df\n",
    "    gc.collect()\n",
    "    if not unique_clusters:\n",
    "        log_progress(\"No se encontraron clusters únicos en el archivo. Terminando script.\")\n",
    "        exit()\n",
    "    log_progress(f\"Encontrados {len(unique_clusters)} clusters únicos para procesar: {unique_clusters}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error crítico al leer la columna de clusters del Parquet: {e}\", exc_info=True)\n",
    "    exit()\n",
    "\n",
    "processed_clusters_metrics = []\n",
    "clusters_successfully_processed_count = 0\n",
    "clusters_skipped_or_failed_count = 0\n",
    "\n",
    "# Crear archivo de métricas con encabezado si no existe o está vacío\n",
    "if not os.path.exists(METRICS_FILE) or os.path.getsize(METRICS_FILE) == 0:\n",
    "    pd.DataFrame(columns=['cluster_id', 'Stacking_mae', 'Stacking_rmse', 'Stacking_r2']).to_csv(METRICS_FILE, index=False)\n",
    "    log_progress(f\"Archivo de métricas {METRICS_FILE} creado con encabezado.\")\n",
    "\n",
    "# Lógica de Checkpoint para reanudar\n",
    "last_successfully_processed_cluster_idx = -1\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    try:\n",
    "        with open(CHECKPOINT_FILE, 'r') as f_check:\n",
    "            checkpoint_content = f_check.read().strip()\n",
    "            if checkpoint_content:\n",
    "                last_successfully_processed_cluster_idx = int(checkpoint_content)\n",
    "                log_progress(f\"Reanudando desde checkpoint: último índice de clúster procesado con éxito = {last_successfully_processed_cluster_idx}\")\n",
    "    except Exception as e_check:\n",
    "        logger.error(f\"Error leyendo archivo de checkpoint ({CHECKPOINT_FILE}): {e_check}. Se procesarán todos los clusters.\", exc_info=True)\n",
    "        last_successfully_processed_cluster_idx = -1 # Resetear si hay error\n",
    "\n",
    "# --- Bucle Principal por Cluster ---\n",
    "for current_cluster_idx, cluster_id_val in enumerate(unique_clusters):\n",
    "    if current_cluster_idx <= last_successfully_processed_cluster_idx:\n",
    "        log_progress(f\"Saltando Clúster {cluster_id_val} (índice {current_cluster_idx}), ya procesado según checkpoint.\")\n",
    "        continue\n",
    "\n",
    "    cluster_loop_start_time = time.time()\n",
    "    log_progress(f\"\\n========== Procesando Cluster {current_cluster_idx + 1}/{len(unique_clusters)}: ID = {cluster_id_val} ==========\")\n",
    "    log_memory_usage()\n",
    "    \n",
    "    cluster_processed_flag = False\n",
    "    try:\n",
    "        # 1. Cargar y preparar datos para el cluster\n",
    "        X_train_c, y_train_c, X_test_c, y_test_c, cat_features_c, dates_test_c, ids_test_c = \\\n",
    "            load_and_prepare_cluster_data(\n",
    "                PARQUET_FILE_PATH, cluster_id_val, TARGET_COL, DATE_COL, CLUSTER_COL,\n",
    "                SERIES_ID_COLS, N_TEST_WEEKS, max_rows=MAX_ROWS_PER_CLUSTER\n",
    "            )\n",
    "\n",
    "        if X_train_c is None or X_test_c is None or y_train_c is None or y_test_c is None:\n",
    "            log_progress(f\"Datos insuficientes o error en carga para cluster {cluster_id_val}. Saltando.\")\n",
    "            clusters_skipped_or_failed_count += 1\n",
    "            # Guardar checkpoint para marcar este cluster como \"intentado\" y no reintentar si falla consistentemente\n",
    "            with open(CHECKPOINT_FILE, 'w') as f_checkpoint_skip:\n",
    "                f_checkpoint_skip.write(str(current_cluster_idx))\n",
    "            continue\n",
    "\n",
    "        # 2. Generar predicciones de modelos base preentrenados\n",
    "        oof_lgbm, oof_rf, test_lgbm, test_rf = generate_base_model_predictions_cluster(\n",
    "            X_train_c, X_test_c, cluster_id_val, PRETRAINED_MODELS_DIR,\n",
    "            BASE_LGBM_MODEL_NAME_TEMPLATE, BASE_RF_MODEL_NAME_TEMPLATE, cat_features_c\n",
    "        )\n",
    "\n",
    "        if oof_lgbm is None or oof_rf is None or test_lgbm is None or test_rf is None:\n",
    "            log_progress(f\"Fallo en generación de predicciones base para cluster {cluster_id_val}. Saltando.\")\n",
    "            clusters_skipped_or_failed_count += 1\n",
    "            with open(CHECKPOINT_FILE, 'w') as f_checkpoint_skip:\n",
    "                f_checkpoint_skip.write(str(current_cluster_idx))\n",
    "            continue\n",
    "\n",
    "        # 3. Entrenar meta-modelo\n",
    "        meta_model_trained = train_meta_model_cluster(\n",
    "            X_train_c, oof_lgbm, oof_rf, y_train_c, cluster_id_val, META_MODEL_TYPE, MODELS_DIR\n",
    "        )\n",
    "\n",
    "        if meta_model_trained is None:\n",
    "            log_progress(f\"Fallo en entrenamiento del meta-modelo para cluster {cluster_id_val}. Saltando.\")\n",
    "            clusters_skipped_or_failed_count += 1\n",
    "            with open(CHECKPOINT_FILE, 'w') as f_checkpoint_skip:\n",
    "                f_checkpoint_skip.write(str(current_cluster_idx))\n",
    "            continue\n",
    "\n",
    "        # 4. Preparar datos de Test para el Meta-Modelo y Predecir\n",
    "        log_progress(\"Construyendo X_meta_test_final usando SOLO las predicciones de prueba de los modelos base.\")\n",
    "        X_meta_test_final = pd.DataFrame({\n",
    "            'oof_lgbm_pred_meta': test_lgbm, # Usar el mismo nombre que en entrenamiento del meta\n",
    "            'oof_rf_pred_meta': test_rf    # Usar el mismo nombre que en entrenamiento del meta\n",
    "        }, index=X_test_c.index) # Mantener el índice de X_test_c para posible alineación futura si es necesario\n",
    "        # Aplicar transformaciones a X_meta_test_final si es necesario (ej. get_dummies, reindexar)\n",
    "        # Esto debe reflejar exactamente lo que se hizo en train_meta_model_cluster\n",
    "        if META_MODEL_TYPE == 'ridge' or META_MODEL_TYPE == 'rf':\n",
    "            cat_cols_meta_test = X_meta_test_final.select_dtypes(include='category').columns\n",
    "            if not cat_cols_meta_test.empty:\n",
    "                X_meta_test_final = pd.get_dummies(X_meta_test_final, columns=cat_cols_meta_test, dummy_na=False, \n",
    "                                                    sparse=(META_MODEL_TYPE == 'ridge')) # sparse solo para ridge si se usó\n",
    "            \n",
    "            # Reindexar para asegurar consistencia de columnas con el entrenamiento\n",
    "            if hasattr(meta_model_trained, 'feature_names_in_'):\n",
    "                train_cols = meta_model_trained.feature_names_in_\n",
    "                X_meta_test_final = X_meta_test_final.reindex(columns=train_cols, fill_value=0)\n",
    "            else:\n",
    "                logger.warning(f\"Meta-modelo ({META_MODEL_TYPE}) no tiene 'feature_names_in_'. No se puede reindexar X_meta_test_final para cluster {cluster_id_val}.\")\n",
    "        \n",
    "        # Para LGBM meta-modelo, las categóricas ya deberían estar como 'category'\n",
    "        # No se necesita get_dummies, pero asegurar que los tipos son correctos.\n",
    "        \n",
    "        # Imputar NaNs que pudieron surgir de reindexación o por otras razones\n",
    "        if isinstance(X_meta_test_final, pd.DataFrame):\n",
    "            X_meta_test_final.fillna(0, inplace=True) # Imputación simple\n",
    "        elif hasattr(X_meta_test_final, 'fillna'): # Pandas SparseDataFrame\n",
    "                X_meta_test_final = X_meta_test_final.fillna(0)\n",
    "\n",
    "\n",
    "        log_progress(f\"Generando predicciones finales Stacking para cluster {cluster_id_val}\")\n",
    "        stacking_final_predictions = meta_model_trained.predict(X_meta_test_final)\n",
    "        stacking_final_predictions = np.maximum(0, stacking_final_predictions) # Asegurar no-negatividad\n",
    "\n",
    "        # 5. Evaluar y Guardar Resultados\n",
    "        cluster_metrics = evaluate_model_cluster(y_test_c.values, stacking_final_predictions, cluster_id_val)\n",
    "        processed_clusters_metrics.append(cluster_metrics)\n",
    "        pd.DataFrame([cluster_metrics]).to_csv(METRICS_FILE, mode='a', header=False, index=False)\n",
    "\n",
    "        # Guardar predicciones detalladas\n",
    "        preds_df = pd.DataFrame({\n",
    "            'establecimiento': ids_test_c['establecimiento'], 'material': ids_test_c['material'],\n",
    "            'week': dates_test_c, 'actual_volume': y_test_c.values,\n",
    "            'stacking_predicted_volume': stacking_final_predictions, 'cluster_label': cluster_id_val\n",
    "        }).sort_values(by=SERIES_ID_COLS + ['week']).reset_index(drop=True)\n",
    "        \n",
    "        pred_file_path = os.path.join(PREDICTIONS_DIR, f'stacking_preds_cluster_{cluster_id_val}.csv')\n",
    "        preds_df.to_csv(pred_file_path, index=False)\n",
    "        log_progress(f\"Predicciones para cluster {cluster_id_val} guardadas en: {pred_file_path}\")\n",
    "\n",
    "        # 6. Generar Gráficos (para una muestra de series)\n",
    "        log_progress(f\"Generando gráficos para series del cluster {cluster_id_val}\")\n",
    "        unique_series_to_plot = ids_test_c.drop_duplicates().head(min(5, len(ids_test_c))).values.tolist() # Max 5 plots\n",
    "        for series_components in unique_series_to_plot:\n",
    "            estab_val, mat_val = series_components[0], series_components[1]\n",
    "            series_mask = (preds_df['establecimiento'] == estab_val) & (preds_df['material'] == mat_val)\n",
    "            series_data_for_plot = preds_df[series_mask]\n",
    "            if not series_data_for_plot.empty:\n",
    "                plot_file = os.path.join(PLOTS_DIR, f'plot_est{estab_val}_mat{mat_val}_clust{cluster_id_val}.png')\n",
    "                plot_predictions_series(\n",
    "                    series_data_for_plot['week'], series_data_for_plot['actual_volume'],\n",
    "                    series_data_for_plot['stacking_predicted_volume'],\n",
    "                    estab_val, mat_val, cluster_id_val, plot_file\n",
    "                )\n",
    "        cluster_processed_flag = True # Marcador de éxito para este clúster\n",
    "\n",
    "    except Exception as e_cluster_loop:\n",
    "        logger.error(f\"ERROR INESPERADO procesando cluster {cluster_id_val} (índice {current_cluster_idx}): {e_cluster_loop}\", exc_info=True)\n",
    "        clusters_skipped_or_failed_count += 1\n",
    "    finally:\n",
    "        # Actualizar checkpoint\n",
    "        if cluster_processed_flag:\n",
    "            clusters_successfully_processed_count += 1\n",
    "            with open(CHECKPOINT_FILE, 'w') as f_checkpoint:\n",
    "                f_checkpoint.write(str(current_cluster_idx))\n",
    "            log_progress(f\"Checkpoint actualizado a índice {current_cluster_idx} (Cluster {cluster_id_val} procesado con éxito).\")\n",
    "        # else: # Si falló, el checkpoint ya se guardó dentro del try o no se guarda para reintentar la próxima vez\n",
    "        #    log_progress(f\"Cluster {cluster_id_val} (índice {current_cluster_idx}) no se procesó con éxito. Checkpoint no avanzado por esta razón.\")\n",
    "\n",
    "\n",
    "        # Limpieza de memoria al final de cada iteración del bucle del clúster\n",
    "        vars_to_del_loop = ['X_train_c', 'y_train_c', 'X_test_c', 'y_test_c', 'cat_features_c', \n",
    "                            'dates_test_c', 'ids_test_c', 'oof_lgbm', 'oof_rf', 'test_lgbm', 'test_rf',\n",
    "                            'meta_model_trained', 'X_meta_test_final', 'stacking_final_predictions', 'preds_df']\n",
    "        for var_name_del in vars_to_del_loop:\n",
    "            if var_name_del in locals():\n",
    "                del locals()[var_name_del]\n",
    "        gc.collect()\n",
    "        \n",
    "        cluster_loop_time = time.time() - cluster_loop_start_time\n",
    "        log_progress(f\"--- Cluster {cluster_id_val} (índice {current_cluster_idx}) finalizado. Tiempo: {cluster_loop_time:.2f} seg. ---\")\n",
    "        log_memory_usage()\n",
    "\n",
    "        # Estimación de tiempo restante\n",
    "        # current_processed_in_this_run = (current_cluster_idx - last_successfully_processed_cluster_idx)\n",
    "        # if current_processed_in_this_run > 0:\n",
    "        #    time_elapsed_this_run = time.time() - master_start_time # O un timer específico para esta corrida\n",
    "        #    avg_time_per_cluster_this_run = time_elapsed_this_run / current_processed_in_this_run\n",
    "        #    remaining_clusters = len(unique_clusters) - (current_cluster_idx + 1)\n",
    "        #    est_time_left = avg_time_per_cluster_this_run * remaining_clusters\n",
    "        #    log_progress(f\"Tiempo promedio por cluster (esta ejecución): {avg_time_per_cluster_this_run:.2f} seg.\")\n",
    "        #    log_progress(f\"Tiempo restante estimado: {est_time_left / 60:.2f} min / {est_time_left / 3600:.2f} hrs.\")\n",
    "\n",
    "\n",
    "# --- Finalización del Script ---\n",
    "master_total_time = time.time() - master_start_time\n",
    "log_progress(\"\\n====================== PROCESAMIENTO DE TODOS LOS CLUSTERS COMPLETADO ======================\")\n",
    "log_progress(f\"Tiempo total de ejecución del script: {master_total_time / 60:.2f} minutos ({master_total_time / 3600:.2f} horas).\")\n",
    "log_progress(f\"Total de clusters configurados: {len(unique_clusters)}\")\n",
    "log_progress(f\"Clusters procesados con éxito: {clusters_successfully_processed_count}\")\n",
    "log_progress(f\"Clusters saltados o fallidos: {clusters_skipped_or_failed_count}\")\n",
    "\n",
    "log_progress(f\"Resultados guardados en el directorio base: {OUTPUT_DIR}\")\n",
    "log_progress(f\"Métricas consolidadas por cluster en: {METRICS_FILE}\")\n",
    "\n",
    "try:\n",
    "    if os.path.exists(METRICS_FILE) and os.path.getsize(METRICS_FILE) > 0:\n",
    "        final_metrics_df = pd.read_csv(METRICS_FILE)\n",
    "        if not final_metrics_df.empty and 'Stacking_mae' in final_metrics_df.columns and 'Stacking_rmse' in final_metrics_df.columns and 'Stacking_r2' in final_metrics_df.columns:\n",
    "            log_progress(\"\\n--- Resumen Métricas Finales Agregadas (sobre clusters procesados exitosamente) ---\")\n",
    "            log_progress(f\"MAE Promedio Stacking: {final_metrics_df['Stacking_mae'].mean():.4f}\")\n",
    "            log_progress(f\"RMSE Promedio Stacking: {final_metrics_df['Stacking_rmse'].mean():.4f}\")\n",
    "            log_progress(f\"R2 Promedio Stacking: {final_metrics_df['Stacking_r2'].mean():.4f}\")\n",
    "        else:\n",
    "            log_progress(\"No se pudieron calcular métricas promedio (archivo vacío o columnas faltantes).\")\n",
    "    else:\n",
    "        log_progress(\"Archivo de métricas no encontrado o vacío. No hay resumen de promedio.\")\n",
    "except Exception as e_summary:\n",
    "    logger.error(f\"Error al generar el resumen de métricas: {e_summary}\", exc_info=True)\n",
    "\n",
    "# Eliminar archivo checkpoint si todos los clusters fueron procesados (o intentados)\n",
    "total_attempted_clusters = last_successfully_processed_cluster_idx + 1 + clusters_successfully_processed_count + clusters_skipped_or_failed_count\n",
    "# Esta lógica de conteo para eliminación de checkpoint puede ser compleja si se reanuda.\n",
    "# Simplificación: si el último índice procesado es el último de la lista.\n",
    "if (last_successfully_processed_cluster_idx + clusters_successfully_processed_count + clusters_skipped_or_failed_count) >= (len(unique_clusters) -1) :\n",
    "        if os.path.exists(CHECKPOINT_FILE):\n",
    "            log_progress(\"Todos los clusters han sido abordados. Eliminando archivo checkpoint.\")\n",
    "        try:\n",
    "            os.remove(CHECKPOINT_FILE)\n",
    "        except OSError as e_rm_check:\n",
    "            logger.error(f\"No se pudo eliminar el archivo checkpoint ({CHECKPOINT_FILE}): {e_rm_check}\")\n",
    "else:\n",
    "    log_progress(f\"El archivo checkpoint {CHECKPOINT_FILE} se mantiene, ya que no todos los clusters parecen haber sido procesados en esta o previas ejecuciones.\")\n",
    "\n",
    "log_progress(\"========================== SCRIPT DE STACKING FINALIZADO ==========================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
