{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13c58d3f-4e9a-4b65-b7f1-7142b36a4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "\n",
    "SILVER_VENTAS_PATH = '../data/silver_ventas_establecimiento.parquet'\n",
    "GOLD_VENTAS_WEEKLY_PATH = '../data/gold_ventas_semanales.parquet'\n",
    "\n",
    "TIPOS_A_EXCLUIR = ['Evento','Eventos','Catering, colectividades y Otros','Parque Temático',\\\n",
    "                   'Z015','Sociedades','Particular','Z023','Z013','Alimentacion tradicional'\\\n",
    "                   'Cash, distribuidor', 'Z020', 'Z016']\n",
    "COLUMNS_TO_REMOVE = ['uom', 'material_name', 'tipo', 'region', 'promo_id', 'promo_flag', 'promo_type', 'promo_mechanics']\n",
    "\n",
    "\n",
    "MATERIALS_TO_INCLUDE = ['ED13', 'FD13', 'DL13', 'VI13', 'ED30', 'FD30', 'DL30', 'VI30', 'ED15', 'FD15', 'DL15', 'VI15']\n",
    "MATERIALS_TO_INCLUDE = ('ED', 'FD', 'DL', 'VI', 'VD', 'BD')\n",
    "\n",
    "def filter_sales_by_not_type(table: pa.Table, types: list[str]) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filter sales by type using DuckDB SQL on a PyArrow Table.\n",
    "    Creates a temporary in-memory DuckDB connection.\n",
    "    Returns a new PyArrow Table object.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Create a SQL-safe string representation of the list for the IN clause\n",
    "        types_sql = \", \".join([f\"'{t}'\" for t in types])\n",
    "        \n",
    "        # Construct the filtering query to run on the registered table\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE tipo NOT IN ({types_sql})\n",
    "        \"\"\"\n",
    "        # Execute the query and fetch the result as an Arrow table\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def filter_sales_date(table: pa.Table, date_from: str, date_to: str) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filter sales by date using DuckDB SQL on a PyArrow Table.\n",
    "    Creates a temporary in-memory DuckDB connection.\n",
    "    Returns a new PyArrow Table object.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE week BETWEEN '{date_from}' AND '{date_to}'\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def promoid_to_boolean(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Create a binary flag indicating whether a row has a promotion or not.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table\n",
    "        column: Name of the promotion column to check\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table with an additional binary column 'has_promo'\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *,\n",
    "                CASE\n",
    "                    WHEN promo_id IS NOT NULL THEN 1\n",
    "                    ELSE 0\n",
    "                END AS has_promo\n",
    "            FROM input_table\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "def remove_columns(table: pa.Table, columns: list[str]) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Remove specified columns from a PyArrow Table.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table\n",
    "        columns: List of column names to remove\n",
    "\n",
    "    Returns:\n",
    "        PyArrow table with the specified columns removed\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Get all column names from the table\n",
    "        all_columns = table.column_names\n",
    "        \n",
    "        # Filter out the columns we want to remove\n",
    "        columns_to_keep = [col for col in all_columns if col not in columns]\n",
    "        \n",
    "        # If there are no columns left, return an empty table\n",
    "        if not columns_to_keep:\n",
    "            raise ValueError(\"Cannot remove all columns from the table\")\n",
    "        \n",
    "        # Create the SELECT clause with the columns to keep\n",
    "        select_clause = ', '.join([f'\"{col}\"' for col in columns_to_keep])\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT {select_clause}\n",
    "            FROM input_table\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "def covid_flag(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Create a binary flag indicating whether a row is in the COVID period or not.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table\n",
    "    \n",
    "    Returns:    \n",
    "        PyArrow table with an additional binary column 'is_covid'\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *,\n",
    "                CASE\n",
    "                    WHEN calday BETWEEN '2020-03-01' AND '2022-04-30' THEN 1\n",
    "                    ELSE 0\n",
    "                END AS is_covid_period\n",
    "            FROM input_table\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def filter_by_string_in_column(table: pa.Table, column: str, string_to_filter: str) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filter rows based on whether a column contains a specific string.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table\n",
    "        column: Name of the column to filter    \n",
    "        string_to_filter: String to filter for\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table with rows where the specified column contains the string\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE {column} LIKE '%{string_to_filter}%'\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def process_data(initial_table: pa.Table, processing_functions: list, \n",
    "               show_intermediate: bool = False,\n",
    "               save_result: bool = False,\n",
    "               output_path: str = None,\n",
    "               output_compression: str = 'snappy',\n",
    "               memory_limit: str = '8GB') -> pa.Table:\n",
    "    \"\"\"\n",
    "    Apply a list of processing functions to a PyArrow table in sequence.\n",
    "    \n",
    "    Args:\n",
    "        initial_table: The starting PyArrow table\n",
    "        processing_functions: List of functions to apply, where each function:\n",
    "                             - Takes a PyArrow table as its first argument\n",
    "                             - May take additional args/kwargs\n",
    "                             - Returns a PyArrow table\n",
    "        show_intermediate: Whether to print information about intermediate tables\n",
    "        save_result: Whether to save the final result as a Parquet file\n",
    "        output_path: Path where to save the Parquet file (required if save_result=True)\n",
    "        output_compression: Compression algorithm to use (default: 'snappy')\n",
    "        memory_limit: Memory limit for DuckDB operations (default: '4GB')\n",
    "    \n",
    "    Returns:\n",
    "        The final PyArrow table after all processing steps\n",
    "    \"\"\"\n",
    "    current_table = initial_table\n",
    "    \n",
    "    # Create a temporary connection for displaying results if needed\n",
    "    con = None\n",
    "    if show_intermediate:\n",
    "        con = duckdb.connect()\n",
    "        con.execute(f\"PRAGMA memory_limit='{memory_limit}'\")\n",
    "    \n",
    "    try:\n",
    "        # Apply each function in the list\n",
    "        for i, func_info in enumerate(processing_functions):\n",
    "            # Each func_info should be either:\n",
    "            # 1. A function reference\n",
    "            # 2. A tuple of (function, args, kwargs)\n",
    "            \n",
    "            if callable(func_info):\n",
    "                # Just a function with no extra args\n",
    "                function = func_info\n",
    "                args = []\n",
    "                kwargs = {}\n",
    "            elif isinstance(func_info, tuple) and len(func_info) >= 1 and callable(func_info[0]):\n",
    "                # Tuple of (function, args, kwargs)\n",
    "                function = func_info[0]\n",
    "                args = func_info[1] if len(func_info) > 1 else []\n",
    "                kwargs = func_info[2] if len(func_info) > 2 else {}\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid function specification at position {i}\")\n",
    "            \n",
    "            # Apply the function\n",
    "            function_name = function.__name__\n",
    "            if show_intermediate:\n",
    "                print(f\"\\nStep {i+1}: Applying {function_name}\")\n",
    "                print(f\"Rows before: {len(current_table):,}\")\n",
    "            \n",
    "            # Apply the function with current_table as first arg\n",
    "            current_table = function(current_table, *args, **kwargs)\n",
    "            \n",
    "            # Force garbage collection after each step to free memory\n",
    "            gc.collect()\n",
    "            \n",
    "            if show_intermediate:\n",
    "                print(f\"Rows after: {len(current_table):,}\")\n",
    "                # Show first few rows\n",
    "                if con:\n",
    "                    con.register('current_table', current_table)\n",
    "                    print(f\"\\nSample after {function_name} (first 5 rows):\")\n",
    "                    con.sql(\"SELECT * FROM current_table LIMIT 5\").show()\n",
    "                    \n",
    "                    # Reset DuckDB connection to free memory\n",
    "                    con.close()\n",
    "                    con = duckdb.connect()\n",
    "                    con.execute(f\"PRAGMA memory_limit='{memory_limit}'\")\n",
    "        \n",
    "        # Save the result if requested\n",
    "        if save_result:\n",
    "            if output_path is None:\n",
    "                raise ValueError(\"output_path must be specified when save_result=True\")\n",
    "            \n",
    "            print(f\"\\nSaving result to {output_path}\")\n",
    "            # Use chunked writing for large tables\n",
    "            if len(current_table) > 1000000:  # If table is large\n",
    "                print(\"Using chunked writing for large table...\")\n",
    "                # For large tables, we'll split the table into chunks\n",
    "                import os\n",
    "                from pathlib import Path\n",
    "                \n",
    "                # Create a temporary directory for chunks\n",
    "                temp_dir = Path(output_path).parent / f\"temp_{Path(output_path).stem}\"\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                \n",
    "                # Write chunks to individual files\n",
    "                chunk_size = 250000\n",
    "                num_chunks = (len(current_table) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "                \n",
    "                for i in range(num_chunks):\n",
    "                    start_idx = i * chunk_size\n",
    "                    end_idx = min((i + 1) * chunk_size, len(current_table))\n",
    "                    \n",
    "                    # Extract chunk\n",
    "                    chunk = current_table.slice(start_idx, end_idx - start_idx)\n",
    "                    \n",
    "                    # Write to temp file\n",
    "                    chunk_path = temp_dir / f\"chunk_{i}.parquet\"\n",
    "                    pq.write_table(chunk, chunk_path, compression=output_compression)\n",
    "                    print(f\"  - Saved chunk {i+1}/{num_chunks} to {chunk_path}\")\n",
    "                    \n",
    "                    # Release memory\n",
    "                    del chunk\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Merge chunks into final file\n",
    "                print(f\"Merging {num_chunks} chunks into final file...\")\n",
    "                \n",
    "                # Read and concatenate all chunks\n",
    "                chunk_files = sorted(temp_dir.glob(\"chunk_*.parquet\"))\n",
    "                tables = []\n",
    "                \n",
    "                for chunk_file in chunk_files:\n",
    "                    tables.append(pq.read_table(chunk_file))\n",
    "                \n",
    "                # Write concatenated table to final path\n",
    "                merged_table = pa.concat_tables(tables)\n",
    "                pq.write_table(merged_table, output_path, compression=output_compression)\n",
    "                \n",
    "                # Clean up temporary files\n",
    "                for chunk_file in chunk_files:\n",
    "                    os.remove(chunk_file)\n",
    "                os.rmdir(temp_dir)\n",
    "                \n",
    "                print(f\"Successfully merged chunks and cleaned up temporary files\")\n",
    "            else:\n",
    "                # Use standard PyArrow writing for smaller tables\n",
    "                pq.write_table(current_table, output_path, compression=output_compression)\n",
    "            \n",
    "            print(f\"Saved {len(current_table):,} rows to {output_path}\")\n",
    "        \n",
    "        return current_table\n",
    "    \n",
    "    finally:\n",
    "        # Close the connection if it was created\n",
    "        if con:\n",
    "            con.close()\n",
    "        \n",
    "        # Final garbage collection\n",
    "        gc.collect()\n",
    "\n",
    "def group_by_week(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Group sales by week derived from calday.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table with calday column (date format)\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table with sales grouped by week, with aggregated metrics\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = \"\"\"\n",
    "            SELECT \n",
    "                establecimiento,\n",
    "                material,\n",
    "                DATE_TRUNC('week', calday) AS week,\n",
    "                -- Keep has_promo as 1 if ANY row in the group had a promotion\n",
    "                MAX(has_promo) AS has_promo,\n",
    "                -- Aggregate metrics\n",
    "                SUM(volume_ap) AS weekly_volume,\n",
    "                -- Keep other dimension columns\n",
    "                MAX(is_covid_period) AS is_covid_period\n",
    "            FROM input_table\n",
    "            GROUP BY \n",
    "                establecimiento,\n",
    "                material,\n",
    "                DATE_TRUNC('week', calday)\n",
    "            ORDER BY \n",
    "                establecimiento,\n",
    "                material,\n",
    "                week\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "def filter_by_min_weeks(table: pa.Table, min_weeks: int) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filter out store-product combinations that have fewer than min_weeks of data.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table with week column\n",
    "        min_weeks: Minimum number of weeks required to keep a store-product combination\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table with only store-product combinations having sufficient data\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            WITH series_counts AS (\n",
    "                SELECT \n",
    "                    establecimiento, \n",
    "                    material,\n",
    "                    COUNT(DISTINCT week) as week_count\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "            )\n",
    "            SELECT t.*\n",
    "            FROM input_table t\n",
    "            JOIN series_counts s\n",
    "                ON t.establecimiento = s.establecimiento \n",
    "                AND t.material = s.material\n",
    "            WHERE s.week_count >= {min_weeks}\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def fill_time_series_gaps(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Fill gaps in time series data for each store-product combination.\n",
    "    For each combination, generates rows for any missing weeks between min and max date.\n",
    "    \n",
    "    Processes data in smaller chunks to avoid memory issues.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table with 'week' column and store-product identifiers\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table with continuous weekly data, filling missing weeks with NULL values\n",
    "    \"\"\"\n",
    "    import pyarrow as pa\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Create a DuckDB connection with memory limits\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='4GB'\")  # Limit DuckDB memory usage\n",
    "    \n",
    "    try:\n",
    "        # Register the input table\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Get unique store-product combinations\n",
    "        combinations = con.execute(\"\"\"\n",
    "            SELECT DISTINCT establecimiento, material \n",
    "            FROM input_table\n",
    "            ORDER BY establecimiento, material\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        print(f\"Processing {len(combinations)} unique store-product combinations in batches\")\n",
    "        \n",
    "        # Process in smaller batches to avoid memory issues\n",
    "        batch_size = 500  # Adjust based on memory constraints\n",
    "        all_results = []\n",
    "        \n",
    "        # Process in batches\n",
    "        for i in range(0, len(combinations), batch_size):\n",
    "            batch = combinations[i:i+batch_size]\n",
    "            batch_conditions = []\n",
    "            \n",
    "            # Build WHERE conditions for the current batch\n",
    "            for estab, mat in batch:\n",
    "                batch_conditions.append(f\"(establecimiento = '{estab}' AND material = '{mat}')\")\n",
    "            \n",
    "            # Process this batch\n",
    "            where_clause = \" OR \".join(batch_conditions)\n",
    "            \n",
    "            # This query:\n",
    "            # 1. Finds min and max weeks for each store-product combination in the batch\n",
    "            # 2. Generates a continuous sequence of weeks for each combination\n",
    "            # 3. Left joins with original data to get metrics where available\n",
    "            # 4. Fills NULL with 0 for numeric columns and appropriate values for flags\n",
    "            batch_query = f\"\"\"\n",
    "                WITH \n",
    "                -- Get min and max week for each store-product in this batch\n",
    "                date_ranges AS (\n",
    "                    SELECT \n",
    "                        establecimiento,\n",
    "                        material,\n",
    "                        MIN(week) AS min_week,\n",
    "                        MAX(week) AS max_week\n",
    "                    FROM input_table\n",
    "                    WHERE {where_clause}\n",
    "                    GROUP BY establecimiento, material\n",
    "                ),\n",
    "                \n",
    "                -- Generate all weeks between min and max for each combination\n",
    "                all_weeks AS (\n",
    "                    SELECT \n",
    "                        d.establecimiento,\n",
    "                        d.material,\n",
    "                        -- Cast GENERATE_SERIES result to DATE explicitly\n",
    "                        calendar_value::DATE AS week\n",
    "                    FROM date_ranges d,\n",
    "                    LATERAL UNNEST(\n",
    "                        GENERATE_SERIES(\n",
    "                            d.min_week, \n",
    "                            d.max_week, \n",
    "                            INTERVAL '1 week'\n",
    "                        )\n",
    "                    ) AS t(calendar_value)\n",
    "                )\n",
    "                \n",
    "                -- Join with original data to get metrics where available\n",
    "                SELECT \n",
    "                    a.establecimiento,\n",
    "                    a.material,\n",
    "                    a.week,\n",
    "                    COALESCE(o.has_promo, 0) AS has_promo,\n",
    "                    COALESCE(o.weekly_volume, 0) AS weekly_volume,\n",
    "                    COALESCE(o.is_covid_period, \n",
    "                        CASE \n",
    "                            WHEN a.week BETWEEN '2020-03-01' AND '2022-04-30' THEN 1\n",
    "                            ELSE 0\n",
    "                        END\n",
    "                    ) AS is_covid_period\n",
    "                FROM all_weeks a\n",
    "                LEFT JOIN input_table o\n",
    "                    ON a.establecimiento = o.establecimiento\n",
    "                    AND a.material = o.material\n",
    "                    AND a.week = o.week\n",
    "                ORDER BY \n",
    "                    a.establecimiento,\n",
    "                    a.material,\n",
    "                    a.week\n",
    "            \"\"\"\n",
    "            \n",
    "            # Execute query and collect results\n",
    "            print(f\"Processing batch {i//batch_size + 1}/{(len(combinations)-1)//batch_size + 1} \" +\n",
    "                  f\"(items {i+1}-{min(i+batch_size, len(combinations))})\")\n",
    "            \n",
    "            batch_result = con.execute(batch_query).fetch_arrow_table()\n",
    "            all_results.append(batch_result)\n",
    "            \n",
    "            # Force memory cleanup\n",
    "            # Close and reopen connection to clear memory between batches\n",
    "            con.close()\n",
    "            con = duckdb.connect()\n",
    "            con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "            con.register('input_table', table)\n",
    "            \n",
    "            # Also force Python garbage collection\n",
    "            gc.collect()\n",
    "            \n",
    "        # Combine all batches into one table\n",
    "        if len(all_results) == 1:\n",
    "            result_table = all_results[0]\n",
    "        else:\n",
    "            result_table = pa.concat_tables(all_results)\n",
    "            \n",
    "        return result_table\n",
    "    finally:\n",
    "        con.close()  # Ensure connection is closed\n",
    "\n",
    "def sort_series_by_volume(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Sort the time series data by total volume for each store-product combination.\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table with weekly_volume column and store-product identifiers\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table sorted by total volume of each series (establecimiento-material pair)\n",
    "    \"\"\"\n",
    "    con = duckdb.connect() # Create temporary connection\n",
    "    con.execute(\"PRAGMA memory_limit='4GB'\")  # Limit DuckDB memory usage\n",
    "    \n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Calculate total volume for each store-product combination\n",
    "        # Then join back to the original data and sort\n",
    "        query = \"\"\"\n",
    "            WITH series_totals AS (\n",
    "                SELECT \n",
    "                    establecimiento,\n",
    "                    material,\n",
    "                    SUM(weekly_volume) AS total_volume\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "            )\n",
    "            SELECT t.*\n",
    "            FROM input_table t\n",
    "            JOIN series_totals s\n",
    "                ON t.establecimiento = s.establecimiento \n",
    "                AND t.material = s.material\n",
    "            ORDER BY \n",
    "                s.total_volume DESC,  -- Primary sort by total volume\n",
    "                t.establecimiento,    -- Secondary sort to keep series together\n",
    "                t.material,\n",
    "                t.week                -- Maintain time order within each series\n",
    "        \"\"\"\n",
    "        \n",
    "        # For very large tables, process in batches\n",
    "        if table.num_rows > 1000000:\n",
    "            # Get the unique combinations and their total volumes\n",
    "            totals_df = con.execute(\"\"\"\n",
    "                SELECT \n",
    "                    establecimiento,\n",
    "                    material,\n",
    "                    SUM(weekly_volume) AS total_volume\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "                ORDER BY SUM(weekly_volume) DESC\n",
    "            \"\"\").fetchdf()\n",
    "            \n",
    "            # Process in batches of combinations\n",
    "            batch_size = 500\n",
    "            all_results = []\n",
    "            \n",
    "            for i in range(0, len(totals_df), batch_size):\n",
    "                batch_df = totals_df.iloc[i:i+batch_size]\n",
    "                estabs = [f\"'{e}'\" for e in batch_df['establecimiento']]\n",
    "                mats = [f\"'{m}'\" for m in batch_df['material']]\n",
    "                \n",
    "                batch_conditions = []\n",
    "                for idx in range(len(batch_df)):\n",
    "                    e = batch_df.iloc[idx]['establecimiento']\n",
    "                    m = batch_df.iloc[idx]['material']\n",
    "                    batch_conditions.append(f\"(establecimiento = '{e}' AND material = '{m}')\")\n",
    "                \n",
    "                where_clause = \" OR \".join(batch_conditions)\n",
    "                \n",
    "                batch_query = f\"\"\"\n",
    "                    WITH series_totals AS (\n",
    "                        SELECT \n",
    "                            establecimiento,\n",
    "                            material,\n",
    "                            SUM(weekly_volume) AS total_volume\n",
    "                        FROM input_table\n",
    "                        WHERE {where_clause}\n",
    "                        GROUP BY establecimiento, material\n",
    "                    )\n",
    "                    SELECT t.*\n",
    "                    FROM input_table t\n",
    "                    JOIN series_totals s\n",
    "                        ON t.establecimiento = s.establecimiento \n",
    "                        AND t.material = s.material\n",
    "                    ORDER BY \n",
    "                        s.total_volume DESC,\n",
    "                        t.establecimiento,\n",
    "                        t.material,\n",
    "                        t.week\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Processing sort batch {i//batch_size + 1}/{(len(totals_df)-1)//batch_size + 1}\")\n",
    "                batch_result = con.execute(batch_query).fetch_arrow_table()\n",
    "                all_results.append(batch_result)\n",
    "                \n",
    "                # Force memory cleanup\n",
    "                # Close and reopen connection to clear memory between batches\n",
    "                con.close()\n",
    "                con = duckdb.connect()\n",
    "                con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "                con.register('input_table', table)\n",
    "                \n",
    "                # Also force Python garbage collection\n",
    "                gc.collect()\n",
    "            \n",
    "            # Combine results\n",
    "            result_table = pa.concat_tables(all_results)\n",
    "        else:\n",
    "            # For smaller tables, process all at once\n",
    "            result_table = con.execute(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    return result_table\n",
    "\n",
    "def create_nested_series_format(table: pa.Table, output_path: str = None) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Create a nested format of the time series data, with one row per series \n",
    "    (establecimiento-material combination) and the time series data stored as \n",
    "    a list of dicts in a 'series' column.\n",
    "    \n",
    "    Each dict in the series contains:\n",
    "    - ds: week date\n",
    "    - y: weekly volume\n",
    "    - has_promo: promotion flag\n",
    "    - is_covid_period: covid period flag\n",
    "    \n",
    "    Args:\n",
    "        table: Input PyArrow table with weekly_volume data\n",
    "        output_path: Path to save the Parquet file (optional)\n",
    "    \n",
    "    Returns:\n",
    "        PyArrow table with nested series format\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='4GB'\")  # Limit DuckDB memory usage\n",
    "    \n",
    "    try:\n",
    "        # Register the PyArrow table with DuckDB\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # For large tables, process in batches by store-product combinations\n",
    "        if table.num_rows > 1000000:\n",
    "            print(\"Large table detected, processing nested series in batches...\")\n",
    "            \n",
    "            # Get unique store-product combinations with their total volumes\n",
    "            combinations = con.execute(\"\"\"\n",
    "                SELECT \n",
    "                    establecimiento, \n",
    "                    material,\n",
    "                    SUM(weekly_volume) AS total_volume\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "                ORDER BY \n",
    "                    total_volume DESC,\n",
    "                    establecimiento, \n",
    "                    material\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            # Process in batches of combinations\n",
    "            batch_size = 500\n",
    "            all_results = []\n",
    "            \n",
    "            for i in range(0, len(combinations), batch_size):\n",
    "                batch = combinations[i:i+batch_size]\n",
    "                batch_conditions = []\n",
    "                \n",
    "                # Build WHERE conditions for the current batch\n",
    "                for estab, mat, _ in batch:\n",
    "                    batch_conditions.append(f\"(establecimiento = '{estab}' AND material = '{mat}')\")\n",
    "                \n",
    "                # Process this batch\n",
    "                where_clause = \" OR \".join(batch_conditions)\n",
    "                \n",
    "                batch_query = f\"\"\"\n",
    "                    SELECT \n",
    "                        establecimiento,\n",
    "                        material,\n",
    "                        -- Create the nested series array with date-value pairs including flags\n",
    "                        LIST(STRUCT_PACK(\n",
    "                            ds := week::VARCHAR, \n",
    "                            y := weekly_volume,\n",
    "                            has_promo := has_promo,\n",
    "                            is_covid_period := is_covid_period\n",
    "                        )) AS series,\n",
    "                        -- Add a count of points for reference\n",
    "                        COUNT(*) AS num_points,\n",
    "                        -- Add total and average volume for quick reference\n",
    "                        SUM(weekly_volume) AS total_volume,\n",
    "                        AVG(weekly_volume) AS avg_weekly_volume\n",
    "                    FROM input_table\n",
    "                    WHERE {where_clause}\n",
    "                    GROUP BY establecimiento, material\n",
    "                    ORDER BY \n",
    "                        SUM(weekly_volume) DESC,\n",
    "                        establecimiento,\n",
    "                        material\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Processing nested series batch {i//batch_size + 1}/{(len(combinations)-1)//batch_size + 1}\")\n",
    "                batch_result = con.execute(batch_query).fetch_arrow_table()\n",
    "                all_results.append(batch_result)\n",
    "                \n",
    "                # Force memory cleanup\n",
    "                con.close()\n",
    "                con = duckdb.connect()\n",
    "                con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "                con.register('input_table', table)\n",
    "                \n",
    "                # Force Python garbage collection\n",
    "                gc.collect()\n",
    "            \n",
    "            # Combine all batches\n",
    "            result_table = pa.concat_tables(all_results)\n",
    "        else:\n",
    "            # For smaller tables, process all at once using the original query\n",
    "            query = \"\"\"\n",
    "                SELECT \n",
    "                    establecimiento,\n",
    "                    material,\n",
    "                    -- Create the nested series array with date-value pairs including flags\n",
    "                    LIST(STRUCT_PACK(\n",
    "                        ds := week::VARCHAR, \n",
    "                        y := weekly_volume,\n",
    "                        has_promo := has_promo,\n",
    "                        is_covid_period := is_covid_period\n",
    "                    )) AS series,\n",
    "                    -- Add a count of points for reference\n",
    "                    COUNT(*) AS num_points,\n",
    "                    -- Add total and average volume for quick reference\n",
    "                    SUM(weekly_volume) AS total_volume,\n",
    "                    AVG(weekly_volume) AS avg_weekly_volume\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "                ORDER BY \n",
    "                    SUM(weekly_volume) DESC,  -- Sort by total volume\n",
    "                    establecimiento,\n",
    "                    material\n",
    "            \"\"\"\n",
    "            result_table = con.execute(query).fetch_arrow_table()\n",
    "        \n",
    "        # Save to parquet if output_path provided\n",
    "        if output_path:\n",
    "            print(f\"\\nSaving nested series format to {output_path}\")\n",
    "            # For large tables, write in chunks\n",
    "            if len(result_table) > 100000:\n",
    "                print(\"Large result table, writing in chunks...\")\n",
    "                import os\n",
    "                from pathlib import Path\n",
    "                \n",
    "                # Create a temporary directory for chunks\n",
    "                temp_dir = Path(output_path).parent / f\"temp_{Path(output_path).stem}\"\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                \n",
    "                # Write chunks to individual files\n",
    "                chunk_size = 50000\n",
    "                num_chunks = (len(result_table) + chunk_size - 1) // chunk_size  # Ceiling division\n",
    "                \n",
    "                for i in range(num_chunks):\n",
    "                    start_idx = i * chunk_size\n",
    "                    end_idx = min((i + 1) * chunk_size, len(result_table))\n",
    "                    \n",
    "                    # Extract chunk\n",
    "                    chunk = result_table.slice(start_idx, end_idx - start_idx)\n",
    "                    \n",
    "                    # Write to temp file\n",
    "                    chunk_path = temp_dir / f\"chunk_{i}.parquet\"\n",
    "                    pq.write_table(chunk, chunk_path, compression='snappy')\n",
    "                    print(f\"  - Saved chunk {i+1}/{num_chunks} to {chunk_path}\")\n",
    "                    \n",
    "                    # Release memory\n",
    "                    del chunk\n",
    "                    gc.collect()\n",
    "                \n",
    "                # Merge chunks into final file\n",
    "                print(f\"Merging {num_chunks} chunks into final file...\")\n",
    "                \n",
    "                # Read and concatenate all chunks\n",
    "                chunk_files = sorted(temp_dir.glob(\"chunk_*.parquet\"))\n",
    "                tables = []\n",
    "                \n",
    "                for chunk_file in chunk_files:\n",
    "                    tables.append(pq.read_table(chunk_file))\n",
    "                \n",
    "                # Write concatenated table to final path\n",
    "                merged_table = pa.concat_tables(tables)\n",
    "                pq.write_table(merged_table, output_path, compression='snappy')\n",
    "                \n",
    "                # Clean up temporary files\n",
    "                for chunk_file in chunk_files:\n",
    "                    os.remove(chunk_file)\n",
    "                os.rmdir(temp_dir)\n",
    "                \n",
    "                print(f\"Successfully merged chunks and cleaned up temporary files\")\n",
    "            else:\n",
    "                # Standard write for smaller tables\n",
    "                pq.write_table(result_table, output_path)\n",
    "                \n",
    "            print(f\"Saved {len(result_table):,} series to {output_path}\")\n",
    "            \n",
    "            # Log a sample to show structure\n",
    "            sample = con.execute(\"\"\"\n",
    "                SELECT \n",
    "                    establecimiento, \n",
    "                    material, \n",
    "                    num_points, \n",
    "                    series[1:3] AS sample_points\n",
    "                FROM result_table \n",
    "                LIMIT 1\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if sample:\n",
    "                print(\"\\nSample of nested structure:\")\n",
    "                print(f\"Series for {sample[0][0]}-{sample[0][1]} has {sample[0][2]} points\")\n",
    "                print(f\"First few points: {sample[0][3]}\")\n",
    "    finally:\n",
    "        con.close() # Ensure connection is closed\n",
    "    \n",
    "    # Final garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    return result_table\n",
    "\n",
    "def list_materials_from_parquet(table_name: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    List the materials from a parquet file.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    result = con.execute(f\"SELECT DISTINCT material FROM read_parquet('{table_name}')\").fetchdf()['material'].tolist()\n",
    "    con.close()\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_by_materials(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filter the table to only include rows where the material is in the list.\n",
    "    Only includes materials starting with: \n",
    "    \"\"\"\n",
    "    # Create connection first\n",
    "    con = duckdb.connect()\n",
    "    \n",
    "    try:\n",
    "        # Get all distinct materials in gold_ventas_semanales\n",
    "        materials = list_materials_from_parquet(SILVER_VENTAS_PATH)\n",
    "        \n",
    "        # Filter materials by prefix\n",
    "        materials = [material for material in materials if material.startswith((MATERIALS_TO_INCLUDE))]\n",
    "\n",
    "        # Register input table and run query\n",
    "        con.register('input_table', table)\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE material IN ({', '.join([f\"'{m}'\" for m in materials])})\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "        \n",
    "        return result_table\n",
    "    finally:\n",
    "        con.close()  # Ensure connection is closed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dab82c-c721-4156-8a7c-6f75a36e3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial data from: ../data/silver_ventas_establecimiento.parquet\n",
      "Initial rows: 67,918,456\n",
      "\n",
      "Initial Silver Table (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬──────────────────────┬───────────┬──────────────┬─────────────────┐\n",
      "│ establecimiento │ material │       calday        │       promo_id       │ volume_ap │ cantidad_umb │      tipo       │\n",
      "│     varchar     │ varchar  │      timestamp      │       varchar        │  double   │    double    │     varchar     │\n",
      "├─────────────────┼──────────┼─────────────────────┼──────────────────────┼───────────┼──────────────┼─────────────────┤\n",
      "│ 8100240876      │ TB8      │ 2024-11-26 00:00:00 │ NULL                 │       8.0 │          1.0 │ Bar Cervecería  │\n",
      "│ 8100032055      │ PI13     │ 2024-11-26 00:00:00 │ NULL                 │      7.92 │          1.0 │ Restaurante     │\n",
      "│ 8100258434      │ FL13SPN  │ 2024-11-26 00:00:00 │ NULL                 │      23.1 │          2.0 │ Bar Cervecería  │\n",
      "│ 8100036860      │ VO13     │ 2024-11-26 00:00:00 │ 00000000000080619348 │       0.0 │          0.0 │ Bar Cervecería  │\n",
      "│ 8100168246      │ FL32SP   │ 2024-11-26 00:00:00 │ NULL                 │      36.0 │          2.0 │ Pub y Discoteca │\n",
      "└─────────────────┴──────────┴─────────────────────┴──────────────────────┴───────────┴──────────────┴─────────────────┘\n",
      "\n",
      "\n",
      "Step 1: Applying filter_sales_by_not_type\n",
      "Rows before: 67,918,456\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after filter_sales_by_not_type (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬──────────┬───────────┬──────────────┬────────────────┐\n",
      "│ establecimiento │ material │       calday        │ promo_id │ volume_ap │ cantidad_umb │      tipo      │\n",
      "│     varchar     │ varchar  │      timestamp      │ varchar  │  double   │    double    │    varchar     │\n",
      "├─────────────────┼──────────┼─────────────────────┼──────────┼───────────┼──────────────┼────────────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          2.0 │ Restaurante    │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │ NULL     │     47.52 │          6.0 │ Restaurante    │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          1.0 │ Bar Cervecería │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │ NULL     │      60.0 │          2.0 │ Restaurante    │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │ NULL     │      54.0 │          3.0 │ Bar Cervecería │\n",
      "└─────────────────┴──────────┴─────────────────────┴──────────┴───────────┴──────────────┴────────────────┘\n",
      "\n",
      "\n",
      "Step 2: Applying promoid_to_boolean\n",
      "Rows before: 66,740,407\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after promoid_to_boolean (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬──────────┬───────────┬──────────────┬────────────────┬───────────┐\n",
      "│ establecimiento │ material │       calday        │ promo_id │ volume_ap │ cantidad_umb │      tipo      │ has_promo │\n",
      "│     varchar     │ varchar  │      timestamp      │ varchar  │  double   │    double    │    varchar     │   int32   │\n",
      "├─────────────────┼──────────┼─────────────────────┼──────────┼───────────┼──────────────┼────────────────┼───────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          2.0 │ Restaurante    │         0 │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │ NULL     │     47.52 │          6.0 │ Restaurante    │         0 │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          1.0 │ Bar Cervecería │         0 │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │ NULL     │      60.0 │          2.0 │ Restaurante    │         0 │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │ NULL     │      54.0 │          3.0 │ Bar Cervecería │         0 │\n",
      "└─────────────────┴──────────┴─────────────────────┴──────────┴───────────┴──────────────┴────────────────┴───────────┘\n",
      "\n",
      "\n",
      "Step 3: Applying remove_columns\n",
      "Rows before: 66,740,407\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after remove_columns (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬───────────┬──────────────┬───────────┐\n",
      "│ establecimiento │ material │       calday        │ volume_ap │ cantidad_umb │ has_promo │\n",
      "│     varchar     │ varchar  │      timestamp      │  double   │    double    │   int32   │\n",
      "├─────────────────┼──────────┼─────────────────────┼───────────┼──────────────┼───────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │      20.0 │          2.0 │         0 │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │     47.52 │          6.0 │         0 │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │      20.0 │          1.0 │         0 │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │      60.0 │          2.0 │         0 │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │      54.0 │          3.0 │         0 │\n",
      "└─────────────────┴──────────┴─────────────────────┴───────────┴──────────────┴───────────┘\n",
      "\n",
      "\n",
      "Step 4: Applying covid_flag\n",
      "Rows before: 66,740,407\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after covid_flag (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬───────────┬──────────────┬───────────┬─────────────────┐\n",
      "│ establecimiento │ material │       calday        │ volume_ap │ cantidad_umb │ has_promo │ is_covid_period │\n",
      "│     varchar     │ varchar  │      timestamp      │  double   │    double    │   int32   │      int32      │\n",
      "├─────────────────┼──────────┼─────────────────────┼───────────┼──────────────┼───────────┼─────────────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │      20.0 │          2.0 │         0 │               0 │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │     47.52 │          6.0 │         0 │               0 │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │      20.0 │          1.0 │         0 │               0 │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │      60.0 │          2.0 │         0 │               0 │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │      54.0 │          3.0 │         0 │               0 │\n",
      "└─────────────────┴──────────┴─────────────────────┴───────────┴──────────────┴───────────┴─────────────────┘\n",
      "\n",
      "\n",
      "Step 5: Applying filter_by_string_in_column\n",
      "Rows before: 66,740,407\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 1. Load the initial data into a PyArrow Table\n",
    "print(f\"Loading initial data from: {SILVER_VENTAS_PATH}\")\n",
    "initial_table = con.sql(f\"SELECT * FROM read_parquet('{SILVER_VENTAS_PATH}')\").fetch_arrow_table()\n",
    "print(f\"Initial rows: {len(initial_table):,}\")\n",
    "\n",
    "# Register the table for querying\n",
    "con.register('initial_table', initial_table)\n",
    "# Show first 5 rows\n",
    "print(\"\\nInitial Silver Table (first 5 rows):\")\n",
    "con.sql(\"SELECT * FROM initial_table LIMIT 5\").show()\n",
    "\n",
    "processing_pipeline = [\n",
    "        (filter_sales_by_not_type, [TIPOS_A_EXCLUIR]),\n",
    "        promoid_to_boolean,\n",
    "        (remove_columns, [COLUMNS_TO_REMOVE]),\n",
    "        covid_flag,\n",
    "        (filter_by_string_in_column, ['establecimiento', '81']),\n",
    "        filter_by_materials,\n",
    "        group_by_week,\n",
    "        fill_time_series_gaps,\n",
    "        (filter_by_min_weeks, [12]),\n",
    "        sort_series_by_volume\n",
    "    ]\n",
    "\n",
    "# Process the data through all steps\n",
    "final_table = process_data(\n",
    "    initial_table, \n",
    "    processing_pipeline, \n",
    "    show_intermediate=True, \n",
    "    save_result=True, \n",
    "    output_path=GOLD_VENTAS_WEEKLY_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67462d-14fd-4776-9eb2-867c68f3d34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
