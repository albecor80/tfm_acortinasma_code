{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c58d3f-4e9a-4b65-b7f1-7142b36a4bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "import os # Importado a nivel global ya que se usa en varias funciones para manejo de archivos\n",
    "from pathlib import Path # Importado a nivel global\n",
    "\n",
    "# --- Definición de Constantes ---\n",
    "\n",
    "# Rutas de archivos Parquet\n",
    "SILVER_VENTAS_PATH = '../data/silver_ventas_establecimiento.parquet'\n",
    "GOLD_VENTAS_WEEKLY_PATH = '../data/gold_ventas_semanales.parquet'\n",
    "\n",
    "# Lista de tipos de establecimiento a excluir en el filtrado\n",
    "TIPOS_A_EXCLUIR = ['Evento','Eventos','Catering, colectividades y Otros','Parque Temático',\n",
    "                   'Z015','Sociedades','Particular','Z023','Z013','Alimentacion tradicional',\n",
    "                   'Cash, distribuidor', 'Z020', 'Z016']\n",
    "\n",
    "# Columnas a eliminar durante el procesamiento\n",
    "COLUMNS_TO_REMOVE = ['uom', 'material_name', 'tipo', 'region', 'promo_id', 'promo_flag', 'promo_type', 'promo_mechanics']\n",
    "\n",
    "# Prefijos de materiales a incluir.\n",
    "MATERIALS_TO_INCLUDE = ('ED', 'FD', 'DL', 'VI', 'VD', 'BD')\n",
    "\n",
    "\n",
    "# --- Funciones de Transformación de Datos ---\n",
    "\n",
    "def filter_sales_by_not_type(table: pa.Table, types: list[str]) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filtra una tabla PyArrow para excluir filas según una lista de 'tipos'.\n",
    "\n",
    "    Utiliza una conexión DuckDB temporal en memoria para realizar la operación SQL.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada.\n",
    "        types: Lista de strings con los 'tipos' a excluir.\n",
    "\n",
    "    Returns:\n",
    "        Una nueva tabla PyArrow con las filas filtradas.\n",
    "    \"\"\"\n",
    "    # Se crea una conexión temporal a DuckDB para esta operación.\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Construye la parte de la consulta SQL para la cláusula IN.\n",
    "        # Consideración: Para mayor seguridad contra inyección SQL (aunque 'types' aquí es interno),\n",
    "        # se podrían explorar consultas parametrizadas si la complejidad no aumenta significativamente.\n",
    "        types_sql = \", \".join([f\"'{t}'\" for t in types])\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE tipo NOT IN ({types_sql})\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        # Asegura que la conexión se cierre siempre.\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def filter_sales_date(table: pa.Table, date_from: str, date_to: str) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filtra ventas por rango de fechas utilizando la columna 'week'.\n",
    "\n",
    "    Crea una conexión DuckDB temporal en memoria.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (debe contener la columna 'week').\n",
    "        date_from: Fecha de inicio del filtro (formato YYYY-MM-DD).\n",
    "        date_to: Fecha de fin del filtro (formato YYYY-MM-DD).\n",
    "\n",
    "    Returns:\n",
    "        Una nueva tabla PyArrow con las filas filtradas por fecha.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE week BETWEEN '{date_from}' AND '{date_to}'\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def promoid_to_boolean(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Crea una columna binaria 'has_promo' (1 o 0) basada en la existencia de 'promo_id'.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (debe contener la columna 'promo_id').\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow con la nueva columna 'has_promo'.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *,\n",
    "                CASE\n",
    "                    WHEN promo_id IS NOT NULL THEN 1\n",
    "                    ELSE 0\n",
    "                END AS has_promo\n",
    "            FROM input_table\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "def remove_columns(table: pa.Table, columns_to_remove: list[str]) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Elimina columnas especificadas de una tabla PyArrow.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada.\n",
    "        columns_to_remove: Lista de nombres de columnas a eliminar.\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow sin las columnas especificadas.\n",
    "    Raises:\n",
    "        ValueError: Si se intentan eliminar todas las columnas.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        all_columns = table.column_names\n",
    "        columns_to_keep = [col for col in all_columns if col not in columns_to_remove]\n",
    "        \n",
    "        if not columns_to_keep:\n",
    "            # No se puede tener una tabla sin columnas.\n",
    "            raise ValueError(\"No se pueden eliminar todas las columnas de la tabla.\")\n",
    "        \n",
    "        # Asegura que los nombres de las columnas estén entrecomillados por si tienen caracteres especiales.\n",
    "        select_clause = ', '.join([f'\"{col}\"' for col in columns_to_keep])\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT {select_clause}\n",
    "            FROM input_table\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "def covid_flag(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Crea una columna binaria 'is_covid_period' para indicar si la fecha 'calday' cae dentro del período COVID.\n",
    "\n",
    "    El período COVID se define entre '2020-03-01' y '2022-04-30'.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (debe contener la columna 'calday').\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow con la nueva columna 'is_covid_period'.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *,\n",
    "                CASE\n",
    "                    WHEN calday BETWEEN '2020-03-01' AND '2022-04-30' THEN 1\n",
    "                    ELSE 0\n",
    "                END AS is_covid_period\n",
    "            FROM input_table\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def filter_by_string_in_column(table: pa.Table, column_name: str, string_to_filter: str) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filtra filas basándose en si una columna contiene una subcadena específica.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada.\n",
    "        column_name: Nombre de la columna donde buscar.\n",
    "        string_to_filter: Subcadena a buscar (usando LIKE '%subcadena%').\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow con las filas donde la columna especificada contiene la subcadena.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # El uso de f-string directamente en la consulta SQL para valores de usuario\n",
    "        # podría ser un riesgo de seguridad si 'string_to_filter' o 'column_name'\n",
    "        # provinieran de fuentes no confiables. Aquí se asumen controlados.\n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE \"{column_name}\" LIKE '%{string_to_filter}%'\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def group_by_week(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Agrupa los datos por 'establecimiento', 'material' y semana (derivada de 'calday').\n",
    "\n",
    "    Calcula sumas y máximos para métricas relevantes.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (con 'calday', 'establecimiento', 'material', 'has_promo', 'volume_ap', 'is_covid_period').\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow con datos agregados semanalmente.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = \"\"\"\n",
    "            SELECT \n",
    "                establecimiento,\n",
    "                material,\n",
    "                DATE_TRUNC('week', calday) AS week,\n",
    "                MAX(has_promo) AS has_promo, -- 1 si alguna fila del grupo tuvo promoción\n",
    "                SUM(volume_ap) AS weekly_volume,\n",
    "                MAX(is_covid_period) AS is_covid_period -- 1 si alguna fila del grupo está en período COVID\n",
    "            FROM input_table\n",
    "            GROUP BY \n",
    "                establecimiento,\n",
    "                material,\n",
    "                DATE_TRUNC('week', calday)\n",
    "            ORDER BY \n",
    "                establecimiento,\n",
    "                material,\n",
    "                week\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "def filter_by_min_weeks(table: pa.Table, min_weeks: int) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filtra combinaciones de 'establecimiento'-'material' que tengan menos de 'min_weeks' semanas de datos.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (con 'establecimiento', 'material', 'week').\n",
    "        min_weeks: Número mínimo de semanas de datos requeridas.\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow filtrada.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            WITH series_counts AS (\n",
    "                SELECT \n",
    "                    establecimiento, \n",
    "                    material,\n",
    "                    COUNT(DISTINCT week) as week_count\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "            )\n",
    "            SELECT t.*\n",
    "            FROM input_table t\n",
    "            JOIN series_counts s\n",
    "                ON t.establecimiento = s.establecimiento \n",
    "                AND t.material = s.material\n",
    "            WHERE s.week_count >= {min_weeks}\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def fill_time_series_gaps(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Rellena huecos en series temporales para cada combinación 'establecimiento'-'material'.\n",
    "\n",
    "    Genera filas para semanas faltantes entre la fecha mínima y máxima de cada serie.\n",
    "    Procesa los datos en lotes para gestionar el uso de memoria.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (con 'week', 'establecimiento', 'material', y métricas).\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow con series temporales continuas, rellenando semanas faltantes.\n",
    "        Las métricas para semanas nuevas se rellenan con 0 o valores por defecto.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    # Límite de memoria para las operaciones de DuckDB en esta función.\n",
    "    con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "    \n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        combinations = con.execute(\"\"\"\n",
    "            SELECT DISTINCT establecimiento, material \n",
    "            FROM input_table\n",
    "            ORDER BY establecimiento, material\n",
    "        \"\"\").fetchall()\n",
    "        \n",
    "        print(f\"Procesando {len(combinations)} combinaciones únicas de establecimiento-material en lotes para rellenar huecos.\")\n",
    "        \n",
    "        batch_size = 500  # Ajustar según memoria disponible y tamaño de datos.\n",
    "        all_results = []\n",
    "        \n",
    "        for i in range(0, len(combinations), batch_size):\n",
    "            batch = combinations[i:i+batch_size]\n",
    "            batch_conditions_list = []\n",
    "            \n",
    "            for estab, mat in batch:\n",
    "                # Escapar apóstrofes en los nombres por si acaso.\n",
    "                estab_escaped = estab.replace(\"'\", \"''\")\n",
    "                mat_escaped = mat.replace(\"'\", \"''\")\n",
    "                batch_conditions_list.append(f\"(establecimiento = '{estab_escaped}' AND material = '{mat_escaped}')\")\n",
    "            \n",
    "            where_clause_batch = \" OR \".join(batch_conditions_list)\n",
    "            \n",
    "            batch_query = f\"\"\"\n",
    "                WITH \n",
    "                date_ranges AS (\n",
    "                    SELECT \n",
    "                        establecimiento,\n",
    "                        material,\n",
    "                        MIN(week) AS min_week,\n",
    "                        MAX(week) AS max_week\n",
    "                    FROM input_table\n",
    "                    WHERE {where_clause_batch}\n",
    "                    GROUP BY establecimiento, material\n",
    "                ),\n",
    "                all_weeks AS (\n",
    "                    SELECT \n",
    "                        d.establecimiento,\n",
    "                        d.material,\n",
    "                        calendar_value::DATE AS week\n",
    "                    FROM date_ranges d,\n",
    "                    LATERAL UNNEST(\n",
    "                        GENERATE_SERIES(\n",
    "                            d.min_week, \n",
    "                            d.max_week, \n",
    "                            INTERVAL '1 week'\n",
    "                        )\n",
    "                    ) AS t(calendar_value)\n",
    "                )\n",
    "                SELECT \n",
    "                    a.establecimiento,\n",
    "                    a.material,\n",
    "                    a.week,\n",
    "                    COALESCE(o.has_promo, 0) AS has_promo,\n",
    "                    COALESCE(o.weekly_volume, 0) AS weekly_volume,\n",
    "                    COALESCE(o.is_covid_period, \n",
    "                        CASE \n",
    "                            WHEN a.week BETWEEN '2020-03-01' AND '2022-04-30' THEN 1\n",
    "                            ELSE 0\n",
    "                        END\n",
    "                    ) AS is_covid_period\n",
    "                FROM all_weeks a\n",
    "                LEFT JOIN input_table o\n",
    "                    ON a.establecimiento = o.establecimiento\n",
    "                    AND a.material = o.material\n",
    "                    AND a.week = o.week\n",
    "                ORDER BY \n",
    "                    a.establecimiento,\n",
    "                    a.material,\n",
    "                    a.week\n",
    "            \"\"\"\n",
    "            \n",
    "            print(f\"Rellenando huecos: Lote {i//batch_size + 1}/{(len(combinations) + batch_size -1)//batch_size}, \" +\n",
    "                  f\"items {i+1}-{min(i+batch_size, len(combinations))}\")\n",
    "            \n",
    "            batch_result = con.execute(batch_query).fetch_arrow_table()\n",
    "            all_results.append(batch_result)\n",
    "            \n",
    "            # Limpieza de memoria explícita entre lotes.\n",
    "            con.unregister('input_table') # Deregistra antes de cerrar para liberar referencias de DuckDB\n",
    "            con.close()\n",
    "            gc.collect() # Python GC\n",
    "            con = duckdb.connect()\n",
    "            con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "            con.register('input_table', table) # Re-registra la tabla original para el siguiente lote\n",
    "            \n",
    "        if not all_results:\n",
    "            return pa.Table.from_arrays([], names=table.schema.names, schema=table.schema) # Devolver tabla vacía con schema\n",
    "\n",
    "        result_table = pa.concat_tables(all_results) if len(all_results) > 1 else all_results[0]\n",
    "        \n",
    "        return result_table\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "def sort_series_by_volume(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Ordena las series temporales por el volumen total de cada combinación 'establecimiento'-'material'.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (con 'weekly_volume', 'establecimiento', 'material', 'week').\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow ordenada. Las series con mayor volumen total aparecen primero.\n",
    "        Dentro de cada serie, se mantiene el orden por semana.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "    \n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Para tablas muy grandes, se procesa en lotes basados en combinaciones ordenadas por volumen.\n",
    "        if table.num_rows > 1000000: # Umbral para procesamiento en lotes.\n",
    "            print(\"Tabla grande detectada para ordenamiento, procesando en lotes...\")\n",
    "            totals_df = con.execute(\"\"\"\n",
    "                SELECT \n",
    "                    establecimiento,\n",
    "                    material,\n",
    "                    SUM(weekly_volume) AS total_volume\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "                ORDER BY total_volume DESC\n",
    "            \"\"\").fetchdf() # Usar fetchdf para facilitar el manejo de lotes de combinaciones.\n",
    "            \n",
    "            batch_size = 500  # Número de combinaciones establecimiento-material por lote.\n",
    "            all_results = []\n",
    "            \n",
    "            for i in range(0, len(totals_df), batch_size):\n",
    "                batch_df = totals_df.iloc[i:i+batch_size]\n",
    "                batch_conditions_list = []\n",
    "                for _, row in batch_df.iterrows():\n",
    "                    estab_escaped = row['establecimiento'].replace(\"'\", \"''\")\n",
    "                    mat_escaped = row['material'].replace(\"'\", \"''\")\n",
    "                    batch_conditions_list.append(f\"(establecimiento = '{estab_escaped}' AND material = '{mat_escaped}')\")\n",
    "                \n",
    "                if not batch_conditions_list: continue # Si el lote está vacío\n",
    "                \n",
    "                where_clause_batch = \" OR \".join(batch_conditions_list)\n",
    "                \n",
    "                # La cláusula ORDER BY en la subconsulta series_totals_batch no es estrictamente necesaria\n",
    "                # para la corrección, pero se mantiene por si ayuda al optimizador en algunos casos.\n",
    "                # El orden final lo da el ORDER BY de la consulta principal.\n",
    "                batch_query = f\"\"\"\n",
    "                    WITH series_totals_batch AS (\n",
    "                        SELECT \n",
    "                            establecimiento,\n",
    "                            material,\n",
    "                            SUM(weekly_volume) AS total_volume\n",
    "                        FROM input_table\n",
    "                        WHERE {where_clause_batch}\n",
    "                        GROUP BY establecimiento, material\n",
    "                    )\n",
    "                    SELECT t.*\n",
    "                    FROM input_table t\n",
    "                    JOIN series_totals_batch s\n",
    "                        ON t.establecimiento = s.establecimiento \n",
    "                        AND t.material = s.material\n",
    "                    ORDER BY \n",
    "                        s.total_volume DESC,\n",
    "                        t.establecimiento,\n",
    "                        t.material,\n",
    "                        t.week\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Ordenando por volumen: Lote {i//batch_size + 1}/{(len(totals_df) + batch_size - 1)//batch_size}\")\n",
    "                batch_result = con.execute(batch_query).fetch_arrow_table()\n",
    "                all_results.append(batch_result)\n",
    "                \n",
    "                # Limpieza de memoria explícita entre lotes.\n",
    "                con.unregister('input_table')\n",
    "                con.close()\n",
    "                gc.collect()\n",
    "                con = duckdb.connect()\n",
    "                con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "                con.register('input_table', table)\n",
    "\n",
    "            if not all_results: # Si no hay resultados (ej. tabla de entrada vacía)\n",
    "                 return pa.Table.from_arrays([], names=table.schema.names, schema=table.schema)\n",
    "\n",
    "            result_table = pa.concat_tables(all_results) if len(all_results) > 1 else all_results[0]\n",
    "\n",
    "        else:\n",
    "            # Procesamiento estándar para tablas más pequeñas.\n",
    "            query = \"\"\"\n",
    "                WITH series_totals AS (\n",
    "                    SELECT \n",
    "                        establecimiento,\n",
    "                        material,\n",
    "                        SUM(weekly_volume) AS total_volume\n",
    "                    FROM input_table\n",
    "                    GROUP BY establecimiento, material\n",
    "                )\n",
    "                SELECT t.*\n",
    "                FROM input_table t\n",
    "                JOIN series_totals s\n",
    "                    ON t.establecimiento = s.establecimiento \n",
    "                    AND t.material = s.material\n",
    "                ORDER BY \n",
    "                    s.total_volume DESC,\n",
    "                    t.establecimiento,\n",
    "                    t.material,\n",
    "                    t.week\n",
    "            \"\"\"\n",
    "            result_table = con.execute(query).fetch_arrow_table()\n",
    "    finally:\n",
    "        con.close()\n",
    "    \n",
    "    gc.collect()\n",
    "    return result_table\n",
    "\n",
    "def create_nested_series_format(table: pa.Table, output_path: str = None) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Transforma datos de series temporales a un formato anidado.\n",
    "\n",
    "    Cada fila representa una combinación 'establecimiento'-'material', con sus datos\n",
    "    temporales (semana, volumen, has_promo, is_covid_period) en una lista de diccionarios\n",
    "    dentro de una columna 'series'.\n",
    "\n",
    "    Args:\n",
    "        table: Tabla PyArrow de entrada (datos semanales, ej: salida de 'group_by_week').\n",
    "        output_path: Ruta opcional para guardar el resultado en Parquet.\n",
    "\n",
    "    Returns:\n",
    "        Tabla PyArrow en formato anidado.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "    \n",
    "    try:\n",
    "        con.register('input_table', table)\n",
    "        \n",
    "        # Procesamiento en lotes para tablas grandes para evitar problemas de memoria al crear estructuras anidadas.\n",
    "        if table.num_rows > 1000000: # Umbral para procesamiento en lotes\n",
    "            print(\"Tabla grande detectada para formato anidado, procesando en lotes...\")\n",
    "            \n",
    "            combinations = con.execute(\"\"\"\n",
    "                SELECT \n",
    "                    establecimiento, \n",
    "                    material,\n",
    "                    SUM(weekly_volume) AS total_volume -- Usado para ordenar lotes\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "                ORDER BY total_volume DESC, establecimiento, material\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            batch_size = 500 # Número de combinaciones por lote\n",
    "            all_results = []\n",
    "            \n",
    "            for i in range(0, len(combinations), batch_size):\n",
    "                batch = combinations[i:i+batch_size]\n",
    "                batch_conditions_list = []\n",
    "                for estab, mat, _ in batch: # Ignorar total_volume aquí, solo para ordenación de lotes\n",
    "                    estab_escaped = estab.replace(\"'\", \"''\")\n",
    "                    mat_escaped = mat.replace(\"'\", \"''\")\n",
    "                    batch_conditions_list.append(f\"(establecimiento = '{estab_escaped}' AND material = '{mat_escaped}')\")\n",
    "\n",
    "                if not batch_conditions_list: continue\n",
    "\n",
    "                where_clause_batch = \" OR \".join(batch_conditions_list)\n",
    "                \n",
    "                batch_query = f\"\"\"\n",
    "                    SELECT \n",
    "                        establecimiento,\n",
    "                        material,\n",
    "                        LIST(STRUCT_PACK(\n",
    "                            ds := week::VARCHAR, \n",
    "                            y := weekly_volume,\n",
    "                            has_promo := has_promo,\n",
    "                            is_covid_period := is_covid_period\n",
    "                        ) ORDER BY week) AS series, -- Asegura el orden dentro de la lista anidada\n",
    "                        COUNT(*) AS num_points,\n",
    "                        SUM(weekly_volume) AS total_volume,\n",
    "                        AVG(weekly_volume) AS avg_weekly_volume\n",
    "                    FROM input_table\n",
    "                    WHERE {where_clause_batch}\n",
    "                    GROUP BY establecimiento, material\n",
    "                    ORDER BY SUM(weekly_volume) DESC, establecimiento, material\n",
    "                \"\"\"\n",
    "                \n",
    "                print(f\"Creando series anidadas: Lote {i//batch_size + 1}/{(len(combinations) + batch_size - 1)//batch_size}\")\n",
    "                batch_result = con.execute(batch_query).fetch_arrow_table()\n",
    "                all_results.append(batch_result)\n",
    "                \n",
    "                # Limpieza de memoria explícita\n",
    "                con.unregister('input_table')\n",
    "                con.close()\n",
    "                gc.collect()\n",
    "                con = duckdb.connect()\n",
    "                con.execute(\"PRAGMA memory_limit='4GB'\")\n",
    "                con.register('input_table', table)\n",
    "\n",
    "            if not all_results:\n",
    "                 return pa.Table.from_schema( # Esquema esperado para la tabla anidada\n",
    "                    pa.schema([\n",
    "                        ('establecimiento', pa.string()), ('material', pa.string()),\n",
    "                        ('series', pa.list_(pa.struct([\n",
    "                            ('ds', pa.string()), ('y', pa.float64()), # Ajustar tipos si es necesario\n",
    "                            ('has_promo', pa.int64()), ('is_covid_period', pa.int64())\n",
    "                        ]))),\n",
    "                        ('num_points', pa.int64()), ('total_volume', pa.float64()), ('avg_weekly_volume', pa.float64())\n",
    "                    ])\n",
    "                 )\n",
    "            result_table = pa.concat_tables(all_results) if len(all_results) > 1 else all_results[0]\n",
    "\n",
    "        else:\n",
    "            # Procesamiento estándar para tablas más pequeñas.\n",
    "            query = \"\"\"\n",
    "                SELECT \n",
    "                    establecimiento,\n",
    "                    material,\n",
    "                    LIST(STRUCT_PACK(\n",
    "                        ds := week::VARCHAR, \n",
    "                        y := weekly_volume,\n",
    "                        has_promo := has_promo,\n",
    "                        is_covid_period := is_covid_period\n",
    "                    ) ORDER BY week) AS series, -- Asegura orden dentro de la lista\n",
    "                    COUNT(*) AS num_points,\n",
    "                    SUM(weekly_volume) AS total_volume,\n",
    "                    AVG(weekly_volume) AS avg_weekly_volume\n",
    "                FROM input_table\n",
    "                GROUP BY establecimiento, material\n",
    "                ORDER BY SUM(weekly_volume) DESC, establecimiento, material\n",
    "            \"\"\"\n",
    "            result_table = con.execute(query).fetch_arrow_table()\n",
    "        \n",
    "        if output_path:\n",
    "            print(f\"\\nGuardando formato de series anidadas en: {output_path}\")\n",
    "            # Escritura en fragmentos (chunks) para tablas de resultados grandes\n",
    "            if len(result_table) > 100000: # Umbral para escritura en fragmentos\n",
    "                print(\"Tabla de resultado anidada grande, escribiendo en fragmentos...\")\n",
    "                temp_dir = Path(output_path).parent / f\"temp_nested_{Path(output_path).stem}\"\n",
    "                os.makedirs(temp_dir, exist_ok=True)\n",
    "                \n",
    "                chunk_size = 50000 # Número de series (filas) por fragmento\n",
    "                num_chunks = (len(result_table) + chunk_size - 1) // chunk_size\n",
    "                \n",
    "                for i_chunk in range(num_chunks):\n",
    "                    start_idx = i_chunk * chunk_size\n",
    "                    end_idx = min((i_chunk + 1) * chunk_size, len(result_table))\n",
    "                    chunk = result_table.slice(start_idx, end_idx - start_idx)\n",
    "                    chunk_path = temp_dir / f\"chunk_{i_chunk}.parquet\"\n",
    "                    pq.write_table(chunk, chunk_path, compression='snappy')\n",
    "                    print(f\"  - Guardado fragmento {i_chunk+1}/{num_chunks} en {chunk_path}\")\n",
    "                    del chunk\n",
    "                    gc.collect()\n",
    "                \n",
    "                print(f\"Fusionando {num_chunks} fragmentos en el archivo final...\")\n",
    "                chunk_files = sorted(temp_dir.glob(\"chunk_*.parquet\"))\n",
    "                # Leer todos los fragmentos y concatenarlos antes de escribir el archivo final.\n",
    "                # Esto puede ser intensivo en memoria si el total es muy grande.\n",
    "                # Alternativa: Usar DuckDB para leer múltiples Parquet y escribir uno nuevo si es más eficiente.\n",
    "                tables_to_merge = [pq.read_table(cf) for cf in chunk_files]\n",
    "                merged_table = pa.concat_tables(tables_to_merge)\n",
    "                pq.write_table(merged_table, output_path, compression='snappy')\n",
    "                \n",
    "                # Limpieza de archivos temporales\n",
    "                for cf in chunk_files:\n",
    "                    os.remove(cf)\n",
    "                os.rmdir(temp_dir)\n",
    "                print(f\"Fragmentos fusionados y archivos temporales eliminados.\")\n",
    "            else:\n",
    "                pq.write_table(result_table, output_path, compression='snappy') # Compresión por defecto, considerar 'ZSTD' para mejor ratio.\n",
    "            \n",
    "            print(f\"Guardadas {len(result_table):,} series en {output_path}\")\n",
    "            \n",
    "            # Muestra de la estructura anidada\n",
    "            # Necesario registrar la tabla resultante para consultarla con SQL\n",
    "            con.register('result_table_nested', result_table)\n",
    "            sample = con.execute(\"\"\"\n",
    "                SELECT \n",
    "                    establecimiento, material, num_points, series[1:3] AS sample_points\n",
    "                FROM result_table_nested \n",
    "                LIMIT 1\n",
    "            \"\"\").fetchall()\n",
    "            \n",
    "            if sample:\n",
    "                print(\"\\nMuestra de la estructura anidada:\")\n",
    "                print(f\"Serie para {sample[0][0]}-{sample[0][1]} tiene {sample[0][2]} puntos.\")\n",
    "                print(f\"Primeros puntos de muestra: {sample[0][3]}\")\n",
    "    finally:\n",
    "        con.close()\n",
    "    \n",
    "    gc.collect()\n",
    "    return result_table\n",
    "\n",
    "\n",
    "def list_materials_from_parquet(file_path: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Obtiene una lista de materiales únicos desde un archivo Parquet.\n",
    "\n",
    "    Args:\n",
    "        file_path: Ruta al archivo Parquet.\n",
    "\n",
    "    Returns:\n",
    "        Lista de strings con los nombres de materiales únicos.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        # Asegurar que la ruta se pasa como string a DuckDB\n",
    "        result = con.execute(f\"SELECT DISTINCT material FROM read_parquet('{str(file_path)}')\").fetchdf()['material'].tolist()\n",
    "    finally:\n",
    "        con.close()\n",
    "    return result\n",
    "\n",
    "\n",
    "def filter_by_materials(table: pa.Table) -> pa.Table:\n",
    "    \"\"\"\n",
    "    Filtra la tabla para incluir solo filas cuyos materiales comiencen con los prefijos definidos en MATERIALS_TO_INCLUDE.\n",
    "    \"\"\"\n",
    "    con = duckdb.connect()\n",
    "    try:\n",
    "        # Obtiene todos los materiales distintos del archivo Parquet silver de referencia.\n",
    "        # Esta operación puede ser costosa si el archivo es grande y se llama repetidamente.\n",
    "        # Considerar obtener esta lista una vez si el pipeline lo permite.\n",
    "        distinct_materials_in_silver = list_materials_from_parquet(SILVER_VENTAS_PATH)\n",
    "        \n",
    "        # Filtra los materiales basándose en los prefijos definidos globalmente.\n",
    "        materials_to_actually_include = [\n",
    "            material for material in distinct_materials_in_silver \n",
    "            if material.startswith(MATERIALS_TO_INCLUDE) # MATERIALS_TO_INCLUDE es una tupla de prefijos\n",
    "        ]\n",
    "\n",
    "        if not materials_to_actually_include:\n",
    "            print(\"Advertencia: No se encontraron materiales que coincidan con los prefijos. Se devolverá una tabla vacía.\")\n",
    "            # Devolver una tabla vacía con el mismo esquema que la tabla de entrada.\n",
    "            return table.slice(0, 0)\n",
    "\n",
    "        con.register('input_table', table)\n",
    "        # Construcción de la cláusula IN. Ver comentario en filter_sales_by_not_type sobre parametrización.\n",
    "        materials_sql = \", \".join([f\"'{m.replace(\\\"'\\\", \\\"''\\\")}'\" for m in materials_to_actually_include]) # Escapar apóstrofes\n",
    "        \n",
    "        query = f\"\"\"\n",
    "            SELECT *\n",
    "            FROM input_table\n",
    "            WHERE material IN ({materials_sql})\n",
    "        \"\"\"\n",
    "        result_table = con.sql(query).fetch_arrow_table()\n",
    "        \n",
    "        return result_table\n",
    "    finally:\n",
    "        con.close()\n",
    "\n",
    "\n",
    "# --- Función Principal de Procesamiento ---\n",
    "\n",
    "def process_data(initial_table: pa.Table, processing_functions: list, \n",
    "               show_intermediate: bool = False,\n",
    "               save_result: bool = False,\n",
    "               output_path: str = None, # Cambiado a str, pero Path es más robusto\n",
    "               output_compression: str = 'snappy',\n",
    "               memory_limit_duckdb: str = '8GB') -> pa.Table: # Renombrado para claridad\n",
    "    \"\"\"\n",
    "    Aplica una secuencia de funciones de procesamiento a una tabla PyArrow.\n",
    "\n",
    "    Args:\n",
    "        initial_table: Tabla PyArrow inicial.\n",
    "        processing_functions: Lista de funciones a aplicar. Cada elemento puede ser:\n",
    "                              - Una referencia a función (que toma la tabla como primer arg).\n",
    "                              - Una tupla (función, lista_args_adicionales).\n",
    "                              - Una tupla (función, lista_args_adicionales, dict_kwargs_adicionales).\n",
    "        show_intermediate: Si es True, imprime información y muestra de tablas intermedias.\n",
    "        save_result: Si es True, guarda la tabla final en formato Parquet.\n",
    "        output_path: Ruta para guardar el archivo Parquet (requerido si save_result=True).\n",
    "                     Se recomienda usar pathlib.Path.\n",
    "        output_compression: Algoritmo de compresión para Parquet (ej: 'snappy', 'gzip', 'zstd').\n",
    "        memory_limit_duckdb: Límite de memoria para DuckDB (ej: '8GB').\n",
    "\n",
    "    Returns:\n",
    "        La tabla PyArrow final después de todas las transformaciones.\n",
    "    \"\"\"\n",
    "    current_table = initial_table\n",
    "    \n",
    "    # Conexión DuckDB para mostrar resultados intermedios si se solicita.\n",
    "    # Se gestiona cuidadosamente para liberar memoria.\n",
    "    con_display = None\n",
    "    if show_intermediate:\n",
    "        con_display = duckdb.connect()\n",
    "        con_display.execute(f\"PRAGMA memory_limit='{memory_limit_duckdb}'\")\n",
    "    \n",
    "    try:\n",
    "        for i, func_spec in enumerate(processing_functions):\n",
    "            function_to_apply = None\n",
    "            args = []\n",
    "            kwargs = {}\n",
    "\n",
    "            if callable(func_spec):\n",
    "                function_to_apply = func_spec\n",
    "            elif isinstance(func_spec, tuple) and len(func_spec) >= 1 and callable(func_spec[0]):\n",
    "                function_to_apply = func_spec[0]\n",
    "                if len(func_spec) > 1:\n",
    "                    args = func_spec[1] # Asumimos que es una lista o tupla de argumentos\n",
    "                if len(func_spec) > 2:\n",
    "                    kwargs = func_spec[2] # Asumimos que es un diccionario de kwargs\n",
    "            else:\n",
    "                raise ValueError(f\"Especificación de función inválida en la posición {i} del pipeline: {func_spec}\")\n",
    "            \n",
    "            function_name = function_to_apply.__name__\n",
    "            if show_intermediate:\n",
    "                print(f\"\\n--- Paso {i+1}: Aplicando '{function_name}' ---\")\n",
    "                print(f\"Filas antes: {len(current_table):,}\")\n",
    "            \n",
    "            current_table = function_to_apply(current_table, *args, **kwargs)\n",
    "            \n",
    "            # Forzar recolección de basura después de cada paso para liberar memoria.\n",
    "            # Esto puede tener un impacto en el rendimiento, pero es útil en entornos con memoria limitada.\n",
    "            gc.collect()\n",
    "            \n",
    "            if show_intermediate and con_display:\n",
    "                print(f\"Filas después de '{function_name}': {len(current_table):,}\")\n",
    "                try:\n",
    "                    con_display.register('current_intermediate_table', current_table)\n",
    "                    print(f\"Muestra después de '{function_name}' (primeras 5 filas):\")\n",
    "                    con_display.sql(\"SELECT * FROM current_intermediate_table LIMIT 5\").show()\n",
    "                    # Des-registrar la tabla para liberar la referencia en DuckDB\n",
    "                    con_display.unregister('current_intermediate_table')\n",
    "                except Exception as e:\n",
    "                    print(f\"Error al mostrar tabla intermedia para '{function_name}': {e}\")\n",
    "                    # Intentar resetear la conexión si hay problemas\n",
    "                    if con_display: con_display.close()\n",
    "                    con_display = duckdb.connect()\n",
    "                    con_display.execute(f\"PRAGMA memory_limit='{memory_limit_duckdb}'\")\n",
    "\n",
    "        if save_result:\n",
    "            if output_path is None:\n",
    "                raise ValueError(\"Se debe especificar 'output_path' cuando 'save_result' es True.\")\n",
    "            \n",
    "            output_path_obj = Path(output_path) # Convertir a Path para manejo robusto\n",
    "            print(f\"\\nGuardando resultado final en: {output_path_obj}\")\n",
    "\n",
    "            # Escritura en fragmentos (chunks) para tablas muy grandes para evitar OOM.\n",
    "            # Este umbral es arbitrario y puede ajustarse.\n",
    "            if len(current_table) > 1000000: \n",
    "                print(f\"Tabla final grande ({len(current_table):,} filas), usando escritura en fragmentos...\")\n",
    "                \n",
    "                temp_dir_final = output_path_obj.parent / f\"temp_final_{output_path_obj.stem}\"\n",
    "                os.makedirs(temp_dir_final, exist_ok=True)\n",
    "                \n",
    "                chunk_size = 250000  # Número de filas por fragmento.\n",
    "                num_chunks = (len(current_table) + chunk_size - 1) // chunk_size\n",
    "                \n",
    "                for i_chunk in range(num_chunks):\n",
    "                    start_idx = i_chunk * chunk_size\n",
    "                    end_idx = min((i_chunk + 1) * chunk_size, len(current_table))\n",
    "                    chunk = current_table.slice(start_idx, end_idx - start_idx)\n",
    "                    chunk_path = temp_dir_final / f\"final_chunk_{i_chunk}.parquet\"\n",
    "                    pq.write_table(chunk, chunk_path, compression=output_compression)\n",
    "                    print(f\"  - Guardado fragmento {i_chunk+1}/{num_chunks} en {chunk_path}\")\n",
    "                    del chunk # Liberar memoria del fragmento explícitamente\n",
    "                    gc.collect()\n",
    "                \n",
    "                print(f\"Fusionando {num_chunks} fragmentos en el archivo final: {output_path_obj}...\")\n",
    "                # Este proceso de fusión puede ser intensivo en memoria.\n",
    "                # Una alternativa podría ser usar DuckDB para leer los Parquets y escribir uno nuevo:\n",
    "                # con_merge = duckdb.connect()\n",
    "                # con_merge.execute(f\"COPY (SELECT * FROM read_parquet('{str(temp_dir_final)}/*.parquet')) TO '{str(output_path_obj)}' (FORMAT PARQUET, COMPRESSION '{output_compression}')\")\n",
    "                # con_merge.close()\n",
    "                # La implementación actual es puramente PyArrow:\n",
    "                chunk_files = sorted(temp_dir_final.glob(\"final_chunk_*.parquet\"))\n",
    "                tables_to_merge = [pq.read_table(cf) for cf in chunk_files]\n",
    "                merged_table = pa.concat_tables(tables_to_merge)\n",
    "                pq.write_table(merged_table, output_path_obj, compression=output_compression)\n",
    "                del tables_to_merge, merged_table # Liberar memoria\n",
    "                gc.collect()\n",
    "\n",
    "                # Limpieza de archivos y directorio temporal\n",
    "                for cf in chunk_files:\n",
    "                    os.remove(cf)\n",
    "                os.rmdir(temp_dir_final)\n",
    "                print(f\"Fragmentos fusionados y directorio temporal '{temp_dir_final}' eliminado.\")\n",
    "            else:\n",
    "                # Escritura estándar para tablas más pequeñas.\n",
    "                pq.write_table(current_table, output_path_obj, compression=output_compression)\n",
    "            \n",
    "            print(f\"Guardadas {len(current_table):,} filas en {output_path_obj}\")\n",
    "        \n",
    "        return current_table\n",
    "    \n",
    "    finally:\n",
    "        if con_display:\n",
    "            con_display.close()\n",
    "        gc.collect() # Recolección de basura final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb39789",
   "metadata": {},
   "outputs": [],
   "source": [
    "con = duckdb.connect()\n",
    "\n",
    "# 1. Cargar los datos iniciales desde un archivo Parquet a una tabla PyArrow\n",
    "print(f\"Cargando datos iniciales desde: {SILVER_VENTAS_PATH}\")\n",
    "# Lee el archivo Parquet y carga su contenido en una tabla de PyArrow\n",
    "initial_table = con.sql(f\"SELECT * FROM read_parquet('{str(SILVER_VENTAS_PATH)}')\").fetch_arrow_table()\n",
    "print(f\"Filas iniciales cargadas: {len(initial_table):,}\")\n",
    "\n",
    "# Registrar la tabla PyArrow en DuckDB para poder consultarla mediante SQL\n",
    "con.register('initial_table_arrow', initial_table) # Se cambió el nombre para evitar posible colisión con tablas SQL\n",
    "\n",
    "# Mostrar las primeras 5 filas de la tabla cargada para verificación\n",
    "print(\"\\nTabla Silver Inicial (primeras 5 filas, desde PyArrow registrada):\")\n",
    "con.sql(\"SELECT * FROM initial_table_arrow LIMIT 5\").show()\n",
    "\n",
    "# 2. Definición del pipeline de procesamiento de datos\n",
    "processing_pipeline = [\n",
    "    (filter_sales_by_not_type, [TIPOS_A_EXCLUIR]), # Filtrar ventas por tipo (excluyendo ciertos tipos)\n",
    "    promoid_to_boolean,                           # Convertir promo_id a un valor booleano\n",
    "    (remove_columns, [COLUMNS_TO_REMOVE]),        # Eliminar columnas no necesarias\n",
    "    covid_flag,                                   # Añadir una bandera/indicador de COVID\n",
    "    (filter_by_string_in_column, ['establecimiento', '81']), # Filtrar por cadena en la columna 'establecimiento'\n",
    "    filter_by_materials,                          # Filtrar por materiales específicos\n",
    "    group_by_week,                                # Agrupar los datos por semana\n",
    "    fill_time_series_gaps,                        # Rellenar huecos en la serie temporal\n",
    "    (filter_by_min_weeks, [12]),                  # Filtrar series con un mínimo de semanas de datos\n",
    "    sort_series_by_volume                         # Ordenar las series por volumen\n",
    "]\n",
    "\n",
    "# 3. Ejecutar el pipeline de procesamiento\n",
    "print(\"\\nIniciando el procesamiento de datos a través del pipeline...\")\n",
    "final_table = process_data(\n",
    "    input_table=initial_table, # La tabla PyArrow de entrada\n",
    "    pipeline_steps=processing_pipeline,\n",
    "    show_intermediate=True,    # Mostrar resultados intermedios (útil para depuración)\n",
    "    save_result=True,          # Guardar la tabla resultante\n",
    "    output_path=GOLD_VENTAS_WEEKLY_PATH # Ruta para guardar la tabla procesada (capa Gold)\n",
    ")\n",
    "\n",
    "print(f\"Procesamiento completado. Tabla final generada con {len(final_table):,} filas.\")\n",
    "if GOLD_VENTAS_WEEKLY_PATH and save_result: # save_result también debería ser True\n",
    "    print(f\"Tabla final guardada en: {GOLD_VENTAS_WEEKLY_PATH}\")\n",
    "\n",
    "con.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbb0c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dab82c-c721-4156-8a7c-6f75a36e3481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading initial data from: ../data/silver_ventas_establecimiento.parquet\n",
      "Initial rows: 67,918,456\n",
      "\n",
      "Initial Silver Table (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬──────────────────────┬───────────┬──────────────┬─────────────────┐\n",
      "│ establecimiento │ material │       calday        │       promo_id       │ volume_ap │ cantidad_umb │      tipo       │\n",
      "│     varchar     │ varchar  │      timestamp      │       varchar        │  double   │    double    │     varchar     │\n",
      "├─────────────────┼──────────┼─────────────────────┼──────────────────────┼───────────┼──────────────┼─────────────────┤\n",
      "│ 8100240876      │ TB8      │ 2024-11-26 00:00:00 │ NULL                 │       8.0 │          1.0 │ Bar Cervecería  │\n",
      "│ 8100032055      │ PI13     │ 2024-11-26 00:00:00 │ NULL                 │      7.92 │          1.0 │ Restaurante     │\n",
      "│ 8100258434      │ FL13SPN  │ 2024-11-26 00:00:00 │ NULL                 │      23.1 │          2.0 │ Bar Cervecería  │\n",
      "│ 8100036860      │ VO13     │ 2024-11-26 00:00:00 │ 00000000000080619348 │       0.0 │          0.0 │ Bar Cervecería  │\n",
      "│ 8100168246      │ FL32SP   │ 2024-11-26 00:00:00 │ NULL                 │      36.0 │          2.0 │ Pub y Discoteca │\n",
      "└─────────────────┴──────────┴─────────────────────┴──────────────────────┴───────────┴──────────────┴─────────────────┘\n",
      "\n",
      "\n",
      "Step 1: Applying filter_sales_by_not_type\n",
      "Rows before: 67,918,456\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after filter_sales_by_not_type (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬──────────┬───────────┬──────────────┬────────────────┐\n",
      "│ establecimiento │ material │       calday        │ promo_id │ volume_ap │ cantidad_umb │      tipo      │\n",
      "│     varchar     │ varchar  │      timestamp      │ varchar  │  double   │    double    │    varchar     │\n",
      "├─────────────────┼──────────┼─────────────────────┼──────────┼───────────┼──────────────┼────────────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          2.0 │ Restaurante    │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │ NULL     │     47.52 │          6.0 │ Restaurante    │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          1.0 │ Bar Cervecería │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │ NULL     │      60.0 │          2.0 │ Restaurante    │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │ NULL     │      54.0 │          3.0 │ Bar Cervecería │\n",
      "└─────────────────┴──────────┴─────────────────────┴──────────┴───────────┴──────────────┴────────────────┘\n",
      "\n",
      "\n",
      "Step 2: Applying promoid_to_boolean\n",
      "Rows before: 66,740,407\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after promoid_to_boolean (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬──────────┬───────────┬──────────────┬────────────────┬───────────┐\n",
      "│ establecimiento │ material │       calday        │ promo_id │ volume_ap │ cantidad_umb │      tipo      │ has_promo │\n",
      "│     varchar     │ varchar  │      timestamp      │ varchar  │  double   │    double    │    varchar     │   int32   │\n",
      "├─────────────────┼──────────┼─────────────────────┼──────────┼───────────┼──────────────┼────────────────┼───────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          2.0 │ Restaurante    │         0 │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │ NULL     │     47.52 │          6.0 │ Restaurante    │         0 │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │ NULL     │      20.0 │          1.0 │ Bar Cervecería │         0 │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │ NULL     │      60.0 │          2.0 │ Restaurante    │         0 │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │ NULL     │      54.0 │          3.0 │ Bar Cervecería │         0 │\n",
      "└─────────────────┴──────────┴─────────────────────┴──────────┴───────────┴──────────────┴────────────────┴───────────┘\n",
      "\n",
      "\n",
      "Step 3: Applying remove_columns\n",
      "Rows before: 66,740,407\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after remove_columns (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬───────────┬──────────────┬───────────┐\n",
      "│ establecimiento │ material │       calday        │ volume_ap │ cantidad_umb │ has_promo │\n",
      "│     varchar     │ varchar  │      timestamp      │  double   │    double    │   int32   │\n",
      "├─────────────────┼──────────┼─────────────────────┼───────────┼──────────────┼───────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │      20.0 │          2.0 │         0 │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │     47.52 │          6.0 │         0 │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │      20.0 │          1.0 │         0 │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │      60.0 │          2.0 │         0 │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │      54.0 │          3.0 │         0 │\n",
      "└─────────────────┴──────────┴─────────────────────┴───────────┴──────────────┴───────────┘\n",
      "\n",
      "\n",
      "Step 4: Applying covid_flag\n",
      "Rows before: 66,740,407\n",
      "Rows after: 66,740,407\n",
      "\n",
      "Sample after covid_flag (first 5 rows):\n",
      "┌─────────────────┬──────────┬─────────────────────┬───────────┬──────────────┬───────────┬─────────────────┐\n",
      "│ establecimiento │ material │       calday        │ volume_ap │ cantidad_umb │ has_promo │ is_covid_period │\n",
      "│     varchar     │ varchar  │      timestamp      │  double   │    double    │   int32   │      int32      │\n",
      "├─────────────────┼──────────┼─────────────────────┼───────────┼──────────────┼───────────┼─────────────────┤\n",
      "│ 8100013803      │ VE12     │ 2024-11-26 00:00:00 │      20.0 │          2.0 │         0 │               0 │\n",
      "│ 8100233778      │ ED13LTW  │ 2024-11-26 00:00:00 │     47.52 │          6.0 │         0 │               0 │\n",
      "│ 8100429845      │ VO20     │ 2024-11-26 00:00:00 │      20.0 │          1.0 │         0 │               0 │\n",
      "│ 8100095340      │ ED30     │ 2024-11-26 00:00:00 │      60.0 │          2.0 │         0 │               0 │\n",
      "│ 8100033887      │ VE32SP   │ 2024-11-26 00:00:00 │      54.0 │          3.0 │         0 │               0 │\n",
      "└─────────────────┴──────────┴─────────────────────┴───────────┴──────────────┴───────────┴─────────────────┘\n",
      "\n",
      "\n",
      "Step 5: Applying filter_by_string_in_column\n",
      "Rows before: 66,740,407\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# 1. Load the initial data into a PyArrow Table\n",
    "print(f\"Loading initial data from: {SILVER_VENTAS_PATH}\")\n",
    "initial_table = con.sql(f\"SELECT * FROM read_parquet('{SILVER_VENTAS_PATH}')\").fetch_arrow_table()\n",
    "print(f\"Initial rows: {len(initial_table):,}\")\n",
    "\n",
    "# Register the table for querying\n",
    "con.register('initial_table', initial_table)\n",
    "# Show first 5 rows\n",
    "print(\"\\nInitial Silver Table (first 5 rows):\")\n",
    "con.sql(\"SELECT * FROM initial_table LIMIT 5\").show()\n",
    "\n",
    "processing_pipeline = [\n",
    "        (filter_sales_by_not_type, [TIPOS_A_EXCLUIR]),\n",
    "        promoid_to_boolean,\n",
    "        (remove_columns, [COLUMNS_TO_REMOVE]),\n",
    "        covid_flag,\n",
    "        (filter_by_string_in_column, ['establecimiento', '81']),\n",
    "        filter_by_materials,\n",
    "        group_by_week,\n",
    "        fill_time_series_gaps,\n",
    "        (filter_by_min_weeks, [12]),\n",
    "        sort_series_by_volume\n",
    "    ]\n",
    "\n",
    "# Process the data through all steps\n",
    "final_table = process_data(\n",
    "    initial_table, \n",
    "    processing_pipeline, \n",
    "    show_intermediate=True, \n",
    "    save_result=True, \n",
    "    output_path=GOLD_VENTAS_WEEKLY_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f67462d-14fd-4776-9eb2-867c68f3d34b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
