{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2f001a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "import pyarrow as pa\n",
    "import pyarrow.compute as pc\n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "\n",
    "import pandas as pd # Only used for date range and holiday generation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46d322f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOLD_VENTAS_WEEKLY_PATH = '../data/gold_ventas_semanales.parquet'\n",
    "GOLD_FEATURE_SCALER_PATH = '../models/clustering/feature_scaler.joblib'\n",
    "GOLD_FEATURES_PATH = '../models/clustering/gold_ventas_semanales_clustered.parquet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff55499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_clustering(table_path: str):\n",
    "# --- 1. Preparar Datos de Entrada como PyArrow Table ---\n",
    "\n",
    "    arrow_table = pq.read_table(table_path)\n",
    "\n",
    "\n",
    "    print(\"--- Tabla Arrow de Entrada ---\")\n",
    "    print(arrow_table)\n",
    "    print(\"\\nEsquema:\")\n",
    "    print(arrow_table.schema)\n",
    "\n",
    "    # --- 2. Conectar a DuckDB y Registrar la Tabla Arrow ---\n",
    "    con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "    # Registrar la tabla Arrow para que DuckDB pueda consultarla\n",
    "    con.register('sales_data', arrow_table)\n",
    "    print(\"--- Tabla Arrow Registrada en DuckDB ---\")\n",
    "    print(con.sql(\"SELECT * FROM sales_data LIMIT 5\").fetch_arrow_table())\n",
    "    # --- 3. Construir y Ejecutar la Consulta SQL para Extraer Features ---\n",
    "\n",
    "    # Usaremos CTEs (Common Table Expressions) para organizar el cálculo, especialmente para ADI\n",
    "    sql_query = \"\"\"\n",
    "    WITH WeeklyData AS (\n",
    "        -- Convertir fecha a número de semana o algo secuencial y ordenable\n",
    "        SELECT\n",
    "            establecimiento,\n",
    "            material,\n",
    "            epoch(week) AS week_epoch, -- Usar epoch para cálculos de diferencia simples\n",
    "            weekly_volume,\n",
    "            has_promo,\n",
    "            is_covid_period\n",
    "        FROM sales_data\n",
    "    ),\n",
    "    SalesWeeks AS (\n",
    "        -- Identificar las semanas con ventas para calcular ADI\n",
    "        SELECT\n",
    "            establecimiento,\n",
    "            material,\n",
    "            week_epoch\n",
    "        FROM WeeklyData\n",
    "        WHERE weekly_volume > 0\n",
    "    ),\n",
    "    LaggedSalesWeeks AS (\n",
    "        -- Calcular la semana anterior con ventas para cada semana de venta\n",
    "        SELECT\n",
    "            establecimiento,\n",
    "            material,\n",
    "            week_epoch,\n",
    "            LAG(week_epoch, 1) OVER (PARTITION BY establecimiento, material ORDER BY week_epoch) AS prev_sale_epoch\n",
    "        FROM SalesWeeks\n",
    "    ),\n",
    "    Intervals AS (\n",
    "        -- Calcular los intervalos en segundos entre ventas\n",
    "        SELECT\n",
    "            establecimiento,\n",
    "            material,\n",
    "            (week_epoch - prev_sale_epoch) AS interval_seconds\n",
    "        FROM LaggedSalesWeeks\n",
    "        WHERE prev_sale_epoch IS NOT NULL\n",
    "    ),\n",
    "    AdiAvg AS (\n",
    "        -- Calcular el ADI promedio en segundos y convertir a semanas (aprox)\n",
    "        SELECT\n",
    "            establecimiento,\n",
    "            material,\n",
    "            AVG(interval_seconds) / (60*60*24*7) AS adi_weeks -- Segundos a semanas\n",
    "        FROM Intervals\n",
    "        GROUP BY establecimiento, material\n",
    "    ),\n",
    "    BaseFeatures AS (\n",
    "        -- Calcular la mayoría de las features usando agregaciones estándar y condicionales\n",
    "        SELECT\n",
    "            establecimiento,\n",
    "            material,\n",
    "            -- Volumen / Magnitud\n",
    "            SUM(weekly_volume) AS total_liters,\n",
    "            AVG(weekly_volume) AS mean_liters,\n",
    "            MEDIAN(weekly_volume) AS median_liters,\n",
    "            MAX(weekly_volume) AS max_liters,\n",
    "            STDDEV_SAMP(weekly_volume) AS std_liters, -- Desviación estándar muestral\n",
    "            -- Intermitencia\n",
    "            COUNT(*) AS num_weeks,\n",
    "            COUNT(CASE WHEN weekly_volume > 0 THEN 1 ELSE NULL END) AS nonzero_weeks_count,\n",
    "            AVG(CASE WHEN weekly_volume = 0 THEN 1.0 ELSE 0.0 END) AS zero_ratio,\n",
    "            AVG(weekly_volume) FILTER (WHERE weekly_volume > 0) AS mean_nonzero_liters,\n",
    "            MEDIAN(weekly_volume) FILTER (WHERE weekly_volume > 0) AS median_nonzero_liters,\n",
    "            STDDEV_SAMP(weekly_volume) FILTER (WHERE weekly_volume > 0) AS std_nonzero_liters,\n",
    "            -- Respuesta a Eventos\n",
    "            AVG(weekly_volume) FILTER (WHERE has_promo = 1) AS mean_liters_promo,\n",
    "            AVG(weekly_volume) FILTER (WHERE has_promo = 0) AS mean_liters_no_promo\n",
    "            -- Agrega aquí cálculos para 'is_covid_period' si es necesario\n",
    "        FROM WeeklyData\n",
    "        GROUP BY establecimiento, material\n",
    "    )\n",
    "    -- Query Final: Unir features base con ADI y calcular CV2, Promo Lift\n",
    "    SELECT\n",
    "        bf.establecimiento,\n",
    "        bf.material,\n",
    "        bf.total_liters,\n",
    "        bf.mean_liters,\n",
    "        bf.median_liters,\n",
    "        bf.max_liters,\n",
    "        COALESCE(bf.std_liters, 0) as std_liters, -- Coalesce para stddev de grupos con 1 elemento\n",
    "        bf.num_weeks,\n",
    "        bf.nonzero_weeks_count,\n",
    "        bf.zero_ratio,\n",
    "        bf.mean_nonzero_liters,\n",
    "        bf.median_nonzero_liters,\n",
    "        COALESCE(bf.std_nonzero_liters, 0) as std_nonzero_liters,\n",
    "        -- Calcular CV2 (manejar división por cero)\n",
    "        CASE\n",
    "            WHEN bf.mean_nonzero_liters IS NOT NULL AND bf.mean_nonzero_liters != 0\n",
    "            THEN pow(COALESCE(bf.std_nonzero_liters, 0) / bf.mean_nonzero_liters, 2)\n",
    "            ELSE NULL -- O 0 si prefieres\n",
    "        END AS cv_squared,\n",
    "        -- Unir ADI\n",
    "        COALESCE(adi.adi_weeks, bf.num_weeks) AS adi, -- Si no hay ADI (pocas ventas), usar num_weeks como valor alto? O NULL?\n",
    "        -- Calcular Promo Lift (manejar división por cero)\n",
    "        CASE\n",
    "            WHEN bf.mean_liters_no_promo IS NOT NULL AND bf.mean_liters_no_promo != 0\n",
    "            THEN bf.mean_liters_promo / bf.mean_liters_no_promo\n",
    "            ELSE NULL -- O 1 si no hay efecto medible o datos base\n",
    "        END AS promo_lift\n",
    "    FROM BaseFeatures bf\n",
    "    LEFT JOIN AdiAvg adi\n",
    "        ON bf.establecimiento = adi.establecimiento AND bf.material = adi.material\n",
    "    ORDER BY bf.establecimiento, bf.material;\n",
    "    \"\"\"\n",
    "\n",
    "    # Ejecutar la consulta y obtener el resultado como una Tabla Arrow\n",
    "    features_arrow_table = con.execute(sql_query).arrow()\n",
    "\n",
    "    # Cerrar conexión DuckDB\n",
    "    con.close()\n",
    "\n",
    "    print(\"\\n--- Tabla Arrow con Features Agregadas (Antes de Escalar y Limpiar) ---\")\n",
    "    print(features_arrow_table)\n",
    "    print(\"\\nEsquema:\")\n",
    "    print(features_arrow_table.schema)\n",
    "\n",
    "    features_arrow_table = fill_null_column(features_arrow_table, 'cv_squared', 0.0)\n",
    "    features_arrow_table = fill_null_column(features_arrow_table, 'promo_lift', 1.0)\n",
    "    features_arrow_table = fill_null_column(features_arrow_table, 'mean_nonzero_liters', 0.0)\n",
    "    features_arrow_table = fill_null_column(features_arrow_table, 'median_nonzero_liters', 0.0)\n",
    "    import pyarrow.compute as pc\n",
    "    max_weeks = pc.max(features_arrow_table['num_weeks']).as_py()\n",
    "    features_arrow_table = fill_null_column(features_arrow_table, 'adi', float(max_weeks))\n",
    "\n",
    "    print(\"\\n--- Tabla Arrow con Features Limpias ---\")\n",
    "    print(features_arrow_table.slice(0, 5)) # Mostrar primeras filas\n",
    "\n",
    "    # --- 5. Escalar las Features usando NumPy y Scikit-learn ---\n",
    "\n",
    "    # Nombres de las columnas de features a escalar\n",
    "    feature_columns_for_clustering = [\n",
    "        'total_liters', 'mean_liters', 'median_liters', 'max_liters', 'std_liters',\n",
    "        'nonzero_weeks_count', 'zero_ratio', 'mean_nonzero_liters', 'median_nonzero_liters',\n",
    "        'std_nonzero_liters', 'cv_squared', 'adi', 'promo_lift'\n",
    "        # 'num_weeks' podría no ser necesaria si usas 'zero_ratio' o 'nonzero_weeks_count'\n",
    "    ]\n",
    "    # Asegurarse que todas las columnas seleccionadas existen\n",
    "    feature_columns_for_clustering = [col for col in feature_columns_for_clustering if col in features_arrow_table.schema.names]\n",
    "\n",
    "\n",
    "    # Extraer solo las columnas de features a un array NumPy\n",
    "    feature_arrays = [features_arrow_table.column(col_name).to_numpy(zero_copy_only=False)\n",
    "                    for col_name in feature_columns_for_clustering]\n",
    "    features_numpy = np.stack(feature_arrays, axis=1)\n",
    "\n",
    "    # Escalar\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features_numpy = scaler.fit_transform(features_numpy)\n",
    "    # Guardar el scaler\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    scaler_path = GOLD_FEATURE_SCALER_PATH\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"Saved scaler to {scaler_path}\")\n",
    "    # --- 6. Crear la Tabla Arrow Final para Clustering ---\n",
    "\n",
    "    # Crear nuevas columnas Arrow a partir de los arrays NumPy escalados\n",
    "    scaled_arrow_cols = [pa.array(scaled_features_numpy[:, i]) for i in range(scaled_features_numpy.shape[1])]\n",
    "\n",
    "    # Seleccionar las columnas identificadoras de la tabla original\n",
    "    id_cols = [features_arrow_table.column('establecimiento'), features_arrow_table.column('material')]\n",
    "    id_names = ['establecimiento', 'material']\n",
    "\n",
    "    # Combinar identificadores y features escaladas en una nueva tabla\n",
    "    final_arrow_table = pa.Table.from_arrays(\n",
    "        id_cols + scaled_arrow_cols,\n",
    "        names=id_names + feature_columns_for_clustering\n",
    "    )\n",
    "\n",
    "    print(\"\\n--- Tabla Arrow Final Lista para Clustering (Features Escaladas) ---\")\n",
    "    print(final_arrow_table)\n",
    "    print(\"\\nEsquema Final:\")\n",
    "    print(final_arrow_table.schema)\n",
    "\n",
    "\n",
    "    # Guardar como parquet file\n",
    "    pq.write_table(final_arrow_table, GOLD_FEATURES_PATH)\n",
    "    print(f\"Saved features table to {GOLD_FEATURES_PATH}\")\n",
    "\n",
    "    # Para asegurarnos que se guarda correctamente, comprobamos si el archivo existe\n",
    "    if os.path.exists(GOLD_FEATURES_PATH):\n",
    "        print(f\"Confirmed: Features file exists at {GOLD_FEATURES_PATH}\")\n",
    "    else:\n",
    "        print(f\"Warning: Features file was not created at {GOLD_FEATURES_PATH}\")\n",
    "\n",
    "\n",
    "    return features_arrow_table\n",
    "\n",
    "    # --- 4. Limpieza de NULLs/NaNs en PyArrow (si es necesario) ---\n",
    "    # DuckDB con COALESCE ya manejó algunos NULLs, pero podríamos tener otros\n",
    "    # Por ejemplo, promo_lift puede ser NULL. Rellenamos con 1 (sin efecto)\n",
    "    # cv_squared puede ser NULL. Rellenamos con 0 (sin varianza relativa)\n",
    "    # mean/median_nonzero_liters pueden ser NULL si nunca hubo ventas. Rellenamos con 0.\n",
    "\n",
    "def fill_null_column(table, col_name, fill_value):\n",
    "    \"\"\"Helper para rellenar NULLs en una columna de una tabla Arrow.\"\"\"\n",
    "    col_index = table.schema.get_field_index(col_name)\n",
    "    if col_index == -1:\n",
    "        return table # Columna no encontrada\n",
    "\n",
    "    col = table.column(col_name)\n",
    "    mask = pc.is_null(col)\n",
    "    filled_col = pc.if_else(mask, pa.scalar(fill_value, type=col.type), col)\n",
    "    return table.set_column(col_index, pa.field(col_name, filled_col.type), filled_col)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7553d75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2863726d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting feature engineering clustering...\n",
      "../data/gold_ventas_semanales.parquet\n",
      "--- Tabla Arrow de Entrada ---\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "week: date32[day]\n",
      "has_promo: int32\n",
      "weekly_volume: double\n",
      "is_covid_period: int32\n",
      "----\n",
      "establecimiento: [[\"8100441311\",\"8100441311\",\"8100441311\",\"8100441311\",\"8100441311\",...,\"8100142319\",\"8100142319\",\"8100142319\",\"8100142319\",\"8100142319\"],[\"8100142319\",\"8100142319\",\"8100142319\",\"8100142319\",\"8100142319\",...,\"8100093438\",\"8100093438\",\"8100093438\",\"8100093438\",\"8100093438\"],...,[\"8100201874\",\"8100201874\",\"8100201874\",\"8100201874\",\"8100201874\",...,\"8100394850\",\"8100394850\",\"8100394850\",\"8100394850\",\"8100394850\"],[\"8100394850\",\"8100394850\",\"8100394894\",\"8100394894\",\"8100394894\",...,\"8100023583\",\"8100023583\",\"8100023583\",\"8100023583\",\"8100023583\"]]\n",
      "material: [[\"ED13LT\",\"ED13LT\",\"ED13LT\",\"ED13LT\",\"ED13LT\",...,\"ED50\",\"ED50\",\"ED50\",\"ED50\",\"ED50\"],[\"ED50\",\"ED50\",\"ED50\",\"ED50\",\"ED50\",...,\"ED50\",\"ED50\",\"ED50\",\"ED50\",\"ED50\"],...,[\"VI13\",\"VI13\",\"VI13\",\"VI13\",\"VI13\",...,\"FDT13LT\",\"FDT13LT\",\"FDT13LT\",\"FDT13LT\",\"FDT13LT\"],[\"FDT13LT\",\"FDT13LT\",\"ED13\",\"ED13\",\"ED13\",...,\"ED11TC13\",\"ED11TC13\",\"ED11TC13\",\"ED11TC13\",\"ED11TC13\"]]\n",
      "week: [[2023-04-24,2023-05-01,2023-05-08,2023-05-15,2023-05-22,...,2023-09-11,2023-09-18,2023-09-25,2023-10-02,2023-10-09],[2023-10-16,2023-10-23,2023-10-30,2023-11-06,2023-11-13,...,2022-05-16,2022-05-23,2022-05-30,2022-06-06,2022-06-13],...,[2022-08-01,2022-08-08,2022-08-15,2022-08-22,2022-08-29,...,2024-01-15,2024-01-22,2024-01-29,2024-02-05,2024-02-12],[2024-02-19,2024-02-26,2022-09-05,2022-09-12,2022-09-19,...,2024-12-02,2024-12-09,2024-12-16,2024-12-23,2024-12-30]]\n",
      "has_promo: [[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]\n",
      "weekly_volume: [[21954.24,0,0,0,0,...,400,200,250,200,800],[150,200,300,250,300,...,250,250,500,0,300],...,[0,0,0,0,0,...,0,0,0,15.84,0],[0,-15.84,15.84,0,0,...,427,-2,-425,0,1509]]\n",
      "is_covid_period: [[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0],...,[0,0,0,0,0,...,0,0,0,0,0],[0,0,0,0,0,...,0,0,0,0,0]]\n",
      "\n",
      "Esquema:\n",
      "establecimiento: string\n",
      "material: string\n",
      "week: date32[day]\n",
      "has_promo: int32\n",
      "weekly_volume: double\n",
      "is_covid_period: int32\n",
      "--- Tabla Arrow Registrada en DuckDB ---\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "week: date32[day]\n",
      "has_promo: int32\n",
      "weekly_volume: double\n",
      "is_covid_period: int32\n",
      "----\n",
      "establecimiento: [[\"8100441311\",\"8100441311\",\"8100441311\",\"8100441311\",\"8100441311\"]]\n",
      "material: [[\"ED13LT\",\"ED13LT\",\"ED13LT\",\"ED13LT\",\"ED13LT\"]]\n",
      "week: [[2023-04-24,2023-05-01,2023-05-08,2023-05-15,2023-05-22]]\n",
      "has_promo: [[0,0,0,0,0]]\n",
      "weekly_volume: [[21954.24,0,0,0,0]]\n",
      "is_covid_period: [[0,0,0,0,0]]\n",
      "\n",
      "--- Tabla Arrow con Features Agregadas (Antes de Escalar y Limpiar) ---\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "num_weeks: int64\n",
      "nonzero_weeks_count: int64\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "----\n",
      "establecimiento: [[\"8100000005\",\"8100000005\",\"8100000005\",\"8100000006\",\"8100000006\",...,\"8100468675\",\"8100468675\",\"8100468675\",\"8100468675\",\"8100468675\"]]\n",
      "material: [[\"DL13\",\"ED13\",\"FDT13\",\"ED13\",\"FD13\",...,\"ED13LT\",\"ED30\",\"FD13\",\"FDL13\",\"FDT13\"]]\n",
      "total_liters: [[39.6,118.80000000000001,110.88000000000001,1425.5999999999995,197.99999999999997,...,39.6,16590,926.6399999999993,285.11999999999983,601.9200000000002]]\n",
      "mean_liters: [[1.5230769230769232,0.8366197183098593,3.696,30.991304347826077,7.615384615384614,...,0.3046153846153846,115.20833333333333,6.434999999999995,2.1119999999999988,4.2689361702127675]]\n",
      "median_liters: [[0,0,0,39.6,0,...,0,120,7.92,0,0]]\n",
      "max_liters: [[15.84,39.6,15.84,79.2,23.759999999999998,...,15.84,330,31.68,23.759999999999998,23.759999999999998]]\n",
      "std_liters: [[3.892407757911053,4.494875235896442,5.397047468691594,22.139896076108162,10.137790382827701,...,1.819408777745307,64.4174405216215,7.2532836065414585,4.348740219021747,5.561568342797824]]\n",
      "num_weeks: [[26,142,30,46,26,...,130,144,144,135,141]]\n",
      "nonzero_weeks_count: [[4,7,11,33,10,...,4,134,74,30,61]]\n",
      "zero_ratio: [[0.8461538461538461,0.9436619718309859,0.6333333333333333,0.2826086956521739,0.6153846153846154,...,0.9692307692307692,0.06944444444444445,0.4861111111111111,0.7777777777777778,0.5673758865248227]]\n",
      "...\n",
      "\n",
      "Esquema:\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "num_weeks: int64\n",
      "nonzero_weeks_count: int64\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "\n",
      "--- Tabla Arrow con Features Limpias ---\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "num_weeks: int64\n",
      "nonzero_weeks_count: int64\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "----\n",
      "establecimiento: [[\"8100000005\",\"8100000005\",\"8100000005\",\"8100000006\",\"8100000006\"]]\n",
      "material: [[\"DL13\",\"ED13\",\"FDT13\",\"ED13\",\"FD13\"]]\n",
      "total_liters: [[39.6,118.80000000000001,110.88000000000001,1425.5999999999995,197.99999999999997]]\n",
      "mean_liters: [[1.5230769230769232,0.8366197183098593,3.696,30.991304347826077,7.615384615384614]]\n",
      "median_liters: [[0,0,0,39.6,0]]\n",
      "max_liters: [[15.84,39.6,15.84,79.2,23.759999999999998]]\n",
      "std_liters: [[3.892407757911053,4.494875235896442,5.397047468691594,22.139896076108162,10.137790382827701]]\n",
      "num_weeks: [[26,142,30,46,26]]\n",
      "nonzero_weeks_count: [[4,7,11,33,10]]\n",
      "zero_ratio: [[0.8461538461538461,0.9436619718309859,0.6333333333333333,0.2826086956521739,0.6153846153846154]]\n",
      "...\n",
      "Saved scaler to ../models/clustering/feature_scaler.joblib\n",
      "\n",
      "--- Tabla Arrow Final Lista para Clustering (Features Escaladas) ---\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "nonzero_weeks_count: double\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "----\n",
      "establecimiento: [[\"8100000005\",\"8100000005\",\"8100000005\",\"8100000006\",\"8100000006\",...,\"8100468675\",\"8100468675\",\"8100468675\",\"8100468675\",\"8100468675\"]]\n",
      "material: [[\"DL13\",\"ED13\",\"FDT13\",\"ED13\",\"FD13\",...,\"ED13LT\",\"ED30\",\"FD13\",\"FDL13\",\"FDT13\"]]\n",
      "total_liters: [[-0.2715087116995092,-0.2603856571892176,-0.2614979626402467,-0.07685525776940644,-0.249262602678926,...,-0.2715087116995092,2.0528726186935455,-0.14693050118424347,-0.23702724271760528,-0.19253502467643888]]\n",
      "mean_liters: [[-0.25617493255741863,-0.26653647342670717,-0.22337633839742496,0.18862518125395455,-0.16421625734248307,...,-0.27456666760040577,1.459816133382566,-0.18203325066537687,-0.24728559395330824,-0.21472830979210544]]\n",
      "median_liters: [[-0.27636592137944144,-0.27636592137944144,-0.27636592137944144,0.5712362332950929,-0.27636592137944144,...,-0.27636592137944144,2.2921254564221774,-0.1068454904445346,-0.27636592137944144,-0.27636592137944144]]\n",
      "max_liters: [[-0.18195482062772375,-0.13297433305246037,-0.18195482062772375,-0.05134018709368802,-0.16562799143596932,...,-0.18195482062772375,0.46567607064520344,-0.14930116224421483,-0.16562799143596932,-0.16562799143596932]]\n",
      "std_liters: [[-0.18059696495042,-0.1745128199407905,-0.16540204319589208,0.0036791479456249627,-0.11752665008848688,...,-0.20153158276965485,0.4306278554803349,-0.14665645069925104,-0.17598859525836186,-0.1637405944177431]]\n",
      "nonzero_weeks_count: [[-0.8439649586683631,-0.7603584214800341,-0.6488830385622621,-0.03576843251451631,-0.6767518842917051,...,-0.8439649586683631,2.7789849861592257,1.1068542423926464,-0.11937496970284528,0.7445592479098875]]\n",
      "zero_ratio: [[0.7234581318408011,1.0973150855853746,-0.09251928609133686,-1.4372363928564,-0.16133665868802294,...,1.1953486867895073,-2.254531741837232,-0.6569855922712996,0.46129671242485326,-0.34540744253326294]]\n",
      "mean_nonzero_liters: [[-0.16368689887416402,-0.12353292968862031,-0.16280577729329002,-0.0006794064124741529,-0.11522521192609404,...,-0.16368689887416402,0.3938964816197086,-0.1508511007095401,-0.16562536635208686,-0.1638457896510429]]\n",
      "...\n",
      "\n",
      "Esquema Final:\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "nonzero_weeks_count: double\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "Saved features table to ../models/clustering/gold_ventas_semanales_clustered.parquet\n",
      "Confirmed: Features file exists at ../models/clustering/gold_ventas_semanales_clustered.parquet\n",
      "\n",
      "--- Processed Table Sample (First 10 rows) ---\n",
      "  establecimiento material  total_liters  mean_liters  median_liters  \\\n",
      "0      8100000005     DL13         39.60     1.523077            0.0   \n",
      "1      8100000005     ED13        118.80     0.836620            0.0   \n",
      "2      8100000005    FDT13        110.88     3.696000            0.0   \n",
      "3      8100000006     ED13       1425.60    30.991304           39.6   \n",
      "4      8100000006     FD13        198.00     7.615385            0.0   \n",
      "5      8100000006    FDL13         71.28     1.738537            0.0   \n",
      "6      8100000006    FDT13         87.12     6.222857            0.0   \n",
      "7      8100000006     VI13        324.72     4.995692            0.0   \n",
      "8      8100000009     DL13        340.56     2.580000            0.0   \n",
      "9      8100000009     ED13       2803.68    19.470000            0.0   \n",
      "\n",
      "   max_liters  std_liters  num_weeks  nonzero_weeks_count  zero_ratio  \\\n",
      "0       15.84    3.892408         26                    4    0.846154   \n",
      "1       39.60    4.494875        142                    7    0.943662   \n",
      "2       15.84    5.397047         30                   11    0.633333   \n",
      "3       79.20   22.139896         46                   33    0.282609   \n",
      "4       23.76   10.137790         26                   10    0.615385   \n",
      "5       15.84    3.761873         41                    8    0.804878   \n",
      "6       31.68   10.840933         14                    4    0.714286   \n",
      "7       39.60   13.101649         65                    9    0.861538   \n",
      "8       31.68    7.611471        132                   15    0.886364   \n",
      "9      102.96   27.449671        144                   56    0.611111   \n",
      "\n",
      "   mean_nonzero_liters  median_nonzero_liters  std_nonzero_liters  cv_squared  \\\n",
      "0             9.900000                   7.92            3.960000    0.160000   \n",
      "1            18.102857                  15.84            9.928245    0.300781   \n",
      "2            10.080000                   7.92            3.699427    0.134694   \n",
      "3            43.200000                  47.52           12.058657    0.077917   \n",
      "4            19.800000                  19.80            4.174207    0.044444   \n",
      "5             8.910000                   7.92            2.800143    0.098765   \n",
      "6            21.780000                  19.80            7.582823    0.121212   \n",
      "7            36.080000                  39.60           10.560000    0.085663   \n",
      "8            22.704000                  23.76            7.250565    0.101986   \n",
      "9            50.065714                  47.52           19.985304    0.159346   \n",
      "\n",
      "         adi  promo_lift  \n",
      "0   8.333333   30.666667  \n",
      "1  23.000000   61.250000  \n",
      "2   2.900000    6.666667  \n",
      "3   1.406250    1.631016  \n",
      "4   2.777778    1.000000  \n",
      "5   5.714286    1.000000  \n",
      "6   4.333333    1.000000  \n",
      "7   8.000000   48.333333  \n",
      "8   9.357143   15.120000  \n",
      "9   2.600000    5.913262  \n",
      "\n",
      "--- Processed Table Schema ---\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "num_weeks: int64\n",
      "nonzero_weeks_count: int64\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting feature engineering clustering...\")\n",
    "print(GOLD_VENTAS_WEEKLY_PATH)\n",
    "processed_table = feature_engineering_clustering(GOLD_VENTAS_WEEKLY_PATH)\n",
    "\n",
    "if processed_table:\n",
    "\n",
    "    print(\"\\n--- Processed Table Sample (First 10 rows) ---\")\n",
    "    # Convert to Pandas just for easy printing of the head\n",
    "    print(processed_table.slice(0, 10).to_pandas())\n",
    "    print(\"\\n--- Processed Table Schema ---\")\n",
    "    print(processed_table.schema)\n",
    "else:\n",
    "    print(\"\\nFeature engineering failed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8a64040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7e3c5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_silhouette_score(features_table):\n",
    "\n",
    "    print(\"Esquema de la tabla cargada:\")\n",
    "    print(features_table.schema)\n",
    "\n",
    "# --- 2. Preparar Datos para Scikit-learn ---\n",
    "\n",
    "    # Nombres de las columnas de features (deben coincidir con las del archivo Parquet)\n",
    "    # Asume que las primeras columnas son 'establecimiento', 'material' y el resto son features\n",
    "    all_columns = features_table.schema.names\n",
    "    identifier_columns = ['establecimiento', 'material'] # Ajusta si tienes otros IDs\n",
    "    feature_columns = [col for col in all_columns if col not in identifier_columns]\n",
    "\n",
    "    print(f\"\\nColumnas identificadoras: {identifier_columns}\")\n",
    "    print(f\"Columnas de features para clustering: {feature_columns}\")\n",
    "\n",
    "    # Extraer solo las columnas de features a un array NumPy\n",
    "    try:\n",
    "        feature_arrays = [features_table.column(col_name).to_numpy(zero_copy_only=False)\n",
    "                        for col_name in feature_columns]\n",
    "        X_features = np.stack(feature_arrays, axis=1)\n",
    "        print(f\"Datos de features extraídos en array NumPy con forma: {X_features.shape}\")\n",
    "\n",
    "        # Verificar si hay NaNs o Infs residuales (importante para K-Means)\n",
    "        if np.isnan(X_features).any() or np.isinf(X_features).any():\n",
    "            print(\"\\n¡Advertencia! Se encontraron valores NaN o Inf en las features.\"\n",
    "                \" K-Means fallará. Revisa el paso de limpieza/relleno de nulos.\")\n",
    "            # Opcional: intentar rellenar aquí, aunque es mejor hacerlo antes de guardar\n",
    "            # X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0) # Ejemplo simple\n",
    "            # print(\"Se intentó rellenar NaNs/Infs con 0.\")\n",
    "        else:\n",
    "            print(\"No se encontraron NaNs/Infs en las features.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error al extraer features a NumPy: {e}\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    # --- 3. Determinar el Número Óptimo de Clusters (k) ---\n",
    "\n",
    "    # Rango de k a probar\n",
    "    k_range = range(2, 11) # Probar de 2 a 10 clusters (ajusta según necesidad)\n",
    "    inertia_values = []\n",
    "    silhouette_scores = []\n",
    "\n",
    "    print(\"\\nCalculando métricas para determinar k óptimo...\")\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)\n",
    "        kmeans.fit(X_features)\n",
    "        inertia_values.append(kmeans.inertia_) # Inertia: Suma de distancias al cuadrado al centroide (WCSS)\n",
    "        # Silhouette score necesita al menos 2 clusters y puede ser lento en datos grandes\n",
    "        if k >= 2:\n",
    "            score = silhouette_score(X_features, kmeans.labels_)\n",
    "            silhouette_scores.append(score)\n",
    "            print(f\"  k={k}, Inertia={kmeans.inertia_:.2f}, Silhouette Score={score:.4f}\")\n",
    "        else:\n",
    "            silhouette_scores.append(np.nan) # No aplicable para k=1\n",
    "            print(f\"  k={k}, Inertia={kmeans.inertia_:.2f}\")\n",
    "\n",
    "\n",
    "    # --- Visualización para encontrar k ---\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Gráfico del Codo (Elbow Method)\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(k_range, inertia_values, marker='o')\n",
    "    plt.title('Método del Codo (Elbow Method)')\n",
    "    plt.xlabel('Número de Clusters (k)')\n",
    "    plt.ylabel('Inertia (WCSS)')\n",
    "    plt.xticks(list(k_range))\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Gráfico del Coeficiente de Silueta\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Asegurarse de que k_range y silhouette_scores tengan la misma longitud si k_range empieza en 2\n",
    "    valid_k_range_for_silhouette = [k for k in k_range if k >= 2]\n",
    "    plt.plot(valid_k_range_for_silhouette, silhouette_scores, marker='o')\n",
    "    plt.title('Coeficiente de Silueta')\n",
    "    plt.xlabel('Número de Clusters (k)')\n",
    "    plt.ylabel('Silhouette Score Promedio')\n",
    "    plt.xticks(valid_k_range_for_silhouette)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n--- Interpretación para elegir k ---\")\n",
    "    print(\" - Método del Codo: Busca el 'codo' en la gráfica de Inertia, el punto donde la disminución se vuelve menos pronunciada.\")\n",
    "    print(\" - Coeficiente de Silueta: Busca el valor de 'k' que maximiza el Silhouette Score (más cercano a 1 es mejor).\")\n",
    "    print(\"   Valores cercanos a 0 indican clusters solapados. Valores negativos indican que las muestras pueden estar en el cluster incorrecto.\")\n",
    "    print(\"Elige un valor de 'k' basado en estas gráficas y/o conocimiento del negocio.\")\n",
    "\n",
    "\n",
    "\n",
    "def train_clustering_model(features_table, k, transpose_view=False):\n",
    "    all_columns = features_table.schema.names\n",
    "    identifier_columns = ['establecimiento', 'material'] # Ajusta si tienes otros IDs\n",
    "    feature_columns = [col for col in all_columns if col not in identifier_columns]\n",
    "\n",
    "    feature_arrays = [features_table.column(col_name).to_numpy(zero_copy_only=False)\n",
    "                      for col_name in feature_columns]\n",
    "    X_features = np.stack(feature_arrays, axis=1)\n",
    "    print(f\"Datos de features extraídos en array NumPy con forma: {X_features.shape}\")\n",
    "\n",
    "    # Verificar si hay NaNs o Infs residuales (importante para K-Means)\n",
    "    if np.isnan(X_features).any() or np.isinf(X_features).any():\n",
    "        print(\"\\n¡Advertencia! Se encontraron valores NaN o Inf en las features.\"\n",
    "              \" K-Means fallará. Revisa el paso de limpieza/relleno de nulos.\")\n",
    "        # Opcional: intentar rellenar aquí, aunque es mejor hacerlo antes de guardar\n",
    "        # X_features = np.nan_to_num(X_features, nan=0.0, posinf=0.0, neginf=0.0) # Ejemplo simple\n",
    "        # print(\"Se intentó rellenar NaNs/Infs con 0.\")\n",
    "    else:\n",
    "        print(\"No se encontraron NaNs/Infs en las features.\")\n",
    "\n",
    "\n",
    "\n",
    "    # !!! CAMBIA ESTE VALOR BASADO EN EL ANÁLISIS ANTERIOR !!!\n",
    "    print(f\"\\nEntrenando modelo K-Means final con k={k}...\")\n",
    "\n",
    "    kmeans_final = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)\n",
    "    kmeans_final.fit(X_features)\n",
    "\n",
    "    # Obtener las etiquetas de cluster para cada punto de datos (cada serie)\n",
    "    cluster_labels = kmeans_final.labels_\n",
    "\n",
    "    print(\"Entrenamiento completado.\")\n",
    "    print(f\"Número de puntos de datos por cluster: {np.bincount(cluster_labels)}\")\n",
    "\n",
    "    # --- 5. Añadir Etiquetas al Dataset Original y Guardar (Opcional) ---\n",
    "\n",
    "    # Crear una columna Arrow con las etiquetas\n",
    "    labels_column = pa.array(cluster_labels, type=pa.int32())\n",
    "\n",
    "    # Añadir la columna de etiquetas a la tabla original (la que tiene los identificadores)\n",
    "    # Nota: Esto asume que el orden de las filas en X_features coincide con features_table\n",
    "    try:\n",
    "        results_table = features_table.add_column(features_table.num_columns,\n",
    "                                            pa.field('cluster_label', pa.int32()),\n",
    "                                            labels_column)\n",
    "\n",
    "        print(\"\\n--- Tabla con Etiquetas de Cluster Añadidas (Primeras filas) ---\")\n",
    "        print(results_table.slice(0, 10))\n",
    "\n",
    "        # Opcional: Guardar la tabla con etiquetas en un nuevo archivo Parquet\n",
    "        # results_output_file = 'clustered_features.parquet'\n",
    "        # pq.write_table(results_table, results_output_file)\n",
    "        # print(f\"\\nTabla con etiquetas guardada en: {results_output_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al añadir/guardar etiquetas: {e}\")\n",
    "        # Si falla la adición directa, crear un mapeo y unirlo puede ser alternativa\n",
    "        # Crear DataFrame de IDs y labels\n",
    "        id_df = features_table.select(identifier_columns).to_pandas()\n",
    "        id_df['cluster_label'] = cluster_labels\n",
    "        print(\"\\nMapeo de Identificadores a Cluster Labels (DataFrame Pandas):\")\n",
    "        print(id_df.head())\n",
    "        # Aquí podrías unir este id_df con la tabla original si es necesario\n",
    "\n",
    "\n",
    "    # --- 6. Analizar Centroides (Opcional pero útil) ---\n",
    "    # Los centroides están en el espacio escalado. Para interpretarlos,\n",
    "    # necesitarías el objeto 'scaler' del paso anterior para aplicar 'inverse_transform'.\n",
    "    # Si no guardaste el scaler, solo puedes analizar los centroides escalados.\n",
    "    print(\"\\n--- Centroides de los Clusters (en espacio escalado) ---\")\n",
    "    centroids_scaled = kmeans_final.cluster_centers_\n",
    "    for i, centroid in enumerate(centroids_scaled):\n",
    "        print(f\"\\nCluster {i}:\")\n",
    "        for feature_name, value in zip(feature_columns, centroid):\n",
    "            print(f\"  {feature_name}: {value:.3f}\")\n",
    "    import pandas as pd\n",
    "    # print as a table with feature_names as row names and values as column values\n",
    "    print(pd.DataFrame(centroids_scaled))\n",
    "\n",
    "    # Si tuvieras el scaler:\n",
    "    try:\n",
    "        from joblib import load\n",
    "        import os\n",
    "        scaler = load(os.path.join(config.DATA_DIR, \"scaler\", \"features_scaler.joblib\")) # Asume que guardaste el scaler\n",
    "        centroids_original_scale = scaler.inverse_transform(centroids_scaled)\n",
    "        print(\"\\n--- Centroides de los Clusters (en escala original) ---\")\n",
    "        print(pd.DataFrame(centroids_original_scale))\n",
    "\n",
    "        # Si se solicita la vista transpuesta\n",
    "        if transpose_view:\n",
    "            print(\"\\n--- Generando tabla transpuesta de centroides ---\")\n",
    "            # Crear DataFrame transpuesto con nombres descriptivos de las features\n",
    "            df_centroids = pd.DataFrame(\n",
    "                centroids_original_scale.T,  # Transponemos la matriz\n",
    "                index=feature_columns,  # Nombres de las features como índice (filas)\n",
    "                columns=[f\"Cluster {i}\" for i in range(k)]  # Nombres de clusters como columnas\n",
    "            )\n",
    "            \n",
    "            # Nombres más descriptivos para las features\n",
    "            feature_descriptions = {\n",
    "                'total_liters': 'Volumen Total (litros)',\n",
    "                'mean_liters': 'Volumen Medio (litros/semana)',\n",
    "                'median_liters': 'Volumen Mediana (litros/semana)',\n",
    "                'max_liters': 'Volumen Máximo (litros/semana)',\n",
    "                'std_liters': 'Desviación Estándar Volumen',\n",
    "                'nonzero_weeks_count': 'Semanas con Venta',\n",
    "                'zero_ratio': 'Ratio Semanas sin Venta',\n",
    "                'mean_nonzero_liters': 'Volumen Medio en Semanas con Venta',\n",
    "                'median_nonzero_liters': 'Mediana Volumen en Semanas con Venta',\n",
    "                'std_nonzero_liters': 'Desv. Est. en Semanas con Venta',\n",
    "                'cv_squared': 'Coeficiente Variación al Cuadrado',\n",
    "                'adi': 'Intervalo Medio entre Demandas (ADI)',\n",
    "                'promo_lift': 'Lift por Promoción'\n",
    "            }\n",
    "            \n",
    "            # Reemplazar índices con descripciones más legibles\n",
    "            df_centroids.index = [feature_descriptions.get(feat, feat) for feat in feature_columns]\n",
    "            \n",
    "            # Mostrar tabla transpuesta\n",
    "            print(\"\\n--- Tabla Transpuesta de Centroides (Escala Original) ---\")\n",
    "            pd.set_option('display.max_rows', None)  # Mostrar todas las filas\n",
    "            pd.set_option('display.width', 120)  # Ancho suficiente para ver bien\n",
    "            pd.set_option('display.precision', 2)  # Reducir decimales para legibilidad\n",
    "            print(df_centroids)\n",
    "            \n",
    "            # Guardar como CSV para fácil acceso\n",
    "            csv_path = os.path.join(config.DATA_DIR, \"centroids_transposed.csv\")\n",
    "            df_centroids.to_csv(csv_path)\n",
    "            print(f\"\\nTabla guardada en: {csv_path}\")\n",
    "            \n",
    "            # Crear una visualización más atractiva con matplotlib\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            \n",
    "            # Crear tabla en matplotlib\n",
    "            the_table = plt.table(\n",
    "                cellText=df_centroids.round(2).values,\n",
    "                rowLabels=df_centroids.index,\n",
    "                colLabels=df_centroids.columns,\n",
    "                loc='center',\n",
    "                cellLoc='center'\n",
    "            )\n",
    "            \n",
    "            # Ajustar tamaño y estilo\n",
    "            the_table.auto_set_font_size(False)\n",
    "            the_table.set_fontsize(9)\n",
    "            the_table.scale(1.2, 1.5)\n",
    "            \n",
    "            # Eliminar ejes\n",
    "            plt.axis('off')\n",
    "            plt.title('Centroides de Clusters (Valores Transpuestos)', fontsize=16)\n",
    "            \n",
    "            # Guardar imagen\n",
    "            plt_path = os.path.join(config.DATA_DIR, \"centroids_transposed.png\")\n",
    "            plt.savefig(plt_path, bbox_inches='tight', dpi=150)\n",
    "            print(f\"Visualización guardada en: {plt_path}\")\n",
    "            plt.close()\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nNo se encontró 'scaler.joblib'. No se pueden mostrar centroides en escala original.\")\n",
    "\n",
    "\n",
    "def transpose_cluster_centroids():\n",
    "    \"\"\"\n",
    "    Genera una tabla transpuesta de los centroides de los clusters.\n",
    "    Las filas serán los atributos/features y las columnas serán los clusters.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from joblib import load\n",
    "    import os\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.font_manager import FontProperties\n",
    "    \n",
    "    print(\"Generando tabla transpuesta de centroides...\")\n",
    "    \n",
    "    # Cargar datos\n",
    "    features_table = pq.read_table(config.GOLD_FEATURES_FULL_PATH)\n",
    "    \n",
    "    # Extraer nombres de features\n",
    "    all_columns = features_table.schema.names\n",
    "    identifier_columns = ['establecimiento', 'material']\n",
    "    feature_columns = [col for col in all_columns if col not in identifier_columns]\n",
    "    \n",
    "    # Extraer datos para clustering\n",
    "    feature_arrays = [features_table.column(col_name).to_numpy(zero_copy_only=False)\n",
    "                      for col_name in feature_columns]\n",
    "    X_features = np.stack(feature_arrays, axis=1)\n",
    "    \n",
    "    # Entrenar modelo KMeans con k=6 (o el número que se determinó como óptimo)\n",
    "    k = 6\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)\n",
    "    kmeans.fit(X_features)\n",
    "    centroids_scaled = kmeans.cluster_centers_\n",
    "    \n",
    "    # Cargar el scaler para convertir a escala original\n",
    "    try:\n",
    "        scaler = load(os.path.join(config.DATA_DIR, \"scaler\", \"features_scaler.joblib\"))\n",
    "        centroids_original = scaler.inverse_transform(centroids_scaled)\n",
    "        \n",
    "        # Crear DataFrame transpuesto con nombres descriptivos de las features\n",
    "        # Los nombres de las columnas serán \"Cluster 0\", \"Cluster 1\", etc.\n",
    "        df_centroids = pd.DataFrame(\n",
    "            centroids_original.T,  # Transponemos la matriz\n",
    "            index=feature_columns,  # Nombres de las features como índice (filas)\n",
    "            columns=[f\"Cluster {i}\" for i in range(k)]  # Nombres de clusters como columnas\n",
    "        )\n",
    "        \n",
    "        # Nombres más descriptivos para las features\n",
    "        feature_descriptions = {\n",
    "            'total_liters': 'Volumen Total (litros)',\n",
    "            'mean_liters': 'Volumen Medio (litros/semana)',\n",
    "            'median_liters': 'Volumen Mediana (litros/semana)',\n",
    "            'max_liters': 'Volumen Máximo (litros/semana)',\n",
    "            'std_liters': 'Desviación Estándar Volumen',\n",
    "            'nonzero_weeks_count': 'Semanas con Venta',\n",
    "            'zero_ratio': 'Ratio Semanas sin Venta',\n",
    "            'mean_nonzero_liters': 'Volumen Medio en Semanas con Venta',\n",
    "            'median_nonzero_liters': 'Mediana Volumen en Semanas con Venta',\n",
    "            'std_nonzero_liters': 'Desv. Est. en Semanas con Venta',\n",
    "            'cv_squared': 'Coeficiente Variación al Cuadrado',\n",
    "            'adi': 'Intervalo Medio entre Demandas (ADI)',\n",
    "            'promo_lift': 'Lift por Promoción'\n",
    "        }\n",
    "        \n",
    "        # Reemplazar índices con descripciones más legibles\n",
    "        df_centroids.index = [feature_descriptions.get(feat, feat) for feat in feature_columns]\n",
    "        \n",
    "        # Mostrar tabla transpuesta\n",
    "        print(\"\\n--- Tabla Transpuesta de Centroides (Escala Original) ---\")\n",
    "        pd.set_option('display.max_rows', None)  # Mostrar todas las filas\n",
    "        pd.set_option('display.width', 120)  # Ancho suficiente para ver bien\n",
    "        pd.set_option('display.precision', 2)  # Reducir decimales para legibilidad\n",
    "        print(df_centroids)\n",
    "        \n",
    "        # Guardar como CSV para fácil acceso\n",
    "        csv_path = os.path.join(config.DATA_DIR, \"centroids_transposed.csv\")\n",
    "        df_centroids.to_csv(csv_path)\n",
    "        print(f\"\\nTabla guardada en: {csv_path}\")\n",
    "        \n",
    "        # Crear una visualización más atractiva con matplotlib\n",
    "        plt.figure(figsize=(14, 8))\n",
    "        \n",
    "        # Crear tabla en matplotlib\n",
    "        the_table = plt.table(\n",
    "            cellText=df_centroids.round(2).values,\n",
    "            rowLabels=df_centroids.index,\n",
    "            colLabels=df_centroids.columns,\n",
    "            loc='center',\n",
    "            cellLoc='center'\n",
    "        )\n",
    "        \n",
    "        # Ajustar tamaño y estilo\n",
    "        the_table.auto_set_font_size(False)\n",
    "        the_table.set_fontsize(9)\n",
    "        the_table.scale(1.2, 1.5)\n",
    "        \n",
    "        # Eliminar ejes\n",
    "        plt.axis('off')\n",
    "        plt.title('Centroides de Clusters (Valores Transpuestos)', fontsize=16)\n",
    "        \n",
    "        # Guardar imagen\n",
    "        plt_path = os.path.join(config.DATA_DIR, \"centroids_transposed.png\")\n",
    "        plt.savefig(plt_path, bbox_inches='tight', dpi=150)\n",
    "        print(f\"Visualización guardada en: {plt_path}\")\n",
    "        plt.close()\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"\\nNo se encontró el archivo scaler. No se puede generar la tabla transpuesta.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError al generar tabla transpuesta: {e}\")\n",
    "\n",
    "# Función para asignar clusters a los datos de entrenamiento\n",
    "def assign_clusters_to_training_data(k: int):\n",
    "    \"\"\"\n",
    "    Función principal que asigna los clusters a los datos de entrenamiento.\n",
    "    \"\"\"\n",
    "    print(\"Asignando clusters a los datos de entrenamiento...\")\n",
    "    \n",
    "    # 1. Cargar datos de entrenamiento\n",
    "    training_data = pq.read_table(config.GOLD_WEEKLY_FULL_PATH)\n",
    "    print(f\"Datos de entrenamiento cargados desde {config.GOLD_WEEKLY_FULL_PATH}\")\n",
    "    print(f\"Número de registros: {training_data.num_rows}\")\n",
    "    \n",
    "    # 2. Cargar datos de features (que contienen los identificadores y las features para clustering)\n",
    "    features_table = pq.read_table(config.GOLD_FEATURES_FULL_PATH)\n",
    "    print(f\"Datos de features cargados desde {config.GOLD_FEATURES_FULL_PATH}\")\n",
    "    print(f\"Número de registros: {features_table.num_rows}\")\n",
    "    \n",
    "    # 3. Extraer identificadores y features para clustering\n",
    "    all_columns = features_table.schema.names\n",
    "    identifier_columns = ['establecimiento', 'material']\n",
    "    feature_columns = [col for col in all_columns if col not in identifier_columns]\n",
    "    \n",
    "    # 4. Extraer datos para clustering\n",
    "    feature_arrays = [features_table.column(col_name).to_numpy(zero_copy_only=False)\n",
    "                    for col_name in feature_columns]\n",
    "    X_features = np.stack(feature_arrays, axis=1)\n",
    "    \n",
    "    # 5. Ejecutar clustering (mismo modelo, mismos parámetros que en 04_clustering_model.py)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', n_init='auto', random_state=42)\n",
    "    kmeans.fit(X_features)\n",
    "    \n",
    "    # 6. Obtener etiquetas\n",
    "    cluster_labels = kmeans.labels_\n",
    "    \n",
    "    # 7. Crear un DataFrame con identificadores y etiquetas de cluster\n",
    "    id_df = features_table.select(identifier_columns).to_pandas()\n",
    "    id_df['cluster_label'] = cluster_labels\n",
    "    \n",
    "    # 8. Convertir los datos de entrenamiento a DataFrame para facilitar el merge\n",
    "    training_df = training_data.to_pandas()\n",
    "    \n",
    "    # 9. Combinar los datos de entrenamiento con las etiquetas de cluster\n",
    "    # Usamos merge en lugar de join para mantener todos los registros\n",
    "    merged_df = pd.merge(\n",
    "        training_df, \n",
    "        id_df, \n",
    "        on=['establecimiento', 'material'],\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # 10. Verificar que todos los registros han sido asignados a un cluster\n",
    "    null_clusters = merged_df['cluster_label'].isna().sum()\n",
    "    if null_clusters > 0:\n",
    "        print(f\"ADVERTENCIA: {null_clusters} registros no tienen asignado un cluster.\")\n",
    "        # Rellenar valores nulos con un valor que indique \"no clasificado\" (-1)\n",
    "        merged_df['cluster_label'] = merged_df['cluster_label'].fillna(-1).astype(int)\n",
    "    \n",
    "    # 11. Convertir de nuevo a tabla Arrow\n",
    "    result_table = pa.Table.from_pandas(merged_df)\n",
    "    \n",
    "    # 12. Guardar el resultado\n",
    "    output_path = os.path.join(config.DATA_DIR, \"gold_ventas_semanales_training_clustered.parquet\")\n",
    "    pq.write_table(result_table, output_path)\n",
    "    print(f\"Datos con clusters guardados en: {output_path}\")\n",
    "    \n",
    "    # 13. Mostrar distribución de clusters\n",
    "    cluster_counts = merged_df['cluster_label'].value_counts().sort_index()\n",
    "    print(\"\\nDistribución de registros por cluster:\")\n",
    "    for cluster, count in cluster_counts.items():\n",
    "        print(f\"Cluster {cluster}: {count} registros ({count/len(merged_df)*100:.2f}%)\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e7a05bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cargando datos de features para análisis de k...\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "nonzero_weeks_count: double\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "----\n",
      "establecimiento: [[\"8100000005\",\"8100000005\",\"8100000005\",\"8100000006\",\"8100000006\"]]\n",
      "material: [[\"DL13\",\"ED13\",\"FDT13\",\"ED13\",\"FD13\"]]\n",
      "total_liters: [[-0.2715087116995092,-0.2603856571892176,-0.2614979626402467,-0.07685525776940644,-0.249262602678926]]\n",
      "mean_liters: [[-0.25617493255741863,-0.26653647342670717,-0.22337633839742496,0.18862518125395455,-0.16421625734248307]]\n",
      "median_liters: [[-0.27636592137944144,-0.27636592137944144,-0.27636592137944144,0.5712362332950929,-0.27636592137944144]]\n",
      "max_liters: [[-0.18195482062772375,-0.13297433305246037,-0.18195482062772375,-0.05134018709368802,-0.16562799143596932]]\n",
      "std_liters: [[-0.18059696495042,-0.1745128199407905,-0.16540204319589208,0.0036791479456249627,-0.11752665008848688]]\n",
      "nonzero_weeks_count: [[-0.8439649586683631,-0.7603584214800341,-0.6488830385622621,-0.03576843251451631,-0.6767518842917051]]\n",
      "zero_ratio: [[0.7234581318408011,1.0973150855853746,-0.09251928609133686,-1.4372363928564,-0.16133665868802294]]\n",
      "mean_nonzero_liters: [[-0.16368689887416402,-0.12353292968862031,-0.16280577729329002,-0.0006794064124741529,-0.11522521192609404]]\n",
      "...\n",
      "Datos cargados desde ../models/clustering/gold_ventas_semanales_clustered.parquet\n",
      "Esquema de la tabla cargada:\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "nonzero_weeks_count: double\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "\n",
      "Columnas identificadoras: ['establecimiento', 'material']\n",
      "Columnas de features para clustering: ['total_liters', 'mean_liters', 'median_liters', 'max_liters', 'std_liters', 'nonzero_weeks_count', 'zero_ratio', 'mean_nonzero_liters', 'median_nonzero_liters', 'std_nonzero_liters', 'cv_squared', 'adi', 'promo_lift']\n",
      "Datos de features extraídos en array NumPy con forma: (306404, 13)\n",
      "No se encontraron NaNs/Infs en las features.\n",
      "\n",
      "Calculando métricas para determinar k óptimo...\n",
      "  k=2, Inertia=3379890.89, Silhouette Score=0.5003\n",
      "  k=3, Inertia=2599238.44, Silhouette Score=0.4957\n",
      "  k=4, Inertia=2198201.14, Silhouette Score=0.4873\n",
      "  k=5, Inertia=1944901.01, Silhouette Score=0.4395\n",
      "  k=6, Inertia=1737370.42, Silhouette Score=0.4745\n",
      "  k=7, Inertia=1605203.91, Silhouette Score=0.4740\n",
      "  k=8, Inertia=1473884.26, Silhouette Score=0.4696\n",
      "  k=9, Inertia=1409973.83, Silhouette Score=0.4722\n",
      "  k=10, Inertia=1318262.41, Silhouette Score=0.4722\n",
      "Error durante el análisis de k: name 'plt' is not defined\n",
      "\n",
      "--- SCRIPT FINALIZADO ---\n",
      "Revisa las gráficas mostradas (Codo y Silueta) para elegir el valor óptimo de 'k'.\n",
      "Luego, modifica el script o ejecuta las funciones correspondientes\n",
      "con el 'k' elegido para entrenar el modelo final y/o asignar clusters.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nCargando datos de features para análisis de k...\")\n",
    "try:\n",
    "    features_path = GOLD_FEATURES_PATH\n",
    "    features_table_for_k = pq.read_table(GOLD_FEATURES_PATH)\n",
    "    print(features_table_for_k.slice(0,5))\n",
    "    print(f\"Datos cargados desde {features_path}\")\n",
    "    # imprimir materiales unicos\n",
    "\n",
    "    # Llamar a la función que calcula y muestra las gráficas\n",
    "    calculate_silhouette_score(features_table_for_k)\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontró el archivo de features en {features_path}\")\n",
    "    print(\"No se puede proceder con el análisis de k.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el análisis de k: {e}\")\n",
    "\n",
    "# --- FIN DEL SCRIPT (en esta modificación) ---\n",
    "# El código se detendrá aquí después de mostrar las gráficas.\n",
    "# Para ejecutar los siguientes pasos (entrenamiento final, asignación),\n",
    "# necesitarías:\n",
    "# 1. Analizar las gráficas mostradas y ELEGIR un valor para 'k'.\n",
    "# 2. Modificar este bloque __main__ o crear otro script para llamar a:\n",
    "#    - train_clustering_model(features_table, k_elegido, transpose_view=True)\n",
    "#    - assign_clusters_to_training_data(k_elegido)\n",
    "#    (Asegúrate de cargar 'features_table' de nuevo si es necesario para esas funciones)\n",
    "\n",
    "print(\"\\n--- SCRIPT FINALIZADO ---\")\n",
    "print(\"Revisa las gráficas mostradas (Codo y Silueta) para elegir el valor óptimo de 'k'.\")\n",
    "print(\"Luego, modifica el script o ejecuta las funciones correspondientes\")\n",
    "print(\"con el 'k' elegido para entrenar el modelo final y/o asignar clusters.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b15330d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos de features extraídos en array NumPy con forma: (306404, 13)\n",
      "No se encontraron NaNs/Infs en las features.\n",
      "\n",
      "Entrenando modelo K-Means final con k=6...\n",
      "Entrenamiento completado.\n",
      "Número de puntos de datos por cluster: [217053  13918   4520      4     47  70862]\n",
      "\n",
      "--- Tabla con Etiquetas de Cluster Añadidas (Primeras filas) ---\n",
      "pyarrow.Table\n",
      "establecimiento: string\n",
      "material: string\n",
      "total_liters: double\n",
      "mean_liters: double\n",
      "median_liters: double\n",
      "max_liters: double\n",
      "std_liters: double\n",
      "nonzero_weeks_count: double\n",
      "zero_ratio: double\n",
      "mean_nonzero_liters: double\n",
      "median_nonzero_liters: double\n",
      "std_nonzero_liters: double\n",
      "cv_squared: double\n",
      "adi: double\n",
      "promo_lift: double\n",
      "cluster_label: int32\n",
      "----\n",
      "establecimiento: [[\"8100000005\",\"8100000005\",\"8100000005\",\"8100000006\",\"8100000006\",\"8100000006\",\"8100000006\",\"8100000006\",\"8100000009\",\"8100000009\"]]\n",
      "material: [[\"DL13\",\"ED13\",\"FDT13\",\"ED13\",\"FD13\",\"FDL13\",\"FDT13\",\"VI13\",\"DL13\",\"ED13\"]]\n",
      "total_liters: [[-0.2715087116995092,-0.2603856571892176,-0.2614979626402467,-0.07685525776940644,-0.249262602678926,-0.2670594898953926,-0.26483487899333424,-0.23146571546245945,-0.22924110456040114,0.11668589070966734]]\n",
      "mean_liters: [[-0.25617493255741863,-0.26653647342670717,-0.22337633839742496,0.18862518125395455,-0.16421625734248307,-0.25292273550713434,-0.18523538310589688,-0.20375848768490534,-0.24022149572088816,0.014719998179912559]]\n",
      "median_liters: [[-0.27636592137944144,-0.27636592137944144,-0.27636592137944144,0.5712362332950929,-0.27636592137944144,-0.27636592137944144,-0.27636592137944144,-0.27636592137944144,-0.27636592137944144,-0.27636592137944144]]\n",
      "max_liters: [[-0.18195482062772375,-0.13297433305246037,-0.18195482062772375,-0.05134018709368802,-0.16562799143596932,-0.18195482062772375,-0.14930116224421483,-0.13297433305246037,-0.14930116224421483,-0.002359699518424605]]\n",
      "std_liters: [[-0.18059696495042,-0.1745128199407905,-0.16540204319589208,0.0036791479456249627,-0.11752665008848688,-0.18191519864981787,-0.1104258117166731,-0.08759549830186428,-0.14303921509700088,0.057301029596773974]]\n",
      "nonzero_weeks_count: [[-0.8439649586683631,-0.7603584214800341,-0.6488830385622621,-0.03576843251451631,-0.6767518842917051,-0.7324895757505911,-0.8439649586683631,-0.7046207300211481,-0.5374076556444901,0.6052150192626725]]\n",
      "zero_ratio: [[0.7234581318408011,1.0973150855853746,-0.09251928609133686,-1.4372363928564,-0.16133665868802294,0.5652021530470278,0.21786110868147307,0.7824444512093895,0.8776269210996114,-0.17772174740151964]]\n",
      "mean_nonzero_liters: [[-0.16368689887416402,-0.12353292968862031,-0.16280577729329002,-0.0006794064124741529,-0.11522521192609404,-0.16853306756897102,-0.10553287453648003,-0.03553266005593447,-0.10100978375466016,0.03292908817229139]]\n",
      "...\n",
      "\n",
      "--- Centroides de los Clusters (en espacio escalado) ---\n",
      "\n",
      "Cluster 0:\n",
      "  total_liters: -0.207\n",
      "  mean_liters: -0.186\n",
      "  median_liters: -0.260\n",
      "  max_liters: -0.099\n",
      "  std_liters: -0.093\n",
      "  nonzero_weeks_count: -0.426\n",
      "  zero_ratio: 0.418\n",
      "  mean_nonzero_liters: -0.078\n",
      "  median_nonzero_liters: -0.077\n",
      "  std_nonzero_liters: -0.066\n",
      "  cv_squared: -0.040\n",
      "  adi: -0.080\n",
      "  promo_lift: 0.005\n",
      "\n",
      "Cluster 1:\n",
      "  total_liters: -0.268\n",
      "  mean_liters: -0.267\n",
      "  median_liters: -0.276\n",
      "  max_liters: -0.128\n",
      "  std_liters: -0.157\n",
      "  nonzero_weeks_count: -0.910\n",
      "  zero_ratio: 1.221\n",
      "  mean_nonzero_liters: -0.048\n",
      "  median_nonzero_liters: -0.032\n",
      "  std_nonzero_liters: -0.083\n",
      "  cv_squared: -0.265\n",
      "  adi: 3.803\n",
      "  promo_lift: -0.082\n",
      "\n",
      "Cluster 2:\n",
      "  total_liters: 4.082\n",
      "  mean_liters: 3.713\n",
      "  median_liters: 4.244\n",
      "  max_liters: 2.562\n",
      "  std_liters: 2.418\n",
      "  nonzero_weeks_count: 1.497\n",
      "  zero_ratio: -1.477\n",
      "  mean_nonzero_liters: 2.163\n",
      "  median_nonzero_liters: 2.128\n",
      "  std_nonzero_liters: 1.946\n",
      "  cv_squared: 0.654\n",
      "  adi: -0.406\n",
      "  promo_lift: 0.002\n",
      "\n",
      "Cluster 3:\n",
      "  total_liters: 88.534\n",
      "  mean_liters: 129.060\n",
      "  median_liters: -0.276\n",
      "  max_liters: 190.421\n",
      "  std_liters: 188.908\n",
      "  nonzero_weeks_count: -0.489\n",
      "  zero_ratio: 0.424\n",
      "  mean_nonzero_liters: 192.135\n",
      "  median_nonzero_liters: 187.258\n",
      "  std_nonzero_liters: 202.110\n",
      "  cv_squared: 0.844\n",
      "  adi: 0.482\n",
      "  promo_lift: 0.002\n",
      "\n",
      "Cluster 4:\n",
      "  total_liters: 28.521\n",
      "  mean_liters: 28.286\n",
      "  median_liters: 20.754\n",
      "  max_liters: 32.007\n",
      "  std_liters: 33.723\n",
      "  nonzero_weeks_count: 0.240\n",
      "  zero_ratio: -0.230\n",
      "  mean_nonzero_liters: 35.057\n",
      "  median_nonzero_liters: 34.723\n",
      "  std_nonzero_liters: 28.780\n",
      "  cv_squared: 1.095\n",
      "  adi: -0.076\n",
      "  promo_lift: 0.002\n",
      "\n",
      "Cluster 5:\n",
      "  total_liters: 0.403\n",
      "  mean_liters: 0.360\n",
      "  median_liters: 0.567\n",
      "  max_liters: 0.134\n",
      "  std_liters: 0.128\n",
      "  nonzero_weeks_count: 1.390\n",
      "  zero_ratio: -1.427\n",
      "  mean_nonzero_liters: 0.075\n",
      "  median_nonzero_liters: 0.074\n",
      "  std_nonzero_liters: 0.063\n",
      "  cv_squared: 0.131\n",
      "  adi: -0.476\n",
      "  promo_lift: 0.002\n",
      "          0           1          2           3           4         5   \\\n",
      "0  -0.207063   -0.186110  -0.259993   -0.099353   -0.092622 -0.426437   \n",
      "1  -0.268306   -0.266562  -0.276366   -0.127615   -0.156680 -0.910439   \n",
      "2   4.081684    3.713389   4.244204    2.562061    2.417689  1.496548   \n",
      "3  88.534217  129.060436  -0.276366  190.420912  188.908036 -0.488637   \n",
      "4  28.520626   28.285814  20.754478   32.007345   33.723065  0.239955   \n",
      "5   0.403415    0.360183   0.567002    0.134413    0.127636  1.390103   \n",
      "\n",
      "         6           7           8           9         10        11        12  \n",
      "0  0.418065   -0.077564   -0.077111   -0.065521 -0.039553 -0.080064  0.004545  \n",
      "1  1.220688   -0.048135   -0.032414   -0.082770 -0.265186  3.802924 -0.081732  \n",
      "2 -1.477299    2.163050    2.128244    1.946024  0.654346 -0.406205  0.002001  \n",
      "3  0.423511  192.135285  187.257633  202.110404  0.843640  0.482235  0.002001  \n",
      "4 -0.229500   35.057295   34.722624   28.779973  1.095129 -0.076328  0.002001  \n",
      "5 -1.426629    0.075317    0.073552    0.062635  0.130860 -0.475894  0.002001  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m features_table = pq.read_table(GOLD_FEATURES_PATH)\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Puedes descomentar una de estas funciones según lo que quieras hacer\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Opción 1: Ejecutar clustering y generar vista transpuesta\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrain_clustering_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_view\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Opción 2: Asignar clusters a los datos de entrenamiento\u001b[39;00m\n\u001b[32m      8\u001b[39m assign_clusters_to_training_data(k)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 179\u001b[39m, in \u001b[36mtrain_clustering_model\u001b[39m\u001b[34m(features_table, k, transpose_view)\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m scaler = load(os.path.join(\u001b[43mconfig\u001b[49m.DATA_DIR, \u001b[33m\"\u001b[39m\u001b[33mscaler\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfeatures_scaler.joblib\u001b[39m\u001b[33m\"\u001b[39m)) \u001b[38;5;66;03m# Asume que guardaste el scaler\u001b[39;00m\n\u001b[32m    180\u001b[39m centroids_original_scale = scaler.inverse_transform(centroids_scaled)\n\u001b[32m    181\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Centroides de los Clusters (en escala original) ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "k = 6\n",
    "features_table = pq.read_table(GOLD_FEATURES_PATH)\n",
    "# Puedes descomentar una de estas funciones según lo que quieras hacer\n",
    "# Opción 1: Ejecutar clustering y generar vista transpuesta\n",
    "train_clustering_model(features_table, k, transpose_view=True)\n",
    "\n",
    "# Opción 2: Asignar clusters a los datos de entrenamiento\n",
    "assign_clusters_to_training_data(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7f4d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
