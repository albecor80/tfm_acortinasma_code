{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a9897e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor # MODIFIED: Import RandomForest\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55ed53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Leyendo columna de clusters 'cluster_label' del archivo: ../data/gold_ventas_semanales_clustered_lgbm.parquet\n",
      "Encontrados 7 clusters únicos: [np.int32(2), np.int32(0), np.int32(5), np.int32(6), np.int32(4), np.int32(3), np.int32(1)]...\n",
      "\n",
      "Iniciando procesamiento para 7 clusters...\n",
      "\n",
      "--- Procesando Cluster 1/7: ID = 2 ---\n",
      "Cargando datos para el cluster 2...\n",
      "Cluster 2: 16151645 filas cargadas.\n",
      "Cluster 2 is very large (16151645 rows). Subsampling complete series to be around 500000 rows.\n",
      "Original series count: 145068. Estimated series to sample: 4490.\n",
      "Cluster 2 now has 500255 rows from 4490 unique series after subsampling.\n",
      "Realizando split Train/Test para el cluster 2...\n",
      "Cluster 2: Train/Val=400204, Test=100051\n",
      "!!! DIAGNOSTIC: Starting manual pipeline fit test on a TINY sample...\n",
      "Sampled 1000 rows for diagnostic fit.\n",
      "!!! DIAGNOSTIC: Calling pipeline.fit() on sample...\n",
      "!!! DIAGNOSTIC: Manual pipeline fit on sample completed in 0.73 seconds.\n",
      "!!! DIAGNOSTIC: End of manual test. Continuing to RandomizedSearchCV (or not, if you exit)...\n",
      "Iniciando HPT con RandomForest para cluster 2...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Mejores parámetros encontrados para cluster 2 (RandomForest): {'regressor__n_estimators': 100, 'regressor__min_samples_split': 100, 'regressor__min_samples_leaf': 10, 'regressor__max_features': 0.7, 'regressor__max_depth': 20}\n",
      "Realizando predicciones en el conjunto de test del cluster 2...\n",
      "Calculando métricas agregadas para el cluster 2...\n",
      "Generando gráficos para las series del cluster 2...\n",
      "Límite de 100 gráficos alcanzado para el cluster 2.\n",
      "Modelo RandomForest del cluster 2 guardado en: ../models/random_forest/trained_cluster_models_rf/rf_model_cluster_2.joblib\n",
      "\n",
      "--- Procesando Cluster 2/7: ID = 0 ---\n",
      "Cargando datos para el cluster 0...\n",
      "Cluster 0: 7702523 filas cargadas.\n",
      "Cluster 0 is very large (7702523 rows). Subsampling complete series to be around 500000 rows.\n",
      "Original series count: 59128. Estimated series to sample: 3838.\n",
      "Cluster 0 now has 500364 rows from 3838 unique series after subsampling.\n",
      "Realizando split Train/Test para el cluster 0...\n",
      "Cluster 0: Train/Val=400291, Test=100073\n",
      "!!! DIAGNOSTIC: Starting manual pipeline fit test on a TINY sample...\n",
      "Sampled 1000 rows for diagnostic fit.\n",
      "!!! DIAGNOSTIC: Calling pipeline.fit() on sample...\n",
      "!!! DIAGNOSTIC: Manual pipeline fit on sample completed in 1.04 seconds.\n",
      "!!! DIAGNOSTIC: End of manual test. Continuing to RandomizedSearchCV (or not, if you exit)...\n",
      "Iniciando HPT con RandomForest para cluster 0...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 325\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;66;03m# You might want to add exit() here during testing to not proceed to the full RandomizedSearchCV\u001b[39;00m\n\u001b[32m    319\u001b[39m \u001b[38;5;66;03m# import sys\u001b[39;00m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# sys.exit(\"Exiting after diagnostic test.\")\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# --- End of Temporary Diagnostic Code ---\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIniciando HPT con RandomForest para cluster \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcluster_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[43msearch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_val\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    326\u001b[39m best_model_cluster = search.best_estimator_\n\u001b[32m    327\u001b[39m best_params_cluster = search.best_params_\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/model_selection/_search.py:1062\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m   1060\u001b[39m refit_start_time = time.time()\n\u001b[32m   1061\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1062\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbest_estimator_\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1063\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1064\u001b[39m     \u001b[38;5;28mself\u001b[39m.best_estimator_.fit(X, **routed_params.estimator.fit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/pipeline.py:662\u001b[39m, in \u001b[36mPipeline.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._final_estimator != \u001b[33m\"\u001b[39m\u001b[33mpassthrough\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    657\u001b[39m         last_step_params = \u001b[38;5;28mself\u001b[39m._get_metadata_for_step(\n\u001b[32m    658\u001b[39m             step_idx=\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) - \u001b[32m1\u001b[39m,\n\u001b[32m    659\u001b[39m             step_params=routed_params[\u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m0\u001b[39m]],\n\u001b[32m    660\u001b[39m             all_params=params,\n\u001b[32m    661\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_final_estimator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfit\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/utils/parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/joblib/parallel.py:1985\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1983\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1984\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1985\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1987\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/joblib/parallel.py:1913\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1911\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1913\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1914\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/utils/parallel.py:139\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    137\u001b[39m     config = {}\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config):\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:189\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    186\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    187\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    197\u001b[39m     tree._fit(\n\u001b[32m    198\u001b[39m         X,\n\u001b[32m    199\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    203\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/tfm/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# --- 1. Configuración General ---\n",
    "PARQUET_FILE = '../data/gold_ventas_semanales_clustered_lgbm.parquet' # Ensure this path is correct\n",
    "OUTPUT_DIR = '../models/random_forest'\n",
    "METRICS_FILE = os.path.join(OUTPUT_DIR, 'all_cluster_metrics_rf.csv')\n",
    "PREDICTIONS_FILE = os.path.join(OUTPUT_DIR, 'all_series_predictions_rf.csv')\n",
    "PLOTS_DIR = os.path.join(OUTPUT_DIR, 'prediction_plots_by_series_rf')\n",
    "MODELS_DIR = os.path.join(OUTPUT_DIR, 'trained_cluster_models_rf')\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "os.makedirs(MODELS_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_VARIABLE = 'weekly_volume'\n",
    "SERIES_ID_COLS = ['establecimiento', 'material']\n",
    "CLUSTER_COL = 'cluster_label'\n",
    "DATE_COL = 'week'\n",
    "\n",
    "# Leer metadata para obtener todas las columnas\n",
    "try:\n",
    "    parquet_meta = pq.read_metadata(PARQUET_FILE)\n",
    "    ALL_COLS = [col.name for col in parquet_meta.schema]\n",
    "except Exception as e:\n",
    "    print(f\"Error reading Parquet metadata from {PARQUET_FILE}: {e}. Please check the file path and integrity. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "FEATURES_COLS = [\n",
    "    col for col in ALL_COLS\n",
    "    if col not in [TARGET_VARIABLE, DATE_COL, CLUSTER_COL, 'last_sale_week']\n",
    "]\n",
    "\n",
    "CATEGORICAL_FEATURES_FOR_MODEL = [f for f in ['establecimiento', 'material'] if f in FEATURES_COLS]\n",
    "NUMERICAL_FEATURES = [col for col in FEATURES_COLS if col not in CATEGORICAL_FEATURES_FOR_MODEL]\n",
    "\n",
    "if not NUMERICAL_FEATURES and not CATEGORICAL_FEATURES_FOR_MODEL:\n",
    "    print(f\"Warning: No numerical or categorical features selected for the model based on FEATURES_COLS. FEATURES_COLS: {FEATURES_COLS}\")\n",
    "    print(f\"NUMERICAL_FEATURES: {NUMERICAL_FEATURES}, CATEGORICAL_FEATURES_FOR_MODEL: {CATEGORICAL_FEATURES_FOR_MODEL}\")\n",
    "    # Potentially exit or handle this scenario if it's unexpected\n",
    "    # exit()\n",
    "elif not FEATURES_COLS:\n",
    "    print(f\"Warning: FEATURES_COLS is empty. No features to train on. Exiting.\")\n",
    "    # exit()\n",
    "\n",
    "\n",
    "# --- 2. Funciones de Utilidad (Métricas y Gráficos) ---\n",
    "def calculate_mase(y_true_train, y_true_test, y_pred_test):\n",
    "    y_true_train = np.array(y_true_train).flatten()\n",
    "    y_true_test = np.array(y_true_test).flatten()\n",
    "    y_pred_test = np.array(y_pred_test).flatten()\n",
    "\n",
    "    if len(y_true_train) < 2:\n",
    "        return np.nan\n",
    "\n",
    "    naive_forecast_error_train = np.mean(np.abs(np.diff(y_true_train)))\n",
    "\n",
    "    if naive_forecast_error_train < 1e-9:\n",
    "         model_mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "         return np.inf if model_mae_test > 1e-9 else 0.0\n",
    "\n",
    "    model_mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "    return model_mae_test / naive_forecast_error_train\n",
    "\n",
    "\n",
    "def evaluate_model(y_true_train, y_true_test, y_pred_test, label=\"Cluster\"):\n",
    "    metrics = {\n",
    "        f'{label}_mae': mean_absolute_error(y_true_test, y_pred_test),\n",
    "        f'{label}_rmse': np.sqrt(mean_squared_error(y_true_test, y_pred_test)),\n",
    "        f'{label}_mape': mean_absolute_percentage_error(y_true_test, y_pred_test) if np.all(np.abs(y_true_test) > 1e-9) else np.nan,\n",
    "        f'{label}_r2': r2_score(y_true_test, y_pred_test),\n",
    "        f'{label}_mase': calculate_mase(y_true_train, y_true_test, y_pred_test)\n",
    "    }\n",
    "    mape_key = f'{label}_mape'\n",
    "    if mape_key in metrics and np.isinf(metrics[mape_key]):\n",
    "         metrics[mape_key] = np.nan\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_predictions(dates_test, y_true_test, y_pred_test, estab, material, cluster_id, filepath):\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(dates_test, y_true_test, label='Real', marker='.', linestyle='-')\n",
    "    plt.plot(dates_test, y_pred_test, label=f'Predicción RF (Cluster {cluster_id})', marker='x', linestyle='--')\n",
    "    plt.title(f'Predicción RF vs Real - {estab} / {material} (Cluster {cluster_id})')\n",
    "    plt.xlabel('Semana')\n",
    "    plt.ylabel('Volumen Semanal')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filepath)\n",
    "    plt.close()\n",
    "\n",
    "# --- 3. Carga Inicial de Clusters ---\n",
    "unique_clusters = [] # Initialize\n",
    "try:\n",
    "    print(f\"Leyendo columna de clusters '{CLUSTER_COL}' del archivo: {PARQUET_FILE}\")\n",
    "    pf = pq.ParquetFile(PARQUET_FILE)\n",
    "    cluster_labels_df = pf.read(columns=[CLUSTER_COL]).to_pandas()\n",
    "    unique_clusters = cluster_labels_df[CLUSTER_COL].unique()\n",
    "    unique_clusters = [c for c in unique_clusters if pd.notna(c) and c != ''] # Handle empty strings as well if they act like NaN\n",
    "    print(f\"Encontrados {len(unique_clusters)} clusters únicos: {unique_clusters[:10]}...\") # Print a sample\n",
    "    del cluster_labels_df\n",
    "    gc.collect()\n",
    "except Exception as e:\n",
    "    print(f\"Error fatal al leer la columna de clusters del Parquet: {e}. Verifique el archivo y la columna '{CLUSTER_COL}'. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "if not unique_clusters:\n",
    "    print(f\"No se encontraron clusters válidos en la columna '{CLUSTER_COL}' del archivo '{PARQUET_FILE}'. Verifique los datos. Exiting.\")\n",
    "    exit()\n",
    "\n",
    "# --- 4. Procesamiento por Cluster (Entrenamiento, CV, HPT, Evaluación) ---\n",
    "all_cluster_metrics = []\n",
    "header_preds = SERIES_ID_COLS + [DATE_COL, 'actual_volume', 'predicted_volume', CLUSTER_COL]\n",
    "pd.DataFrame(columns=header_preds).to_csv(PREDICTIONS_FILE, index=False)\n",
    "\n",
    "N_SPLITS_CV = 5\n",
    "N_ITER_HPT = 5 # Adjust for speed vs. thoroughness\n",
    "SCORING_METRIC_HPT = 'neg_mean_absolute_error'\n",
    "\n",
    "param_dist_rf = {\n",
    "    'regressor__n_estimators': [50, 100, 150], # Reduced for quicker example runs\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [10, 20, 100],\n",
    "    'regressor__min_samples_leaf': [5, 10, 50],\n",
    "    'regressor__max_features': ['sqrt', 'log2', 0.7]\n",
    "}\n",
    "\n",
    "header_metrics = [CLUSTER_COL] + [f'Cluster_{m}' for m in ['mae', 'rmse', 'mape', 'r2', 'mase']] + ['best_params']\n",
    "pd.DataFrame(columns=header_metrics).to_csv(METRICS_FILE, index=False)\n",
    "\n",
    "\n",
    "print(f\"\\nIniciando procesamiento para {len(unique_clusters)} clusters...\")\n",
    "for cluster_count, cluster_id in enumerate(unique_clusters):\n",
    "    print(f\"\\n--- Procesando Cluster {cluster_count+1}/{len(unique_clusters)}: ID = {cluster_id} ---\")\n",
    "\n",
    "    cluster_df = None\n",
    "    X_cluster, y_cluster, ids_cluster, dates_cluster = None, None, None, None\n",
    "    X_train_val, X_test, y_train_val, y_test = None, None, None, None\n",
    "    ids_test, dates_test = None, None\n",
    "    pipeline, search, best_model_cluster = None, None, None\n",
    "    y_pred_test_cluster = None\n",
    "    predictions_df = None\n",
    "\n",
    "    try:\n",
    "        print(f\"Cargando datos para el cluster {cluster_id}...\")\n",
    "        filters = [(CLUSTER_COL, '=', cluster_id)]\n",
    "        cluster_df = pd.read_parquet(PARQUET_FILE, filters=filters)\n",
    "        cluster_df = cluster_df.sort_values(by=SERIES_ID_COLS + [DATE_COL]).reset_index(drop=True)\n",
    "        print(f\"Cluster {cluster_id}: {len(cluster_df)} filas cargadas.\")\n",
    "\n",
    "        if len(cluster_df) < (N_SPLITS_CV + 2) * 2 :\n",
    "            print(f\"Datos insuficientes para CV en cluster {cluster_id} ({len(cluster_df)} filas). Saltando cluster.\")\n",
    "            continue\n",
    "        # Define a threshold and a sampling size\n",
    "        VERY_LARGE_CLUSTER_THRESHOLD = 2000000 # e.g., if more than 2M rows\n",
    "        SAMPLE_SIZE_FOR_LARGE_CLUSTERS = 500000 # Target approximate sample size\n",
    "\n",
    "        # --- MODIFIED SUBSAMPLING LOGIC ---\n",
    "        # Apply this subsampling if the cluster is very large (e.g., cluster_id == 2 or general threshold)\n",
    "        # if cluster_id == 2 and len(cluster_df) > VERY_LARGE_CLUSTER_THRESHOLD: # Example for a specific cluster\n",
    "        if len(cluster_df) > VERY_LARGE_CLUSTER_THRESHOLD: # General threshold for any large cluster\n",
    "            print(f\"Cluster {cluster_id} is very large ({len(cluster_df)} rows). Subsampling complete series to be around {SAMPLE_SIZE_FOR_LARGE_CLUSTERS} rows.\")\n",
    "\n",
    "            unique_series_identifiers = cluster_df[SERIES_ID_COLS].drop_duplicates()\n",
    "            n_unique_series_original = len(unique_series_identifiers)\n",
    "\n",
    "            if n_unique_series_original == 0:\n",
    "                print(f\"Warning: No unique series found in cluster {cluster_id} for subsampling. Skipping subsampling logic.\")\n",
    "            elif n_unique_series_original == 1:\n",
    "                print(f\"Cluster {cluster_id} has only one series ({len(cluster_df)} rows). If it's too large, consider other strategies like truncating the series or feature engineering for scalability.\")\n",
    "                # If this single series is still too large, you might need to truncate it, e.g., take the last N points\n",
    "                if len(cluster_df) > SAMPLE_SIZE_FOR_LARGE_CLUSTERS:\n",
    "                    print(f\"The single series in cluster {cluster_id} has {len(cluster_df)} rows, which is more than the target {SAMPLE_SIZE_FOR_LARGE_CLUSTERS}. Truncating to the most recent {SAMPLE_SIZE_FOR_LARGE_CLUSTERS} observations for this series.\")\n",
    "                    # Ensure it's sorted by date to get the most recent\n",
    "                    cluster_df = cluster_df.sort_values(by=SERIES_ID_COLS + [DATE_COL])\n",
    "                    cluster_df = cluster_df.tail(SAMPLE_SIZE_FOR_LARGE_CLUSTERS).reset_index(drop=True)\n",
    "\n",
    "            else:\n",
    "                # Estimate average rows per series to guide how many series to sample\n",
    "                avg_rows_per_series = len(cluster_df) / n_unique_series_original\n",
    "                \n",
    "                # Calculate how many series to sample to get *approximately* SAMPLE_SIZE_FOR_LARGE_CLUSTERS rows\n",
    "                # This is an estimate; the actual number of rows will vary.\n",
    "                num_series_to_sample = int(SAMPLE_SIZE_FOR_LARGE_CLUSTERS / avg_rows_per_series)\n",
    "                num_series_to_sample = max(1, min(num_series_to_sample, n_unique_series_original)) # Ensure it's within bounds\n",
    "\n",
    "                print(f\"Original series count: {n_unique_series_original}. Estimated series to sample: {num_series_to_sample}.\")\n",
    "                \n",
    "                # Randomly select series identifiers\n",
    "                sampled_series_ids = unique_series_identifiers.sample(n=num_series_to_sample, random_state=42)\n",
    "                \n",
    "                # Filter the original DataFrame to include only the rows from the sampled series\n",
    "                # Using merge is a robust way to do this\n",
    "                cluster_df = pd.merge(cluster_df, sampled_series_ids, on=SERIES_ID_COLS, how='inner')\n",
    "                \n",
    "                # After merging, it's good practice to re-sort if the order might have changed,\n",
    "                # though 'inner' merge should preserve order within groups from the left DataFrame.\n",
    "                # Explicitly re-sorting ensures data is correctly ordered for time series operations.\n",
    "                cluster_df = cluster_df.sort_values(by=SERIES_ID_COLS + [DATE_COL]).reset_index(drop=True)\n",
    "                \n",
    "                print(f\"Cluster {cluster_id} now has {len(cluster_df)} rows from {len(sampled_series_ids)} unique series after subsampling.\")\n",
    "\n",
    "                # Optional: If still too large (because some sampled series were huge), you could add a second pass\n",
    "                # to trim the largest series or further remove some of the sampled series.\n",
    "                # For now, this provides a good balance.\n",
    "        # --- END OF MODIFIED SUBSAMPLING LOGIC ---\n",
    "\n",
    "\n",
    "        X_cluster = cluster_df[FEATURES_COLS]\n",
    "        y_cluster = cluster_df[TARGET_VARIABLE]\n",
    "        ids_cluster = cluster_df[SERIES_ID_COLS + [CLUSTER_COL]]\n",
    "        dates_cluster = cluster_df[DATE_COL]\n",
    "\n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True)) # MODIFIED\n",
    "        ])\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, NUMERICAL_FEATURES),\n",
    "                ('cat', categorical_transformer, CATEGORICAL_FEATURES_FOR_MODEL)\n",
    "            ],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        print(f\"Realizando split Train/Test para el cluster {cluster_id}...\")\n",
    "        test_size_ratio = 0.2\n",
    "        n_rows = len(cluster_df)\n",
    "        split_point = int(n_rows * (1 - test_size_ratio))\n",
    "\n",
    "        if split_point < N_SPLITS_CV + 1 or n_rows - split_point < 1:\n",
    "             print(f\"Datos insuficientes para split Train/Test adecuado en cluster {cluster_id}. Saltando.\")\n",
    "             continue\n",
    "\n",
    "        X_train_val, X_test = X_cluster.iloc[:split_point], X_cluster.iloc[split_point:]\n",
    "        y_train_val, y_test = y_cluster.iloc[:split_point], y_cluster.iloc[split_point:]\n",
    "        ids_test = ids_cluster.iloc[split_point:]\n",
    "        dates_test = dates_cluster.iloc[split_point:]\n",
    "        y_true_train_for_mase = y_train_val.copy().values\n",
    "        print(f\"Cluster {cluster_id}: Train/Val={len(X_train_val)}, Test={len(X_test)}\")\n",
    "\n",
    "        cv_test_size = max(1, len(X_train_val) // (N_SPLITS_CV + 1))\n",
    "        tscv = TimeSeriesSplit(n_splits=N_SPLITS_CV, gap=0, test_size=cv_test_size)\n",
    "\n",
    "        rf_model = RandomForestRegressor(random_state=42, n_jobs=1\n",
    "                                         )\n",
    "        pipeline = Pipeline([\n",
    "            ('preprocess', preprocessor),\n",
    "            ('regressor', rf_model)\n",
    "        ])\n",
    "\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=pipeline,\n",
    "            param_distributions=param_dist_rf,\n",
    "            n_iter=N_ITER_HPT,\n",
    "            cv=tscv,\n",
    "            scoring=SCORING_METRIC_HPT,\n",
    "            n_jobs=1, # Use 1 for RandomizedSearchCV to avoid issues with nested parallelism if RF also uses n_jobs\n",
    "            refit=True,\n",
    "            random_state=42,\n",
    "            verbose=1\n",
    "        )\n",
    "        print(\"!!! DIAGNOSTIC: Starting manual pipeline fit test on a TINY sample...\")\n",
    "        if 'X_train_val' in locals() and 'y_train_val' in locals() and not X_train_val.empty:\n",
    "            # Ensure X_train_val and y_train_val are Pandas DataFrames/Series for .head()\n",
    "            if not isinstance(X_train_val, pd.DataFrame):\n",
    "                print(\"Warning: X_train_val is not a Pandas DataFrame, .head() might fail or behave unexpectedly.\")\n",
    "            if not isinstance(y_train_val, pd.Series):\n",
    "                print(\"Warning: y_train_val is not a Pandas Series, .head() might fail or behave unexpectedly.\")\n",
    "\n",
    "            X_sample_diag = X_train_val.head(1000) # Use a very small sample, e.g., 1000 rows\n",
    "            y_sample_diag = y_train_val.head(1000)\n",
    "            print(f\"Sampled {len(X_sample_diag)} rows for diagnostic fit.\")\n",
    "\n",
    "            # 'pipeline' should be the Pipeline object you're passing to RandomizedSearchCV\n",
    "            # It contains your preprocessor and the RandomForestRegressor\n",
    "            # Ensure 'pipeline' is defined correctly before this point.\n",
    "            # Example if you need to redefine for clarity (but use your actual pipeline):\n",
    "            # from sklearn.ensemble import RandomForestRegressor\n",
    "            # rf_model_diag = RandomForestRegressor(random_state=42, n_jobs=2) # Or n_jobs=1\n",
    "            # pipeline_diag_instance = Pipeline([\n",
    "            #     ('preprocess', preprocessor), # 'preprocessor' is your ColumnTransformer\n",
    "            #     ('regressor', rf_model_diag)\n",
    "            # ])\n",
    "\n",
    "            try:\n",
    "                import time\n",
    "                start_time_diag = time.time()\n",
    "                print(\"!!! DIAGNOSTIC: Calling pipeline.fit() on sample...\")\n",
    "                pipeline.fit(X_sample_diag, y_sample_diag) # Use your actual 'pipeline' variable\n",
    "                end_time_diag = time.time()\n",
    "                print(f\"!!! DIAGNOSTIC: Manual pipeline fit on sample completed in {end_time_diag - start_time_diag:.2f} seconds.\")\n",
    "            except Exception as e_diag:\n",
    "                print(f\"!!! DIAGNOSTIC ERROR during manual pipeline fit on sample: {e_diag}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(\"!!! DIAGNOSTIC: X_train_val or y_train_val not available for sample fitting test.\")\n",
    "        print(\"!!! DIAGNOSTIC: End of manual test. Continuing to RandomizedSearchCV (or not, if you exit)...\")\n",
    "        # You might want to add exit() here during testing to not proceed to the full RandomizedSearchCV\n",
    "        # import sys\n",
    "        # sys.exit(\"Exiting after diagnostic test.\")\n",
    "        # --- End of Temporary Diagnostic Code ---\n",
    "\n",
    "\n",
    "        print(f\"Iniciando HPT con RandomForest para cluster {cluster_id}...\")\n",
    "        search.fit(X_train_val, y_train_val)\n",
    "        best_model_cluster = search.best_estimator_\n",
    "        best_params_cluster = search.best_params_\n",
    "        print(f\"Mejores parámetros encontrados para cluster {cluster_id} (RandomForest): {best_params_cluster}\")\n",
    "\n",
    "        print(f\"Realizando predicciones en el conjunto de test del cluster {cluster_id}...\")\n",
    "        y_pred_test_cluster = best_model_cluster.predict(X_test)\n",
    "\n",
    "        print(f\"Calculando métricas agregadas para el cluster {cluster_id}...\")\n",
    "        cluster_metrics = evaluate_model(y_true_train_for_mase, y_test.values, y_pred_test_cluster, label=\"Cluster\")\n",
    "        metrics_row = {CLUSTER_COL: cluster_id}\n",
    "        metrics_row.update(cluster_metrics)\n",
    "        metrics_row['best_params'] = str(best_params_cluster)\n",
    "        all_cluster_metrics.append(metrics_row)\n",
    "        pd.DataFrame([metrics_row]).to_csv(METRICS_FILE, mode='a', header=False, index=False)\n",
    "\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'establecimiento': ids_test['establecimiento'],\n",
    "            'material': ids_test['material'],\n",
    "            DATE_COL: dates_test,\n",
    "            'actual_volume': y_test.values,\n",
    "            'predicted_volume': y_pred_test_cluster,\n",
    "            CLUSTER_COL: ids_test[CLUSTER_COL]\n",
    "        })\n",
    "        predictions_df.to_csv(PREDICTIONS_FILE, mode='a', header=False, index=False)\n",
    "\n",
    "        print(f\"Generando gráficos para las series del cluster {cluster_id}...\")\n",
    "        unique_series_in_test_df = ids_test[SERIES_ID_COLS].drop_duplicates()\n",
    "        max_plots_per_cluster = 100\n",
    "        plot_count = 0\n",
    "        for _, series_row in unique_series_in_test_df.iterrows():\n",
    "            if plot_count >= max_plots_per_cluster:\n",
    "                print(f\"Límite de {max_plots_per_cluster} gráficos alcanzado para el cluster {cluster_id}.\")\n",
    "                break\n",
    "            estab = series_row['establecimiento']\n",
    "            material = series_row['material']\n",
    "            mask = (predictions_df['establecimiento'] == estab) & \\\n",
    "                   (predictions_df['material'] == material)\n",
    "            series_pred_df = predictions_df[mask]\n",
    "            if not series_pred_df.empty:\n",
    "                plot_filename = os.path.join(PLOTS_DIR, f'pred_vs_actual_rf_{estab}_{material}_cluster{cluster_id}.png')\n",
    "                plot_predictions(\n",
    "                    series_pred_df[DATE_COL],\n",
    "                    series_pred_df['actual_volume'],\n",
    "                    series_pred_df['predicted_volume'],\n",
    "                    estab, material, cluster_id, plot_filename\n",
    "                )\n",
    "                plot_count +=1\n",
    "\n",
    "        model_filename = os.path.join(MODELS_DIR, f'rf_model_cluster_{cluster_id}.joblib')\n",
    "        joblib.dump(best_model_cluster, model_filename)\n",
    "        print(f\"Modelo RandomForest del cluster {cluster_id} guardado en: {model_filename}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR procesando el cluster {cluster_id}: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc() # Print full traceback for detailed error diagnosis\n",
    "        error_row = {CLUSTER_COL: cluster_id}\n",
    "        for m_key in header_metrics:\n",
    "            if m_key not in [CLUSTER_COL, 'best_params']:\n",
    "                error_row[m_key] = 'ERROR'\n",
    "        error_row['best_params'] = str(e)\n",
    "        pd.DataFrame([error_row]).to_csv(METRICS_FILE, mode='a', header=False, index=False)\n",
    "\n",
    "    finally:\n",
    "        del cluster_df, X_cluster, y_cluster, ids_cluster, dates_cluster\n",
    "        del X_train_val, X_test, y_train_val, y_test, ids_test, dates_test\n",
    "        del pipeline, search, best_model_cluster, y_pred_test_cluster, predictions_df\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n--- Proceso Completado ---\")\n",
    "# Final summary metrics can be loaded and printed here if desired\n",
    "# df_metrics = pd.read_csv(METRICS_FILE)\n",
    "# print(df_metrics.describe())\n",
    "print(f\"Resultados de RandomForest guardados en el directorio: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "29984a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_random_forest_by_clusters(\n",
    "    cluster_labels,\n",
    "    max_series_per_cluster=500000,\n",
    "    max_plots_per_cluster=100,\n",
    "    parquet_file=PARQUET_FILE,\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    metrics_file=METRICS_FILE,\n",
    "    predictions_file=PREDICTIONS_FILE,\n",
    "    plots_dir=PLOTS_DIR,\n",
    "    models_dir=MODELS_DIR\n",
    "):\n",
    "    import gc\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pyarrow.parquet as pq\n",
    "    from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import matplotlib.pyplot as plt\n",
    "    import joblib\n",
    "    import os\n",
    "\n",
    "    # --- Configuración General ---\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(plots_dir, exist_ok=True)\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    TARGET_VARIABLE = 'weekly_volume'\n",
    "    SERIES_ID_COLS = ['establecimiento', 'material']\n",
    "    CLUSTER_COL = 'cluster_label'\n",
    "    DATE_COL = 'week'\n",
    "\n",
    "    # Leer metadata para obtener todas las columnas\n",
    "    parquet_meta = pq.read_metadata(parquet_file)\n",
    "    ALL_COLS = [col.name for col in parquet_meta.schema]\n",
    "\n",
    "    FEATURES_COLS = [\n",
    "        col for col in ALL_COLS\n",
    "        if col not in [TARGET_VARIABLE, DATE_COL, CLUSTER_COL, 'last_sale_week']\n",
    "    ]\n",
    "\n",
    "    CATEGORICAL_FEATURES_FOR_MODEL = [f for f in ['establecimiento', 'material'] if f in FEATURES_COLS]\n",
    "    NUMERICAL_FEATURES = [col for col in FEATURES_COLS if col not in CATEGORICAL_FEATURES_FOR_MODEL]\n",
    "\n",
    "    def calculate_mase(y_true_train, y_true_test, y_pred_test):\n",
    "        y_true_train = np.array(y_true_train).flatten()\n",
    "        y_true_test = np.array(y_true_test).flatten()\n",
    "        y_pred_test = np.array(y_pred_test).flatten()\n",
    "        if len(y_true_train) < 2:\n",
    "            return np.nan\n",
    "        naive_forecast_error_train = np.mean(np.abs(np.diff(y_true_train)))\n",
    "        if naive_forecast_error_train < 1e-9:\n",
    "            model_mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "            return np.inf if model_mae_test > 1e-9 else 0.0\n",
    "        model_mae_test = mean_absolute_error(y_true_test, y_pred_test)\n",
    "        return model_mae_test / naive_forecast_error_train\n",
    "\n",
    "    def evaluate_model(y_true_train, y_true_test, y_pred_test, label=\"Cluster\"):\n",
    "        metrics = {\n",
    "            f'{label}_mae': mean_absolute_error(y_true_test, y_pred_test),\n",
    "            f'{label}_rmse': np.sqrt(mean_squared_error(y_true_test, y_pred_test)),\n",
    "            f'{label}_mape': mean_absolute_percentage_error(y_true_test, y_pred_test) if np.all(np.abs(y_true_test) > 1e-9) else np.nan,\n",
    "            f'{label}_r2': r2_score(y_true_test, y_pred_test),\n",
    "            f'{label}_mase': calculate_mase(y_true_train, y_true_test, y_pred_test)\n",
    "        }\n",
    "        mape_key = f'{label}_mape'\n",
    "        if mape_key in metrics and np.isinf(metrics[mape_key]):\n",
    "            metrics[mape_key] = np.nan\n",
    "        return metrics\n",
    "\n",
    "    def plot_predictions(dates_test, y_true_test, y_pred_test, estab, material, cluster_id, filepath):\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.plot(dates_test, y_true_test, label='Real', marker='.', linestyle='-')\n",
    "        plt.plot(dates_test, y_pred_test, label=f'Predicción RF (Cluster {cluster_id})', marker='x', linestyle='--')\n",
    "        plt.title(f'Predicción RF vs Real - {estab} / {material} (Cluster {cluster_id})')\n",
    "        plt.xlabel('Semana')\n",
    "        plt.ylabel('Volumen Semanal')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(filepath)\n",
    "        plt.close()\n",
    "\n",
    "    header_preds = SERIES_ID_COLS + [DATE_COL, 'actual_volume', 'predicted_volume', CLUSTER_COL]\n",
    "    pd.DataFrame(columns=header_preds).to_csv(predictions_file, index=False)\n",
    "\n",
    "    header_metrics = [CLUSTER_COL] + [f'Cluster_{m}' for m in ['mae', 'rmse', 'mape', 'r2', 'mase']] + ['best_params']\n",
    "    pd.DataFrame(columns=header_metrics).to_csv(metrics_file, index=False)\n",
    "\n",
    "    N_SPLITS_CV = 5\n",
    "    N_ITER_HPT = 5\n",
    "    SCORING_METRIC_HPT = 'neg_mean_absolute_error'\n",
    "    param_dist_rf = {\n",
    "        'regressor__n_estimators': [50, 100, 150],\n",
    "        'regressor__max_depth': [None, 10, 20],\n",
    "        'regressor__min_samples_split': [10, 20, 100],\n",
    "        'regressor__min_samples_leaf': [5, 10, 50],\n",
    "        'regressor__max_features': ['sqrt', 'log2', 0.7]\n",
    "    }\n",
    "\n",
    "    print(f\"\\nIniciando procesamiento para {len(cluster_labels)} clusters...\")\n",
    "    for cluster_count, cluster_id in enumerate(cluster_labels):\n",
    "        print(f\"\\n--- Procesando Cluster {cluster_count+1}/{len(cluster_labels)}: ID = {cluster_id} ---\")\n",
    "        try:\n",
    "            print(f\"Cargando datos para el cluster {cluster_id}...\")\n",
    "            filters = [(CLUSTER_COL, '=', cluster_id)]\n",
    "            cluster_df = pd.read_parquet(parquet_file, filters=filters)\n",
    "            cluster_df = cluster_df.sort_values(by=SERIES_ID_COLS + [DATE_COL]).reset_index(drop=True)\n",
    "            print(f\"Cluster {cluster_id}: {len(cluster_df)} filas cargadas.\")\n",
    "            # Limitar a max_series_per_cluster series únicas\n",
    "            unique_series = cluster_df[SERIES_ID_COLS].drop_duplicates()\n",
    "\n",
    "            if len(cluster_df) > max_series_per_cluster:\n",
    "                print(f\"Cluster {cluster_id} is very large ({len(cluster_df)} rows). Subsampling complete series to be around {max_series_per_cluster} rows.\")\n",
    "                unique_series = cluster_df[SERIES_ID_COLS].drop_duplicates()\n",
    "                n_unique_series_original = len(unique_series)\n",
    "                if n_unique_series_original == 0:\n",
    "                    print(f\"Warning: No unique series found in cluster {cluster_id} for subsampling. Skipping subsampling logic.\")\n",
    "                elif n_unique_series_original == 1:\n",
    "                    print(f\"Cluster {cluster_id} has only one series ({len(cluster_df)} rows). If it's too large, consider truncating.\")\n",
    "                    if len(cluster_df) > max_series_per_cluster:\n",
    "                        cluster_df = cluster_df.sort_values(by=SERIES_ID_COLS + [DATE_COL])\n",
    "                        cluster_df = cluster_df.tail(max_series_per_cluster).reset_index(drop=True)\n",
    "                else:\n",
    "                    avg_rows_per_series = len(cluster_df) / n_unique_series_original\n",
    "                    num_series_to_sample = int(max_series_per_cluster / avg_rows_per_series)\n",
    "                    num_series_to_sample = max(1, min(num_series_to_sample, n_unique_series_original))\n",
    "                    sampled_series = unique_series.sample(n=num_series_to_sample, random_state=42)\n",
    "                    cluster_df = cluster_df.merge(sampled_series, on=SERIES_ID_COLS, how='inner')\n",
    "                    cluster_df = cluster_df.sort_values(by=SERIES_ID_COLS + [DATE_COL]).reset_index(drop=True)\n",
    "                    print(f\"Cluster {cluster_id} now has {len(cluster_df)} rows from {len(sampled_series)} unique series after subsampling.\")\n",
    "            X_cluster = cluster_df[FEATURES_COLS]\n",
    "            y_cluster = cluster_df[TARGET_VARIABLE]\n",
    "            ids_cluster = cluster_df[SERIES_ID_COLS + [CLUSTER_COL]]\n",
    "            dates_cluster = cluster_df[DATE_COL]\n",
    "\n",
    "            numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())\n",
    "            ])\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "                ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True))\n",
    "            ])\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numeric_transformer, NUMERICAL_FEATURES),\n",
    "                    ('cat', categorical_transformer, CATEGORICAL_FEATURES_FOR_MODEL)\n",
    "                ],\n",
    "                remainder='passthrough'\n",
    "            )\n",
    "\n",
    "            print(f\"Realizando split Train/Test para el cluster {cluster_id}...\")\n",
    "            test_size_ratio = 0.2\n",
    "            n_rows = len(cluster_df)\n",
    "            split_point = int(n_rows * (1 - test_size_ratio))\n",
    "\n",
    "            if split_point < N_SPLITS_CV + 1 or n_rows - split_point < 1:\n",
    "                print(f\"Datos insuficientes para split Train/Test adecuado en cluster {cluster_id}. Saltando.\")\n",
    "                continue\n",
    "\n",
    "            X_train_val, X_test = X_cluster.iloc[:split_point], X_cluster.iloc[split_point:]\n",
    "            y_train_val, y_test = y_cluster.iloc[:split_point], y_cluster.iloc[split_point:]\n",
    "            ids_test = ids_cluster.iloc[split_point:]\n",
    "            dates_test = dates_cluster.iloc[split_point:]\n",
    "            y_true_train_for_mase = y_train_val.copy().values\n",
    "            print(f\"Cluster {cluster_id}: Train/Val={len(X_train_val)}, Test={len(X_test)}\")\n",
    "\n",
    "            cv_test_size = max(1, len(X_train_val) // (N_SPLITS_CV + 1))\n",
    "            tscv = TimeSeriesSplit(n_splits=N_SPLITS_CV, gap=0, test_size=cv_test_size)\n",
    "\n",
    "            rf_model = RandomForestRegressor(random_state=42, n_jobs=1)\n",
    "            pipeline = Pipeline([\n",
    "                ('preprocess', preprocessor),\n",
    "                ('regressor', rf_model)\n",
    "            ])\n",
    "\n",
    "            search = RandomizedSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_distributions=param_dist_rf,\n",
    "                n_iter=N_ITER_HPT,\n",
    "                cv=tscv,\n",
    "                scoring=SCORING_METRIC_HPT,\n",
    "                n_jobs=1,\n",
    "                refit=True,\n",
    "                random_state=42,\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "            print(f\"Iniciando HPT con RandomForest para cluster {cluster_id}...\")\n",
    "            search.fit(X_train_val, y_train_val)\n",
    "            best_model_cluster = search.best_estimator_\n",
    "            best_params_cluster = search.best_params_\n",
    "            print(f\"Mejores parámetros encontrados para cluster {cluster_id} (RandomForest): {best_params_cluster}\")\n",
    "\n",
    "            print(f\"Realizando predicciones en el conjunto de test del cluster {cluster_id}...\")\n",
    "            y_pred_test_cluster = best_model_cluster.predict(X_test)\n",
    "\n",
    "            print(f\"Calculando métricas agregadas para el cluster {cluster_id}...\")\n",
    "            cluster_metrics = evaluate_model(y_true_train_for_mase, y_test.values, y_pred_test_cluster, label=\"Cluster\")\n",
    "            metrics_row = {CLUSTER_COL: cluster_id}\n",
    "            metrics_row.update(cluster_metrics)\n",
    "            metrics_row['best_params'] = str(best_params_cluster)\n",
    "            pd.DataFrame([metrics_row]).to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "\n",
    "            predictions_df = pd.DataFrame({\n",
    "                'establecimiento': ids_test['establecimiento'],\n",
    "                'material': ids_test['material'],\n",
    "                DATE_COL: dates_test,\n",
    "                'actual_volume': y_test.values,\n",
    "                'predicted_volume': y_pred_test_cluster,\n",
    "                CLUSTER_COL: ids_test[CLUSTER_COL]\n",
    "            })\n",
    "            predictions_df.to_csv(predictions_file, mode='a', header=False, index=False)\n",
    "\n",
    "            print(f\"Generando gráficos para las series del cluster {cluster_id}...\")\n",
    "            unique_series_in_test_df = ids_test[SERIES_ID_COLS].drop_duplicates()\n",
    "            plot_count = 0\n",
    "            for _, series_row in unique_series_in_test_df.iterrows():\n",
    "                if plot_count >= max_plots_per_cluster:\n",
    "                    print(f\"Límite de {max_plots_per_cluster} gráficos alcanzado para el cluster {cluster_id}.\")\n",
    "                    break\n",
    "                estab = series_row['establecimiento']\n",
    "                material = series_row['material']\n",
    "                mask = (predictions_df['establecimiento'] == estab) & \\\n",
    "                       (predictions_df['material'] == material)\n",
    "                series_pred_df = predictions_df[mask]\n",
    "                if not series_pred_df.empty:\n",
    "                    plot_filename = os.path.join(plots_dir, f'pred_vs_actual_rf_{estab}_{material}_cluster{cluster_id}.png')\n",
    "                    plot_predictions(\n",
    "                        series_pred_df[DATE_COL],\n",
    "                        series_pred_df['actual_volume'],\n",
    "                        series_pred_df['predicted_volume'],\n",
    "                        estab, material, cluster_id, plot_filename\n",
    "                    )\n",
    "                    plot_count += 1\n",
    "\n",
    "            model_filename = os.path.join(models_dir, f'rf_model_cluster_{cluster_id}.joblib')\n",
    "            joblib.dump(best_model_cluster, model_filename)\n",
    "            print(f\"Modelo RandomForest del cluster {cluster_id} guardado en: {model_filename}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR procesando el cluster {cluster_id}: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            error_row = {CLUSTER_COL: cluster_id}\n",
    "            for m_key in header_metrics:\n",
    "                if m_key not in [CLUSTER_COL, 'best_params']:\n",
    "                    error_row[m_key] = 'ERROR'\n",
    "            error_row['best_params'] = str(e)\n",
    "            pd.DataFrame([error_row]).to_csv(metrics_file, mode='a', header=False, index=False)\n",
    "\n",
    "        finally:\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"\\n--- Proceso Completado ---\")\n",
    "    print(f\"Resultados de RandomForest guardados en el directorio: {output_dir}\")\n",
    "\n",
    "# Ejemplo de uso:\n",
    "# cluster_labels = [1, 2, 3]  # O cargar desde archivo/metadata\n",
    "# run_random_forest_by_clusters(cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de53ca9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iniciando procesamiento para 1 clusters...\n",
      "\n",
      "--- Procesando Cluster 1/1: ID = 6 ---\n",
      "Cargando datos para el cluster 6...\n",
      "Cluster 6: 941142 filas cargadas.\n",
      "Cluster 6 is very large (941142 rows). Subsampling complete series to be around 500000 rows.\n",
      "Cluster 6 now has 497618 rows from 3835 unique series after subsampling.\n",
      "Realizando split Train/Test para el cluster 6...\n",
      "Cluster 6: Train/Val=398094, Test=99524\n",
      "Iniciando HPT con RandomForest para cluster 6...\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Mejores parámetros encontrados para cluster 6 (RandomForest): {'regressor__n_estimators': 100, 'regressor__min_samples_split': 100, 'regressor__min_samples_leaf': 10, 'regressor__max_features': 0.7, 'regressor__max_depth': 20}\n",
      "Realizando predicciones en el conjunto de test del cluster 6...\n",
      "Calculando métricas agregadas para el cluster 6...\n",
      "Generando gráficos para las series del cluster 6...\n",
      "Límite de 100 gráficos alcanzado para el cluster 6.\n",
      "Modelo RandomForest del cluster 6 guardado en: ../models/random_forest/trained_cluster_models_rf/rf_model_cluster_6.joblib\n",
      "\n",
      "--- Proceso Completado ---\n",
      "Resultados de RandomForest guardados en el directorio: ../models/random_forest\n"
     ]
    }
   ],
   "source": [
    "run_random_forest_by_clusters([6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
